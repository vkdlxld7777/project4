arXiv:1607.01759v3  [cs.CL]  9 Aug 2016
BagofTricksforEfcientTextClassication
ArmandJoulinEdouardGravePiotrBojanowskiTomasMikolov
FacebookAIResearch
f
ajoulin,egrave,bojanowski,tmikolov
g
@fb.com
Abstract
Thispaperexploresasimpleandefcient

baselinefortextclassication.Ourex-

perimentsshowthatourfasttextclassi-

er
fastText
isoftenonparwithdeep
learningclassiersintermsofaccuracy,and

manyordersofmagnitudefasterfortraining

andevaluation.Wecantrain
fastText
on
morethanonebillionwordsinlessthanten

minutesusingastandardmulticoreCPU,and

classifyhalfamillionsentencesamong312K

classesinlessthanaminute.
1Introduction

Textclassi cationisanim portanttask inN atural

L anguageP rocessingw ithm any applications,such

asw ebsearch,inform ation retrieval,rankingand

docum entclassi cation (D eerw esteretal.,1990;

PangandL ee,2008).R ecently,models based

on neuralnetworkshave becomeincreasingly

popular(K im,2014;Z hang andL eC un,2015;

C onneau etal. ,2016).W hilethesemodelsachieve

verygoodperform anceinpractice,theytendto be

relatively slowbothattrainandtesttime,limiting

theiruseonverylarge datasets.
M eanw hile, linearclassi ersare of-
tenconsideredasstrongbaselinesfortext

classi cationproblem s(Joachims,1998;

M cC allumandN igam,1998; Fanetal.,2008).

D espite theirsim plicity,they often obtainstate-

of-the-artperform ancesiftherightfeatures are

used(WangandM anning,2012).T heyalso

havethepotentialtoscaletoverylargecor-

pus(A garwaletal. ,2014). 
Inthiswork,w eexplorewaystoscalethese
baselinestoverylargecorpuswithalargeoutput

space,inthecontextoftextclassi cation.Inspired

bytherecentworkinef cientwordrepresentation

learning(Mikolov etal.,2013;L evyetal.,2015),

w eshow thatlinearmodelswitharankconstraint

and afastloss approximationcan train onabillion

wordsw ithintenminutes,whileachievingperfor-

manceon parw iththestate-of-the-art.Weevalu-

ate the qualityofourapproach
f a s t T e x t 
1
on two
differenttasks,namely tag predictionand sentiment

analysis.

2Modelarchitecture

Asimple andef cientbaselineforsentence

classi cation istorepresentsentencesasbagof

words(B oW)andtrainalinearclassi er,e.g.,a

logisticregressionoran S VM(Joachims,1998;

Fanetal.,2008). H owever,linearclassi ersdo

notshareparametersamongfeaturesandclasses.

T hispossiblylimitstheirgeneralizationin the

contextoflargeoutputspacewheresomeclasses

haveveryfewexamples.Commonsolutions

to thisproblemare tofactorizethelinearclas-

si er into low rank matrices(S chutze,1992;

Mikolov etal.,2013) ortousemultilayer

neuralnetworks(CollobertandWeston,2008;

Z hang etal.,2015).
F igure1show sasimplelinearmodelwithrank
constraint. T he rst weight matrix
A
isalook-up
table overthewords.T hewordrepresentationsare

thenaveragedintoa textrepresentation,whichisin

turnfedtoalinearclassi er.T hetextrepresenta-
1
https://github.com/facebookresearch/fastText
x 
1
x 
2
: : :
x
N

1
x
N
hidden
output
Figure 1:
Modelarchitectureof
fastText
forasentencewith
N
ngramfeatures
x
1
;:::;x
N
.Thefeaturesareembeddedand
averagedtoformthehiddenvariable.

tionis anhiddenvariablew hichcanbepotentially

bereused.T hisarchitecture issimilartothecbow 

m odelofM ikolovetal.(2013),w herethe middle

wordisreplaced byalabel.Weusethesoftmax

function
f 
tocom pute theprobabilitydistribution
overtheprede nedclasses. Forasetof
N
doc-
um ents,this leads tom inimizingthe negative log-

likelihoodovertheclasses:
 
1
N
N
X
n
=1
y 
n
log(
f 
(
BAx
n
))
;
w here
x 
n
isthenorm alizedbagoffeatures ofthe
n 
-
thdocum ent, 
y 
n
the label, 
A
and
B 
thew eightmatri-
ces.T his m odelistrainedasynchronouslyonmul-

tipleC P U susingstochastic gradientdescentanda

linearly decayinglearningrate.

2. 1 H ierarch ical softm ax

W hen the num berof classesis large,computingthe

linearclassi eris com putationally expensive.More

precisely,thecom putationalcomplexityis
O
(
k h
)
w here
k
is thenum berof classesand
h
thedi-
m ensionof thetextrepresentation.Inordertoim-

prove ourrunningtim e, w euse ahierarchicalsoft-

m ax(G oodman,2001)basedontheH uffmancod-

ingtree(M ikolov etal. ,2013).D uringtraining,the

com putationalcom plexity drops to
O
(
h
log
2
(
k
))
.
T he hierarchicalsoftm axisalsoadvantageousat
testtim ew hensearchingforthemostlikely class.

E achnodeisassociatedw ithaprobability thatisthe

probabilityofthe pathfrom the roottothatnode.If

thenodeisatdepth
l
+ 1
w ithparents
n 
1
; : : :; n 
l
,its
probabilityis
P
(
n 
l
+1
) = 
l
Y
i
=1
P
(
n 
i
)
:
T hismeansthattheprobabilityofanodeisalways

low erthan theoneofitsparent.E xploringthetree

w ithadepth rstsearchandtrackingthemaximum

probabilityamong theleavesallowsustodiscard

anybranchassociatedwithasmallprobability.In

practice,w eobserveareductionofthecomplexity

to
O
(
h
log
2
(
k
))
attesttime.T hisapproachisfur-
therextendedtocomputethe
T
-toptargetsatthe
costof
O
(log(
T
))
,usingabinary heap.
2.2 N -gram featu res

B agofwordsisinvarianttowordorderbuttaking

explicitlythisorderintoaccountisoftencomputa-

tionallyveryexpensive.Instead,weuseabagof

n-grams asadditionalfeaturestocapturesomepar-

tialinformationaboutthelocalwordorder.T his

isveryef cient inpracticewhileachieving compa-

rableresultstomethodsthatexplicitlyusetheor-

der(WangandManning,2012).
Wemaintain afastandmemory ef cient
mapping ofthen-gramsbyusingthe
hashing
trick
(Weinbergeretal.,2009)withthesamehash-
ingfunctionasinM ikolovetal.(2011)and10M 

binsifw eonly usedbigrams,and100Motherwise.

3Experiments

Weevaluate
f a s t T e x t 
on two differenttasks.
F irst,w ecompare ittoexisting textclassifersonthe

problemofsentimentanalysis.T hen,weevaluate

itscapacity toscaletolargeoutputspaceonatag

predictiondataset.N otethatourmodelcouldbeim-

plemented w iththeVowpalWabbitlibrary,
2
butwe
observein practice,thatourtailoredimplementation

isatleast2-5
 
faster.
3.1 S en timentan alysis

D atasetsan db aselines.
Weemploy the
same8datasets andevaluation protocol

ofZ hang etal.(2015). Wereportthen-grams

and T F IDFbaselinesfromZ hangetal.(2015),

asw ell asthe characterlevelconvolutional

model(char-C N N)ofZ hangandL eCun (2015),

thecharacterbased convolutionrecurrentnet-

work(char-C R NN )of(XiaoandCho,2016)and

the verydeepconvolutionalnetwork(VDCNN)

ofC onneau etal.(2016). Wealsocompare
2
Usingtheoptions
--nn
,
--ngrams
and
--log
multi
ModelAGSogouDBPYelpP.YelpF.Yah.A.Amz.F.Amz.P.
BoW(Zhangetal.,2015)88.892.996.692.258.068.954.690.
4
ngrams(Zhangetal.,2015)92.097.198.695.656.368.554.3
92.0
ngramsTFIDF(Zhangetal.,2015)92.497.298.795.454.868.
552.491.5
char-CNN(ZhangandLeCun,2015)87.295.198.394.762.071.
259.594.5
char-CRNN(XiaoandCho,2016)91.495.298.694.561.871.75
9.294.1
VDCNN(Conneauetal.,2016)91.396.898.795.764.773.463.
095.7
fastText
,
h
=10
91.593.998.193.860.472.055.891.2
fastText
,
h
=10
,bigram92.596.898.695.763.972.360.294.6
Table1:
Testaccuracy [%]onsentimentdatasets.
FastText
hasbeenrunwiththesameparametersforallthedatasets.It
has
10
hiddenunitsandweevaluateitwithandwithoutbigrams.For
char-CNN,weshowthebestreportednumberswithoutdata
augmentation.
ZhangandLeCun(2015)Conneauetal.(2016)
fastText
smallchar-CNNbigchar-CNNdepth=9depth=17depth=29
h
=10
,bigram
AG1h3h24m37m51m1s
Sogou--25m41m56m7s
DBpedia2h5h27m44m1h2s
YelpP.--28m43m1h093s
YelpF.--29m45m1h124s
Yah.A.8h1d1h1h332h5s
Amz.F.2d5d2h454h207h9s
Amz.P.2d5d2h454h257h10s
Table2:
Trainingtimeforasingleepochonsentimentanalysisdatas
etscomparedtochar-CNNandVDCNN.
toTangetal.(2015)follow ingtheirevaluation

protocol.Wereporttheir mainbaselinesas

w ellastheirtwoapproachesbasedonrecurrent

networks (C onv-G R N NandL S TM-G R N N ).

R esu lts. 
We presenttheresultsin F igure1. We
use10hidden unitsandrun
f a s t T e x t 
for5
epochsw ithalearningrateselectedonavalida-

tion set from 
f
0. 05,0. 1, 0. 25,0.5
g
.O n this task,
addingbigraminform ationimprovestheperfor-

m anceby1-4
% 
. O verallouraccuracyisslightly
betterthanchar-C N N andchar-C R NN and, abit

worsethanV D C N N .N otethatw ecanincrease

theaccuracyslightlybyusingmoren-grams, for

exam plew ith trigram s,the performanceon S ogou

goesupto97. 1
% 
. F inally,F igure3show sthat
ourm ethodiscom petitivew iththemethodspre-

sented inTangetal.(2015). Wetunethehyper-

param eterson thevalidationsetandobservethat

usingn-gram s upto5leadstothebestperfor-

m ance.U nlikeTangetal.(2015),
f a s t T e x t 
does
notuse pre-trainedword em beddings,w hichcan be

explainedthe1
% 
difference inaccuracy.
ModelYelp'13Yelp'14Yelp'15IMDB
SVM+TF59.861.862.440.5

CNN59.761.061.537.5

Conv-GRNN63.765.566.042.5

LSTM-GRNN65.167.167.645.3
fastText
64.266.266.645.2
Table3:
ComparisionwithTangetal.(2015).Thehyper-
parametersarechosenonthevalidationset.Wereportthete
st
accuracy.

Train in gtime.
B othchar-CNNandVDCNNare
trainedonaN V IDIATeslaK40GPU,whileour

models aretrained onaCPU using20threads.Ta-

ble2show s thatmethodsusing convolutionsaresev-

eralordersofmagnitudeslowerthan
f a s t T e x t 
.
Whileitispossible tohavea10

speedupfor
char-C N Nbyusing morerecentCUDAimplemen-

tationsofconvolutions,
f a s t T e x t 
takeslessthan
aminute totrainonthesedatasets.T heGRNNs

method ofTangetal.(2015)takesaround 12hours

perepochonC P Uw ithasinglethread.Ourspeed-
Input
PredictionTags
taiyoucon2011digitals:individualsdigitalpho-

tosfromtheanimeconventiontaiyoucon2011in

mesa,arizona.ifyouknowthemodeland/orthe

character,pleasecomment.
#cosplay#24mm#anime#animeconvention
#arizona#canon#con#convention

#cos
#cosplay
#costume#mesa#play
#taiyou#taiyoucon
2012twincitiespride2012twincitiespridepa-

rade
#minneapolis#2012twincitiesprideparade
#min-
neapolis
#mn#usa
beagleenjoysthesnowfall #snow#2007#beagle#hillsboro#
january
#maddison#maddy#oregon
#snow
christmas
#christmas#cameraphone#mobile
euclidavenue
#newyorkcity#cleveland#euclidavenue
Table4:
ExamplesfromthevalidationsetofYFCC100Mdatasetobtain
edwith
fastText
with
200
hiddenunitsandbigrams.
Weshowafewcorrectandincorrecttagpredictions.

upcom paredtoneuralnetwork based methodsin-

creasesw iththesizeofthedataset,goinguptoat

leasta15, 000
 
speed-up. 
3. 2 Tagp red iction 

D atasetan d b aselin es.
Totestscalabilityof
ourapproach,further evaluationiscarried on

theY F C C 100M dataset(T homeeetal.,2016)

w hichconsistsofalm ost100Mimages w ithcap-

tions,titlesandtags.Wefocusonpredicting the

tagsaccordingtothetitleandcaption(w edonot

use the im ages).Werem ovethe words andtags

occurringlessthan100tim esandsplitthedata

intoa train,validationandtestset.T he train

setcontains91, 188, 648examples (1.5Btokens).

T hevalidation has930, 497examples andthetest

set543, 424. T he vocabularysizeis297,141and

thereare312, 116uniquetags.Wew illrelease a

scriptthatrecreatesthis datasetso thatournumbers

could bereproduced. Wereportprecisionat1.
Weconsiderafrequency-basedbaselinew hich
predicts them ostfrequenttag.Wealsocom-

pare w ithTagspace(Westonetal.,2014), w hichis

atagpredictionm odelsim ilartoours, butbasedon

theW sabiem odel ofWeston etal.(2011).While

theTagspacem odelisdescribedusingconvolutions,

w econsider thelinearversion,w hichachievescom-

parable perform ancebutism uch faster.

R esu ltsan dtrain in g tim e. 
Table5presents a
com parisonof
f a s t T e x t 
andthe baselines. We
run
f a s t T e x t 
for5epochsandcompare it
toTagspacefortwosizesofthehiddenlayer,i.e.,50
Modelprec@1
Runningtime
TrainTest
Freq.baseline2.2--

Tagspace,
h
=50
30.13h86h
Tagspace,
h
=200
35.65h3215h
fastText
,
h
=50
31.26m4048s
fastText
,
h
=50
,bigram36.77m4750s
fastText
,
h
=200
41.110m341m29
fastText
,
h
=200
,bigram46.113m381m37
Table5:
Prec@1onthetestsetfortagpredictionon
YFCC100M.Wealsoreportthetrainingtimeandtesttime.

Testtimeisreportedforasinglethread,whiletraininguse
s20
threadsforbothmodels.

and200.B oth modelsachieveasimilar perfor-

mancew ithasmallhidden layer,butadding bi-

gramsgivesusasigni cantboostinaccuracy.At

testtime,Tagspaceneedstocomputethescores

foralltheclasses w hichmakesitrelatively slow,

w hile ourfastinferencegivesasigni cantspeed-up

w hen thenumberofclassesislarge(morethan300K

here).O verall, w earemorethananorderofmag-

nitude fastertoobtainmodel withabetterquality.

T he speedupofthe testphaseiseven moresigni -

cant(a600
 
speedup).Table4showssomequali-
tativeexamples.

4Discussion and conclusion

Inthiswork,w eproposeasimplebaselinemethod

fortextclassi cation.Unlikeunsupervisedly trained

word vectorsfrom word2vec,ourwordfeaturescan
beaveragedtogether toformgoodsentence repre-

sentations.In severaltasks, 
f a s t T e x t 
obtainsper-
form anceonparw ithrecentlyproposedmethodsin-

spiredbydeeplearning, w hilebeingmuchfaster.

A lthoughdeepneuralnetworkshaveintheorymuch

higherrepresentationalpow erthanshallowmodels,

itis notclearif sim pletextclassi cation problems

suchassentim ent analysis aretherightonestoeval-

uate them . Wew ill publishourcode so thatthe

researchcom munitycaneasilybuildontopofour

work. 

A ck n ow led gem en t. 
WethankG abrielS ynnaeve,
H erv´eG´egou,JasonWestonandL´eonB ottoufor

their help andcom ments.Wealso thank A lexisC on-

neau, D uyuTang andZ ichaoZ hangforprovidingus

w ithinform ationabout theirmethods.

References
[Agarwaletal.2014]AlekhAgarwal,OlivierChapelle,
MiroslavDud´k,andJohnLangford.2014.Areliable

effectiveterascalelinearlearningsystem.
JMLR
.
[CollobertandWeston2008]RonanCollobertandJason
Weston.2008.Auniedarchitecturefornaturallan-

guageprocessing:Deepneuralnetworkswithmulti-

tasklearning.In
ICML
.
[Conneauetal.2016]AlexisConneau,HolgerSchwenk,
Lo¨cBarrault,andYannLecun.2016.Verydeepcon-

volutionalnetworksfornaturallanguageprocessing.

arXivpreprintarXiv:1606.01781
.
[Deerwesteretal.1990]ScottDeerwester,SusanTDu-
mais,GeorgeWFurnas,ThomasKLandauer,and

RichardHarshman.1990.Indexingbylatentsemantic

analysis.
JournaloftheAmericansocietyforinforma-
tionscience
.
[Fanetal.2008]Rong-EnFan,Kai-WeiChang,Cho-Jui
Hsieh,Xiang-RuiWang,andChih-JenLin.2008.Li-

blinear:Alibraryforlargelinearclassication.
JMLR
.
[Goodman2001]JoshuaGoodman.2001.Classesforfast
maximumentropytraining.In
ICASSP
.
[Joachims1998]ThorstenJoachims.1998.
Textcatego-
rizationwithsupportvectormachines:Learningwith

manyrelevantfeatures
.Springer.
[Kim2014]YoonKim.2014.Convolutionalneuralnet-
worksforsentenceclassication.In
EMNLP
.
[Levyetal.2015]OmerLevy,YoavGoldberg,andIdo
Dagan.2015.Improvingdistributionalsimilaritywith

lessonslearnedfromwordembeddings.
TACL
.
[McCallumandNigam1998]AndrewMcCallumandKa-
malNigam.1998.Acomparisonofeventmodelsfor
naivebayestextclassication.In
AAAIworkshopon
learningfortextcategorization
.
[Mikolovetal.2011]Tom´asMikolov,AnoopDeoras,
DanielPovey,Luk´asBurget,andJan

Cernock˚y.2011.
Strategiesfortraininglargescaleneuralnetworklan-

guagemodels.In
WorkshoponAutomaticSpeech
RecognitionandUnderstanding
.IEEE.
[Mikolovetal.2013]TomasMikolov,KaiChen,Greg
Corrado,andJeffreyDean.2013.Efcientestimation

ofwordrepresentationsinvectorspace.
arXivpreprint
arXiv:1301.3781
.
[PangandLee2008]BoPangandLillianLee.2008.
Opinionminingandsentimentanalysis.
Foundations
andtrendsininformationretrieval
.
[Schutze1992]HinrichSchutze.1992.Dimensionsof
meaning.In
Supercomputing
.
[Tangetal.2015]DuyuTang,BingQin,andTingLiu.
2015.Documentmodelingwithgatedrecurrentneural

networkforsentimentclassication.In
EMNLP
.
[Thomeeetal.2016]BartThomee,DavidAShamma,
GeraldFriedland,BenjaminElizalde,KarlNi,Dou-

glasPoland,DamianBorth,andLi-JiaLi.2016.

Yfcc100m:Thenewdatainmultimediaresearch.vol-

ume59,pages64Œ73.ACM.
[WangandManning2012]SidaWangandChristopherD
Manning.2012.Baselinesandbigrams:Simple,good

sentimentandtopicclassication.In
ACL
.
[Weinbergeretal.2009]KilianWeinberger,AnirbanDas-
gupta,JohnLangford,AlexSmola,andJoshAtten-

berg.2009.Featurehashingforlargescalemultitask

learning.In
ICML
.
[Westonetal.2011]JasonWeston,SamyBengio,and
NicolasUsunier.2011.Wsabie:Scalinguptolarge

vocabularyimageannotation.In
IJCAI
.
[Westonetal.2014]JasonWeston,SumitChopra,and
KeithAdams.2014.#tagspace:Semanticembed-

dingsfromhashtags.In
EMNLP
.
[XiaoandCho2016]YijunXiaoandKyunghyunCho.
2016.Efcientcharacter-leveldocumentclassication

bycombiningconvolutionandrecurrentlayers.
arXiv
preprintarXiv:1602.00367
.
[ZhangandLeCun2015]XiangZhangandYannLeCun.
2015.Textunderstandingfromscratch.
arXivpreprint
arXiv:1502.01710
.
[Zhangetal.2015]XiangZhang,JunboZhao,andYann
LeCun.2015.Character-levelconvolutionalnetworks

fortextclassication.In
NIPS
.
