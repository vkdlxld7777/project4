arXiv:1409.2329v5  [cs.NE]  19 Feb 2015
UnderreviewasaconferencepaperatICLR2015
R
ECURRENT
N
EURAL
N
ETWORK
R
EGULARIZATION
WojciechZaremba

NewYorkUniversity

woj.zaremba@gmail.com

IlyaSutskever,OriolVinyals

GoogleBrain

f
ilyasu,vinyals
g
@google.com
A
BSTRACT
WepresentasimpleregularizationtechniqueforRecurrent
NeuralNetworks
(RNNs)withLongShort-TermMemory(LSTM)units.Dropout,t
hemostsuc-
cessfultechniqueforregularizingneuralnetworks,doesn
otworkwellwithRNNs
andLSTMs.Inthispaper,weshowhowtocorrectlyapplydropo
uttoLSTMs,
andshowthatitsubstantiallyreducesoverttingonavarie
tyoftasks.Thesetasks
includelanguagemodeling,speechrecognition,imagecapt
iongeneration,and
machinetranslation.

1I
NTRODUCTION
TheRecurrentNeuralNetwork(RNN)isneuralsequencemodel
thatachievesstateoftheartper-
formanceonimportanttasksthatincludelanguagemodeling
Mikolov(2012),speechrecognition
Gravesetal.(2013),andmachinetranslationKalchbrenner
&Blunsom(2013).Itisknownthat
successfulapplicationsofneuralnetworksrequiregoodre
gularization.Unfortunately,dropout
Srivastava(2013),themostpowerfulregularizationmetho
dforfeedforwardneuralnetworks,does
notworkwellwithRNNs.Asaresult,practicalapplications
ofRNNsoftenusemodelsthatare
toosmallbecauselargeRNNstendtoovert.Existingregula
rizationmethodsgiverelativelysmall
improvementsforRNNsGraves(2013).Inthiswork,weshowth
atdropout,whencorrectlyused,
greatlyreducesoverttinginLSTMs,andevaluateitonthre
edifferentproblems.
Thecodeforthisworkcanbefoundin
https://github.com/wojzaremba/lstm
.
2R
ELATEDWORK
DropoutSrivastava(2013)isarecentlyintroducedregular
izationmethodthathasbeenverysuc-
cessfulwithfeed-forwardneuralnetworks.Whilemuchwork
hasextendeddropoutinvariousways
Wang&Manning(2013);Wanetal.(2013),therehasbeenrelat
ivelylittleresearchinapplyingit
toRNNs.TheonlypaperonthistopicisbyBayeretal.(2013),
whofocusesonÔ¨Åmarginalized
dropoutÔ¨ÇWang&Manning(2013),anoiselessdeterministica
pproximationtostandarddropout.
Bayeretal.(2013)claimthatconventionaldropoutdoesnot
workwellwithRNNsbecausethere-
currenceampliesnoise,whichinturnhurtslearning.Inth
iswork,weshowthatthisproblemcan
bexedbyapplyingdropouttoacertainsubsetoftheRNNs'co
nnections.Asaresult,RNNscan
nowalsobenetfromdropout.

Independentlyofourwork,Phametal.(2013)developedthev
erysameRNNregularizationmethod
andappliedittohandwritingrecognition.Werediscovered
thismethodanddemonstratedstrong
empiricalresultsoverawide rangeofproblems.Otherworkt
hatapplieddropouttoLSTMsis
Pachitariu&Sahani(2013).

WorkdonewhiletheauthorwasinGoogleBrain.
1
UnderreviewasaconferencepaperatICLR2015
TherehavebeenanumberofarchitecturalvariantsoftheRNN
thatperformbetteronproblemswith
longtermdependenciesHochreiter&Schmidhuber(1997);Gr
avesetal.(2009); Choetal.(2014);
Jaegeretal.(2007);Koutn¬¥ketal.(2014);Sundermeyeret
al.(2012).Inthiswork,weshowhow
tocorrectlyapplydropouttoLSTMs,themostcommonly-used
RNNvariant;thiswayofapplying
dropoutislikelytoworkwellwithotherRNNarchitecturesa
swell.
Inthispaper,we considerthefollowingtasks:languagemod
eling,speechrecognition,andma-
chinetranslation.Languagemodelingisthersttaskwhere
RNNshaveachievedsubstantialsuc-
cessMikolovetal.(2010;2011);Pascanuetal.(2013).RNNs
havealsobeensuccessfullyused
forspeechrecognitionRobinsonetal.(1996);Gravesetal.
(2013)andhaverecentlybeenapplied
tomachinetranslation,wheretheyareusedforlanguagemod
eling,re-ranking,orphrasemodel-
ingDevlinetal.(2014);Kalchbrenner&Blunsom(2013);Cho
etal.(2014);Chowetal.(1987);
Mikolovetal.(2013).
3R
EGULARIZING
RNN
SWITH
LSTM
CELLS
InthissectionwedescribethedeepLSTM(Section3.1).Next
,weshowhowtoregularizethem
(Section3.2),andexplainwhyourregularizationschemewo
rks.
Weletsubscriptsdenotetimestepsandsuperscriptsdenote
layers.Allourstatesare
n
-dimensional.
Let
h
l

t
2
R
n
beahiddenstateinlayer
l
intimestep
t
.Moreover,let
T
n;m
:
R
n
!
R
m
beanafne
transform(
Wx
+
b
forsome
W
and
b
).Let

beelement-wisemultiplicationandlet
h
0

t
beaninput
wordvectorattimestep
k
.Weusetheactivations
h
L

t
topredict
y
t
,since
L
isthenumberoflayers
inourdeepLSTM.
3.1L
ONG
-
SHORTTERMMEMORYUNITS
TheRNNdynamicscanbedescribedusingdeterministictrans
itionsfromprevioustocurrenthidden
states.Thedeterministicstatetransitionisafunction
RNN
:
h
l

1
t
;h
l

t

1
!
h
l

t
ForclassicalRNNs,thisfunctionisgivenby
h
l

t
=
f
(
T
n;n
h
l

1
t
+
T
n;n
h
l

t

1
)
,where
f
2f
sigm
;
tanh
g
TheLSTMhascomplicateddynamicsthatallowittoeasilyÔ¨Åme
morizeÔ¨Çinformationforanextended
numberoftimesteps.TheÔ¨ÅlongtermÔ¨Çmemoryisstoredinavec
torof
memorycells
c
l

t
2
R
n
.Al-
thoughmanyLSTMarchitecturesthatdifferintheirconnect
ivitystructureandactivationfunctions,
allLSTMarchitectureshaveexplicitmemorycellsforstori
nginformationforlongperiodsoftime.
TheLSTMcandecidetooverwritethememorycell,retrieveit
,orkeepitforthenexttimestep.The
LSTMarchitectureusedinourexperimentsisgivenbythefol
lowingequationsGravesetal.(2013):
LSTM
:
h
l

1
t
;h
l

t

1
;c
l

t

1
!
h
l

t
;c
l

t
0

B

@
i
f
o
g
1

C

A
=
0

B

@
sigm

sigm

sigm

tanh
1

C

A
T
2
n;
4
n

h
l

1
t
h
l

t

1

c
l

t
=
f

c
l

t

1
+
i

g
h
l

t
=
o

tanh(
c
l

t
)
Intheseequations,
sigm
and
tanh
areappliedelement-wise.Figure1illustratestheLSTMequ
a-
tions.
3.2R
EGULARIZATIONWITH
D
ROPOUT
Themaincontributionofthispaperisarecipeforapplyingd
ropouttoLSTMsinawaythatsuccess-
fullyreducesovertting.Themainideaistoapplythedropo
utoperatoronlytothenon-recurrent
2
UnderreviewasaconferencepaperatICLR2015


c
t
Cell
f



f
Forgetgate
6
Àö
	


h
l

t

1
A
AK
h
l

1
t


i
Input
gate
AU
h
l

t

1


h
l

1
t


o
Output
gate
AU
h
l

t

1


h
l

1
t


g
Input
modulation
gate
f

-
-
J
J
J^
f

-
-
?
h
l

t
h
l

t

1
h
l

1
t
Àò
Àò:
X
Xz
Figure1:AgraphicalrepresentationofLSTMmemorycellsus
edinthispaper(thereareminor
differencesincomparisontoGraves(2013)).
-
-
-
-
-
-
-
-
-
-
-
-
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
x
t

2
x
t

1
x
t
x
t
+1
x
t
+2
y
t

2
y
t

1
y
t
y
t
+1
y
t
+2
Figure2:RegularizedmultilayerRNN.Thedashedarrowsind
icateconnectionswheredropoutis
applied,andthesolidlinesindicateconnectionswheredro
poutisnotapplied.
connections(Figure2).Thefollowingequationdescribesi
tmoreprecisely,where
D
isthedropout
operatorthatsetsarandomsubsetofitsargumenttozero:
0

B

@
i
f
o
g
1

C

A
=
0

B

@
sigm

sigm

sigm

tanh
1

C

A
T
2
n;
4
n

D
(
h
l

1
t
)
h
l

t

1

c
l

t
=
f

c
l

t

1
+
i

g
h
l

t
=
o

tanh(
c
l

t
)
Ourmethodworksasfollows.Thedropoutoperatorcorruptst
heinformationcarriedbytheunits,
forcingthemtoperformtheirintermediatecomputationsmo
rerobustly.Atthesametime,wedonot
wanttoerasealltheinformationfromtheunits.Itisespeci
allyimportantthattheunitsremember
eventsthatoccurredmanytimestepsinthepast.Figure3sho
wshowinformationcouldowfrom
aneventthatoccurredattimestep
t

2
tothepredictionintimestep
t
+2
inourimplementationof
dropout.Wecanseethattheinformationiscorruptedbythed
ropoutoperatorexactly
L
+1
times,
3
UnderreviewasaconferencepaperatICLR2015
-
-
-
-
-
-
-
-
-
-
-
-
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
x
t

2
x
t

1
x
t
x
t
+1
x
t
+2
y
t

2
y
t

1
y
t
y
t
+1
y
t
+2
Figure3:Thethicklineshowsatypicalpathofinformation
owintheLSTM.Theinformationis
affectedbydropout
L
+1
times,where
L
isdepthofnetwork.
themeaningoflifeis
thatonlyifanendwouldbeofthewholesupplier.widespread
rulesarere-
gardedasthecompaniesofrefusestodeliver.inbalanceoft
henation'sinformationandloan
growth associatedwiththecarrierthriftsareintheproces
sofslowingtheseed and commercialpaper.
themeaningoflifeis
nearlyintherstseveralmonthsbeforethegovernmentwasa
ddressingsuchamoveas
presidentand chiefexecutiveofthenationpastfromanatio
nalcommitmenttocurbgrounds.meanwhilethe
governmentinvestsovercapacitythatcriticismandintheo
uterreversalofsmall-townamerica.
Figure4:Someinterestingsamplesdrawnfromalargeregula
rizedmodelconditionedonÔ¨ÅThe
meaningoflifeisÔ¨Ç.WehaveremovedÔ¨ÅunkÔ¨Ç,Ô¨ÅNÔ¨Ç,Ô¨Å$Ô¨Çfromthes
etofpermissiblewords.
andthisnumberisindependentofthenumberoftimestepstra
versedbytheinformation.Standard
dropoutperturbstherecurrentconnections,whichmakesit
difcultfortheLSTMtolearntostore
informationforlongperiodsoftime.Bynotusingdropouton
therecurrentconnections,theLSTM
canbenetfromdropoutregularizationwithoutsacricing
itsvaluablememorizationability.
4E
XPERIMENTS
Wepresentresultsinthreedomains:languagemodeling(Sec
tion4.1),speechrecognition(Section
4.2),machinetranslation(Section4.3),andimagecaption
generation(Section4.4).
4.1L
ANGUAGEMODELING
Weconductedword-levelpredictionexperimentsonthePenn
TreeBank(PTB)datasetMarcusetal.
(1993),whichconsistsof
929
ktrainingwords,
73
kvalidationwords,and
82
ktestwords.Ithas
10
k
wordsinitsvocabulary.WedownloadeditfromTomasMikolov
'swebpage
y
.Wetrainedregularized
LSTMsoftwosizes;thesearedenotedthemediumLSTMandlarg
eLSTM.BothLSTMshave
twolayersandareunrolledfor
35
steps.Weinitializethehiddenstatestozero.Wethenuseth
e
nalhiddenstatesofthecurrentminibatchastheinitialhi
ddenstateofthesubsequentminibatch
(successiveminibatchessequentiallytraversethetraini
ngset).Thesizeofeachminibatchis20.
y
http://www.fit.vutbr.cz/
≈∏
imikolov/rnnlm/simple-examples.tgz
4
UnderreviewasaconferencepaperatICLR2015
Model ValidationsetTestset
Asinglemodel
Pascanuetal.(2013)107.5

Chengetal.100.0

non-regularizedLSTM120.7114.5

MediumregularizedLSTM86.282.7

LargeregularizedLSTM82.2
78.4
Modelaveraging
Mikolov(2012)83.5

Chengetal.80.6

2non-regularizedLSTMs100.496.1

5non-regularizedLSTMs87.984.1

10non-regularizedLSTMs83.580.0

2mediumregularizedLSTMs80.677.0

5mediumregularizedLSTMs76.773.3

10mediumregularizedLSTMs75.272.0

2largeregularizedLSTMs76.973.6

10largeregularizedLSTMs72.869.5

38largeregularizedLSTMs71.9
68.7
ModelaveragingwithdynamicRNNsandn-grammodels
Mikolov&Zweig(2012)72.9
Table1:Word-levelperplexityonthePennTreeBankdataset
.
ThemediumLSTMhas
650
unitsperlayeranditsparametersareinitializeduniforml
yin
[

0
:
05
;
0
:
05]
.Asdescribedearlier,weapply
50%
dropoutonthenon-recurrentconnections.We
traintheLSTMfor
39
epochswithalearningrateof
1
,andafter
6
epochswedecreaseitbyafactor
of
1
:
2
aftereachepoch.Weclipthenormofthegradients(normaliz
edbyminibatchsize)at
5
.
TrainingthisnetworktakesabouthalfadayonanNVIDIAK20G
PU.
ThelargeLSTMhas
1500
unitsperlayeranditsparametersareinitializeduniforml
yin
[

0
:
04
;
0
:
04]
. Weapply
65%
dropoutonthenon-recurrentconnections.Wetrainthemode
lfor
55
epochswithalearningrateof
1
;after
14
epochswestarttoreducethelearningratebyafactor
of
1
:
15
aftereachepoch.Weclipthenormofthegradients(normaliz
edbyminibatchsize)at
10
Mikolovetal.(2010).Trainingthisnetworktakesanentire
dayonanNVIDIAK20GPU.
Forcomparison,wetrainedanon-regularizednetwork.Weop
timizeditsparameterstogetthebest
validationperformance.Thelackofregularizationeffect
ivelyconstrainssizeofthenetwork,forc-
ingustousesmallnetworkbecauselargernetworksovert.O
urbestperformingnon-regularized
LSTMhastwohiddenlayerswith
200
unitsperlayer,anditsweightsareinitializeduniformlyi
n
[

0
:
1
;
0
:
1]
.Wetrainitfor
4
epochswithalearningrateof
1
andthenwedecreasethelearningrate
byafactorof
2
aftereachepoch,foratotalof
13
trainingepochs.Thesizeofeachminibatchis
20
,
andweunrollthenetworkfor
20
steps.Trainingthisnetworktakes2-3hoursonanNVIDIAK20
GPU.

Table1comparespreviousresultswithourLSTMs,andFigure
4showssamplesdrawnfromasingle
largeregularizedLSTM.
4.2S
PEECHRECOGNITION
DeepNeuralNetworkshavebeenusedforacousticmodelingfo
roverhalfacentury(see
Bourlard&Morgan(1993)foragoodreview).Acousticmodeli
ngisakeycomponentinmap-
pingacousticsignalstosequencesofwords,asitmodels
p
(
s
t
j
X
)
where
s
t
isthephoneticstateat
time
t
and
X
istheacousticobservation.RecentworkhasshownthatLSTM
scanachieveexcellent
performanceonacousticmodelingSaketal.(2014),yetrela
tivelysmallLSTMs(intermsofthe
numberoftheirparameters)caneasilyovertthetrainings
et.Ausefulmetricformeasuringthe
performanceofacousticmodelsisframeaccuracy,whichism
easuredateach
s
t
foralltimesteps
t
.Generally,thismetriccorrelateswiththeactualmetrico
finterest,theWordErrorRate(WER).
5
UnderreviewasaconferencepaperatICLR2015
ModelTrainingsetValidationset
Non-regularizedLSTM71.668.9

RegularizedLSTM69.4
70.5
Table2:Frame-levelaccuracyontheIcelandicSpeechDatas
et.Thetrainingsethas93kutterances.
ModelTestperplexityTestBLEUscore
Non-regularizedLSTM5.825.9

RegularizedLSTM5.029.03
LIUMsystem33.30
Table3:ResultsontheEnglishtoFrenchtranslationtask.
SincecomputingtheWERinvolvesusingalanguagemodelandt
uningthedecodingparametersfor
everychangeintheacousticmodel,wedecidedtofocusonfra
meaccuracyintheseexperiments.
Table2showsthatdropoutimprovestheframeaccuracyofthe
LSTM.Notsurprisingly,thetraining
frameaccuracydropsduetothenoiseaddedduringtraining,
butasisoftenthecasewithdropout,
thisyieldsmodelsthatgeneralizebettertounseendata.No
tethatthetestsetiseasierthanthetrain-
ingset,asitsaccuracyishigher.Wereporttheperformance
ofanLSTMonaninternalGoogle
IcelandicSpeechdataset,whichisrelativelysmall(93kut
terances),sooverttingisagreatconcern.
4.3M
ACHINETRANSLATION
Weformulateamachinetranslationproblemasalanguagemod
ellingtask,whereanLSTMistrained
toassignhighprobabilitytoacorrecttranslationofasour
cesentence.Thus,theLSTMistrainedon
concatenationsofsourcesentencesandtheirtranslations
Sutskeveretal.(2014)(seealsoChoetal.
(2014)).Wecomputeatranslationbyapproximatingthemost
probablesequenceofwordsusinga
simplebeamsearchwithabeamofsize12.WerananLSTMontheW
MT'14EnglishtoFrench
dataset,ontheÔ¨ÅselectedÔ¨ÇsubsetfromSchwenk(2014)which
has340MFrenchwordsand304M
Englishwords.OurLSTMhas4hiddenlayers,andbothitslaye
rsandwordembeddingshave
1000units.ItsEnglishvocabularyhas160,000wordsandits
Frenchvocabularyhas80,000words.
Theoptimaldropoutprobabilitywas0.2.Table3showsthepe
rformanceofanLSTMtrained
withandwithoutdropout.WhileourLSTMdoesnotbeatthephr
ase-basedLIUMSMTsystem
Schwenketal.(2011),ourresultsshowthatdropoutimprove
sthetranslationperformanceofthe
LSTM.
4.4I
MAGE
C
APTION
G
ENERATION
Weappliedthedropoutvarianttotheimagecaptiongenerati
onmodelofVinyalsetal.(2014).The
imagecaptiongenerationissimilartothesequence-to-seq
uencemodelofSutskeveretal.(2014),
butwheretheinputimageismappedontoavectorwithahighly
-accuratepre-trainedconvolutional
neuralnetwork(Szegedyetal.,2014),whichisconvertedin
toacaptionwithasingle-layerLSTM
(seeVinyalsetal.(2014)forthedetailsonthearchitectur
e).WetestourdropoutschemeonLSTM
astheconvolutionalneuralnetworkisnottrainedontheima
gecaptiondatasetbecauseitisnotlarge
(MSCOCO(Linetal.,2014)).

OurresultsaresummarizedinthefollowingTable4.Inbrief
,dropouthelpsrelativetonotusing
dropout,butusinganensembleeliminatesthegainsattaine
dbydropout.Thus,inthissetting,the
maineffectofdropoutistoproduceasinglemodelthatisasg
oodasanensemble,whichisa
reasonableimprovementgiventhesimplicityofthetechniq
ue.
5C
ONCLUSION
Wepresenteda simple wayofapplyingdropouttoLSTMsthatre
sultsinlargeperformancein-
creasesonseveralproblemsindifferentdomains.Ourworkm
akesdropoutusefulforRNNs,and
ourresultssuggestthatourimplementationofdropoutcoul
dimproveperformanceonawidevariety
ofapplications.
6
UnderreviewasaconferencepaperatICLR2015
ModelTestperplexityTestBLEUscore
Non-regularizedmodel 8.4723.5

Regularizedmodel7.9924.3
10non-regularizedmodels7.524.4
Table4:Resultsontheimagecaptiongenerationtask.
6A
CKNOWLEDGMENTS
WewishtoacknowledgeTomasMikolovforusefulcommentsont
herstversionofthepaper.
R
EFERENCES
Bayer,Justin,Osendorfer,Christian,Chen,Nutan,Urban,
Sebastian,andvanderSmagt,Patrick.Onfast
dropoutanditsapplicabilitytorecurrentnetworks.
arXivpreprintarXiv:1311.0701
,2013.
Bourlard,H.andMorgan,N.
ConnectionistSpeechRecognition:AHybridApproach
.KluwerAcademic
Publishers,1993.
Cheng,Wei-Chen,Kok,Stanley,Pham,HoaiVu,Chieu,HaiLeo
ng,andChai,KianMingA.Language
modelingwithsum-productnetworks.
Cho,Kyunghyun,vanMerrienboer,Bart,Gulcehre,Caglar,B
ougares,Fethi,Schwenk,Holger,andBengio,
Yoshua.Learningphraserepresentationsusingrnnencoder
-decoderforstatisticalmachinetranslation.
arXiv
preprintarXiv:1406.1078
,2014.
Chow,Y,Dunham,M,Kimball,O,Krasner,M,Kubala,G,Makhou
l,J,Price,P,Roucos,S,andSchwartz,R.
Byblos:Thebbncontinuousspeechrecognitionsystem.In
Acoustics,Speech,andSignalProcessing,IEEE
InternationalConferenceonICASSP'87.
,volume12,pp.89≈í92.IEEE,1987.
Devlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,and
Makhoul,J.Fastandrobustneuralnetworkjoint
modelsforstatisticalmachinetranslation.In
ACL
,2014.
Graves,Alex.Generatingsequenceswithrecurrentneuraln
etworks.
arXivpreprintarXiv:1308.0850
,2013.
Graves,Alex,Liwicki,Marcus,Fern¬¥andez,Santiago,Bert
olami,Roman,Bunke,Horst,andSchmidhuber,
J¬®urgen.Anovelconnectionistsystemforunconstrainedha
ndwritingrecognition.
PatternAnalysisand
MachineIntelligence,IEEETransactionson
,31(5):855≈í868,2009.
Graves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey
.Speechrecognitionwithdeeprecurrentneural
networks.In
Acoustics,SpeechandSignalProcessing(ICASSP),2013IEE
EInternationalConferenceon
,
pp.6645≈í6649.IEEE,2013.
Hochreiter,Seppand Schmidhuber,J¬®urgen.Longshort-ter
mmemory.
Neuralcomputation
,9(8):1735≈í1780,
1997.
Jaeger, Herbert,Lukosevicius,Mantas,Popovici,Dan,a
ndSiewert,Udo.Optimizationandapplicationsof
echostatenetworkswithleaky-integratorneurons.
NeuralNetworks
,20(3):335≈í352,2007.
Kalchbrenner,N.andBlunsom,P.Recurrentcontinuoustran
slationmodels.In
EMNLP
,2013.
Koutn¬¥k,Jan,Greff, Klaus,Gomez,Faustino,andSchmidhu
ber,J¬®urgen.Aclockworkrnn.
arXivpreprint
arXiv:1402.3511
,2014.
Lin,Tsung-Yi,Maire,Michael,Belongie,Serge,Hays,Jame
s,Perona,Pietro,Ramanan,Deva,Doll¬¥ar,Piotr,
andZitnick,CLawrence.Microsoftcoco:Commonobjectsinc
ontext.
arXivpreprintarXiv:1405.0312
,
2014.
Marcus,MitchellP,Marcinkiewicz,MaryAnn,andSantorini
,Beatrice.Buildingalargeannotatedcorpusof
english:Thepenntreebank.
Computationallinguistics
,19(2):313≈í330,1993.
Mikolov,Tom¬¥as.
Statisticallanguagemodelsbasedonneuralnetworks
.PhDthesis,Ph.D.thesis,Brno
UniversityofTechnology,2012.
Mikolov,Tomasand Zweig,Geoffrey.Contextdependentrecu
rrentneuralnetworklanguagemodel.In
SLT
,
pp.234≈í239,2012.
7
UnderreviewasaconferencepaperatICLR2015
Mikolov,Tomas,Kara¬¥at,Martin,Burget,Lukas,CernockÀö
y,Jan,andKhudanpur,Sanjeev.Recurrentneural
networkbasedlanguagemodel.In
INTERSPEECH
,pp.1045≈í1048,2010.
Mikolov,Tomas,Deoras,Anoop,Povey,Daniel,Burget,Luka
s,andCernocky,Jan.Strategiesfortraininglarge
scaleneuralnetworklanguagemodels.In
AutomaticSpeechRecognitionandUnderstanding(ASRU),20
11
IEEEWorkshopon
,pp.196≈í201.IEEE,2011.
Mikolov,Tomas,Le,QuocV,andSutskever,Ilya.Exploiting
similaritiesamonglanguagesformachinetrans-
lation.
arXivpreprintarXiv:1309.4168
,2013.
Pachitariu,MariusandSahani,Maneesh.Regularizationan
dnonlinearitiesforneurallanguagemodels:when
aretheyneeded?
arXivpreprintarXiv:1301.5650
,2013.
Pascanu,Razvan,Gulcehre,Caglar,Cho,Kyunghyun,andBen
gio,Yoshua.Howtoconstructdeeprecurrent
neuralnetworks.
arXivpreprintarXiv:1312.6026
,2013.
Pham,Vu,Kermorvant,Christopher,andLouradour,J¬¥er‚Äπom
e.Dropoutimprovesrecurrentneuralnetworksfor
handwritingrecognition.
arXivpreprintarXiv:1312.4569
,2013.
Robinson,Tony,Hochberg,Mike,andRenals,Steve.Theuseo
frecurrentneuralnetworksincontinuousspeech
recognition.In
Automaticspeechandspeakerrecognition
,pp.233≈í258.Springer,1996.
Sak,H.,Vinyals,O.,Heigold,G.,Senior,A.,McDermott,E.
,Monga,R.,andMao,M.Sequencediscriminative
distributedtrainingoflongshort-termmemoryrecurrentn
euralnetworks.In
Interspeech
,2014.
Schwenk,Holger.Universitylemans,2014.
http://www-lium.univ-lemans.fr/
≈∏
schwenk/cslm_joint/paper
.
Schwenk,Holger,Lambert,Patrik,Barrault,Lo¬®c,Servan
,Christophe,Ai,Haithem,Abdul-Rauf,Sadaf,and
Shah,Kashif.Lium's smtmachinetranslationsystemsforwm
t2011.In
ProceedingsoftheSixthWorkshop
onStatisticalMachineTranslation
,pp.464≈í469.AssociationforComputationalLinguistics,
2011.
Srivastava,Nitish.
Improvingneuralnetworkswithdropout
.PhDthesis,UniversityofToronto,2013.
Sundermeyer, Martin,Schl¬®uter,Ralf,andNey,Hermann.Ls
tmneural networksforlanguagemodeling.In
INTERSPEECH
,2012.
Sutskever,Ilya,Vinyals,Oriol,andLe,QuocVV.Sequencet
osequencelearningwithneuralnetworks.In
AdvancesinNeuralInformationProcessingSystems
,pp.3104≈í3112,2014.
Szegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet,Pie
rre,Reed,Scott,Anguelov,Dragomir,Erhan,Du-
mitru,Vanhoucke,Vincent,andRabinovich,Andrew.Goingd
eeperwithconvolutions.
arXivpreprint
arXiv:1409.4842
,2014.
Vinyals,Oriol,Toshev,Alexander,Bengio,Samy,andErhan
,Dumitru.Show andtell:A neuralimagecaption
generator.
arXivpreprintarXiv:1411.4555
,2014.
Wan,Li,Zeiler,Matthew,Zhang,Sixin,Cun,YannL,andFerg
us,Rob.Regularizationofneuralnetworks
usingdropconnect.In
Proceedingsofthe30thInternationalConferenceonMachin
eLearning(ICML-13)
,
pp.1058≈í1066,2013.
Wang,SidaandManning,Christopher.Fastdropouttraining
.In
Proceedingsofthe30thInternationalConfer-
enceonMachineLearning(ICML-13)
,pp.118≈í126,2013.
8
