BIRCH:
An
Efficient
Data
Clustering
Method
for
Very
Large
Databases
Tian
Zhang
Raghu
Ramakrishnan
Miron
Livnyﬂ
(ﬁlornputer
Sciences
Dept.
Computer
Sciences
Dept.
Computer
Sciences
Dept.
[Jniv.
of
Wisconsin-Maciison
[Jniv.
of
Wisconsin-Maciison
LJniv.
of
Wisconsin-Maclison
zhang@cs.
wise.edu
raghuf~cs.wise.edu
mironf~cs.
wise.eclu
Abstract
Finding
useful
patterns
in
large
datasets
has
attracted
considerable
interest
recently,
and
one
of
the
most
widely
st,udied
problems
in
this
area
is
the
identification
of
clusters,
or
deusel
y
populated
regions,
in
a
multi-dir
nensi
onal
clataset.
Prior
work
does
not
adequately
address
the
problem
of
large
datasets
and
minimization
of
1/0
costs.
This
paper
presents
a
data
clustering
method
named
Bfll
(;ﬂH
(Balanced
Iterative
Reducing
and
Clustering
using
Hierarchies),
and
demonstrates
that
it
is
especially
suitable
for
very
large
databases.
BIRCH
incrementally
and
clynami-
call
y
clusters
incoming
multi-dimensional
metric
data
points
to
try
to
produce
the
best
quality
clustering
with
the
avail-
able
resources
(i.
e.,
available
memory
and
time
constraints).
BIRCH
can
typically
find
a
goocl
clustering
with
a
single
scan
of
the
data,
and
improve
the
quality
further
with
a
few
acl-
ditioual
scans.
BIRCH
is
also
the
first
clustering
algorithm
proposerl
in
the
database
area
to
handle
ﬁnoise)™
(data
points
that
are
not
part
of
the
underlying
pattern)
effectively.
We
evaluate
BIRCH™S
time/space
efficiency,
data
input
order
sensitivity,
and
clustering
quality
through
several
experiments.
We
also
present
a
performance
comparisons
of
BIR
(;™H
versus
CLARA
NS,
a
clustering
method
proposed
recently
for
large
datasets,
and
S11OW
that
BIRCH
is
consistently
superior.
1
Introduction
In
this
paper,
we
examine
data
clustering,
which
is
a
particular
kind
of
clatla
mining
problem.
(~iven
a
large
set
of
rnulti-clirnensional
data
points,
the
data
spare
is
usually
not
uniformly
occupied.
Data
clustering
identifies
the
sparse
and
the
crowded
places,
and
hence
discovers
the
overall
distribution
patterns
of
the
dataset.
Besides,
the
derived
clusters
can
be
visualized
more
efficiently
and
effectively
than
the
original
dataset[Lee81,
D,J80].
*This
research
has
been
supported
by
NSF
Grant
IRI-9057562
and
NASA
(;rant
144-EC
78.
Permission
to
make
digitalhard
copy
of
part
or
all
of
this
work
for
personal
or
classroom
use
is
ranted
without
fee
provided
that
copies
are
not
made
7
or
distributed
for
pro
tt
or
commercial
advantage,
the
aopyright
notice,
the
title
of
the
publication
and
its
date
appear,
and
notice
is
given
that
copying
is
by
permission
of
ACM,
Inc.
To
copy
otherwise,
to
republish,
to
post
on
servers,
or
to
redistribute
to
lists,
requires
prior
specific
permission
andlor
a
fee.
SIGMOD
™96
6/96
Montreal,
Canada
IQ
1996
ACM
0-89791
-794-4/96/0006
.
..$3.50
Generally,
there
are
two
types
of
attributes
involved
in
the
data
to
be
clustered:
metrtc
and
nonmetrzri.
ln
this
paper,
we
consider
metric
attributes,
as
in
most,
of
the
Statistics
literature,
where
the
clustering
prol>-
lern
is
formalized
as
follows:
G™tven
the
destred
rlum-
ber
of
clusters
K
and
a
dataset
of
N
potnts,
and
a
dzstance-based
measurement
functzon
(e.g.,
the
uletghted
totrd/average
dwtunce
betuleeri
pazr-s
of
pozrtts
tn
clus-
ters),
rue
are
asked
to
find
a
partatzon
of
the
dataset
that
rrl~nrmizes
the
value
of
thf
measurement
functton.
This
is
a
nonconvm
dtscrete
[KR90]
optimization
problem.
Due
to
an
abundance
of
local
minima,
there
is
typically
no
way
to
find
a
global
minimal
solution
without
trying
all
possible
partitions.
We
adopt,
the
problem
definition
used
in
Statistics,
but
with
an
additional,
database-oriented
constraint,:
The
amount
of
memory
available
M
ltrntted
(t~yptcall~y,
much
-smaller
than
the
data
set
.sw~)
and
ule
rvant
to
rninzmt.ze
the
tzrne
required
for
1/0.
A
related
point,
is
that
it
is
desirable
to
Lre
able
to
take
into
account
the
amount
of
ttme
that
a
user
is
willing
to
wait
for
the
results
of
the
clustering
algorithm.
We
present
a
clustering
method
named
BIRCH
and
demonstrate
that
it
is
especially
suitable
for
very
large
databases.
Its
1/(>
cost
is
linear
in
the
size
of
the
dataset:
a
.szngle
scan
of
the
dataset
yields
a
good
clustering,
ancl
one
or
more
additional
passes
can
(optionally)
be
used
to
improve
the
quality
further.
By
evaluating
BIRCH™S
time/space
eficieucy,
data
ill-
put
order
sensitivity,
and
clustering
quality,
and
com-
paring
with
other
existing
algorithms
through
experi-
ments,
we
argue
that
BIRCH
is
the
best
available
clus-
tering
method
for
very
large
databases.
BIRCIPs
ar-
chitecture
also
offers
opportunities
for
parallelism,
and
for
interactive
or
dynamic
performance
tuning
based
on
knowledge
about
the
ciataset,
gained
over
the
course
of
the
execution.
Finally,
BIRCH
is
the
first
clustering
al-
1
Informally,
a
metmc
attribute
is
an
attribote
whose
values
satisfy
the
requirements
of
Eucltdtan
space,
i.e.,
self
identity
(for
any
X,
X
=
X)
and
triangular
inequality
(there
exists
a
distance
definition
such
that
for
any
XI
,XZ,X3,
d(XI
,
X2)
+
d(X2,
X3)
~
[1(X1,X3)).
103
goritlhm
proposed
in
the
datlahase
area
that
addresses
outl~~rs
(intuitively,
data
points
that,
should
be
regarded
a,s
ﬁnoispi™
)
and
proposes
a
plausible
solution.
1.1
Outline
of
Paper
The
rest
of
the
paper
is
organized
as
follows.
Sec.
2
surveys
relat,ed
work
ancl
summarizes
BIRCH™S
contri-
l)utlunh
Sec.
3
presents
some
background
material.
Ser.
4
introduces
the
concepts
of
clustering
feature
((~F)
au(i
(
~F
tree,
which
are
central
to
BIRCH.
The
details
of
BIll(™H
algorithm
is
described
in
Sec.
5,
and
a
pre-
liminary
performance
study
of
BIRCH
is
presented
in
Sec.
6,
Finally
our
conclusions
and
directions
for
fw
ture
rmearch
are
presented
in
Sec.
7,
2
Summary
of
Relevant
Research
Data
clustering
has
been
studied
in
the
Statistics
[DHi™3,
D.J80,
Lee81,
Mur83],
Machine
Learning
[CKS88,
Fis87,
Fis95,
Lel>87]
and
Database
[NH94,
EKX95a,
E1iX951]]
communities
with
different,
methods
and
dif-
ferent
emphases,
Previous
approaches,
probability-
I)ased
(like
most
approaches
in
Machine
Learning)
or
distauce-basecl
(like
most
work
in
Statistics)
,
C1O
not,
adequately
consider
the
case
that,
the
clataset
can
he
too
large
to
fit
in
main
memory.
In
particular,
they
do
not
rerognize
that,
the
problem
must
be
viewed
in
terms
of
how
to
work
with
a
limited
resources
(e.g.,
memory
that,
is
typirally,
mu[-h
smaller
than
the
size
of
the
dataset)
to
do
the
clustering
as
accurately
as
possible
while
keeping
the
1/()
costs
low.
Probability-based
approaches:
They
typically
[FisH7,
(
‚KSt38]
make
the
assumption
that
probahilit,y
distributions
on
separate
attributes
are
statistically
independent
of
each
other,
In
reality,
this
is
far
from
true.
(
correlation
between
attributes
exists,
and
sometimes
this
kind
of
correlation
is
exactly
what
we
are
looking
for.
The
probability
representations
of
c-lusters
make
updating
and
storing
the
clusters
very
expensive,
eslxv-ially
if
the
attributes
have
a
large
number
of
values
because
their
complexities
are
dependent
not,
only
on
the
number
of
attributes,
but
also
on
the
number
of
values
for
each
attribute.
A
related
problem
is
that
often
(e.
g.,
[Fis87]),
the
probability-based
tree
that
is
hllilt
to
identify
clusters
is
not,
height,
-balanced,
For
skpwed
input
data,
this
may
cause
the
performance
to
(Iqiyade
drarnatlically,
Distance-based
apprc)aclles:
They
assume
that
all
data
points
are
given
in
advance
and
can
be
scanned
freclueutly.
They
totally
or
partially
ignore
the
fact
that
not,
all
clata
points
in
the
clataset
are
eclually
irrrportant
with
respect
to
the
clustering
purpose,
and
that
dat,
a
points
which
are
close
ancl
d;nse
shoulci
he
considered
collectively
insteacl
of
individually,
They
are
global
or
.se771z-globol
methods
at
the
granularity
of
data
points.
That,
is,
for
each
clustering
decision,
they
inspect,
all
data
l)oints
or
all
currently
existing
clusters
eclually
no
matter
how
close
or
far
away
they
are,
and
they
use
glol]al
measurements,
which
require
scanning
all
data
points
or
all
currently
existing
clusters.
Hence
none
of
them
have
linear
time
scalability
with
stal)le
quality,
For
example,
using
exhausttvt
eTmnltratzort
(EE),
there
are
approximately
IiN/I<
!
[DH73]
ways
of
par-
titioning
a
set
of
N
data
points
into
K
subsets.
So
in
practice,
though
it
can
find
the
global
minimum,
it
is
infeasible
except,
when
IV
and
K
are
extremely
small.
Iterattve
optzmwatzon
(10)
[DH73,
KR90]
starts
with
an
initial
partition,
then
tries
all
possible
moving
or
swapping
of
data
points
from
one
group
to
another
to
see
if
such
a
moving
or
swapping
improves
the
value
of
the
measurement
function,
It
can
find
a
local
minimum,
hut,
the
c!uality
of
the
local
minimum
is
vpry
sensitivp
to
the
initially
selected
partition,
ancl
the
worst,
case
tirnp
complexity
is
still
exponential.
Hierarchtml
clustertn[g
(HC)
[DH73,
KR90,
Mur83]
does
not
try
to
find
ﬁlwst,ﬂ
clusters,
but
keeps
merging
the
closest,
pair
(or
splitting
the
farthest,
pair)
of
objects
to
form
clusters,
Witlh
a
reasonable
distance
measurement,,
the
best
time
com-
plexity
of
a
practical
HC
algorithm
is
0(N2).
Scj
it
is
still
unable
to
scale
well
with
large
N.
Clustering
has
been
recognized
as
a
useful
spatial
data
mining
method
recently.
[NH94]
presents
(™L,4BAN,$™
that
is
based
on
ranclornizecl
search,
and
proposes
that
(_™LARAN,$
outperforms
traditional
clustering
al-
gorithms
in
Statistics.
In
CLARANS,
a
rluster
is
repre-
sented
by
its
medotd,
or
the
most,
centrally
loc-ated
data
point
in
the
cluster
The
clustering
process
is
formali-
zed
as
searching
a
graph
in
which
each
uocle
is
a
Iiﬂ-
partition
representeci
by
a
set
of
Ii
rnedoi(ls,
and
two
nodes
are
neighbors
if
they
only
differ
by
one
medoid,
CLARANS
starts
with
a
randomly
selectecl
node.
For
the
current
node,
it
checks
at
most
the
n~om~e~ghbor
number
of
neighbors
randomly,
ancl
if
a
better
neigh-
bor
is
founcl,
it
moves
to
the
neighbor
ancl
contiulles;
otherwise
it
records
the
current,
node
as
a
10CCZ1
T7LZn
Z-
mum,
and
restarts
with
a
new
randomly
selected
node
to
search
for
another
local
mtntmum,
{ﬁ?LARAN,S
stops
after
the
numlocol
number
of
the
so-called
loccd
nLznz71w
have
been
found
,
and
returns
the
best,
of
these,
(YA
RA
N,S
suffers
from
the
same
ch-awbacks
as
the
shove
IO
method
wrt.
efficiency
In
addition,
it,
may
not
find
a
real
local
minimum
due
to
the
searching
trimming
controlled
by
mmmezghbor.
Later
[EKX9,5aj
and
[EK.X95h]
propose
focusing
techniques
(based
on
R*-trees)
to
improve
CLARA
N,S™s
ability
to
deal
with
data
objects
that,
may
reside
on
clisks
by
(
1
)
clustering
a
sample
of
the
ciat,
aset
that
is
drawn
from
each
R*-tree
data
page;
ancl
(2)
focusing
on
relevant
data
points
for
clist,
ance
and
quality
updates,
Their
experiments
show
that,
the
time
1s
improved
with
a
small
loss
of
quality,
2.1
Cent
ributions
of
BIRCH
An
important
contribution
is
our
formulation
of
the
clustering,
problem
in
a
way
that,
is
appropriate
for
104
very
large
clatasets,
by
making
the
time
and
memo-
ry
constraints
explicit.
In
addition,
BIRCH
has
the
following
advantages
over
previous
distance-based
ap-
proaches.
l
BIRCH
is
local
(as
opposed
to
global)
in
that
each
clustering
decision
is
made
without
scanning
all
data
points
or
all
currently
existing
clusters.
It
uses
rneasnrements
that
reflect
the
natural
closeness
of
points,
and
at
the
same
time,
can
be
incrementally
maintained
during
the
clustering
process.
l
BIRCH
exploits
the
observation
that
the
data
space
is
usually
not
uniformly
occupied,
and
hence
not
every
data
point
is
equally
important
for
clustering
purposes.
A
dense
region
of
points
is
treated
collectively
as
a
single
cluster.
Points
in
sparse
regions
are
treated
as
outlters
and
removed
optionally.
.
BIRt~H
makes
full
use
of
available
memory
to
derive
the
finest
possible
subclusters
(to
ensure
accuracy)
while
minimizing
1/0
costs
(to
ensure
efficiency).
The
clustering
and
reducing
process
is
organized
and
characterized
by
the
use
of
an
in-memory,
height-
balanced
and
highly-occupied
tree
structure.
Due
to
these
features,
its
running
time
is
linearly
scalable.
.
If
we
ornit
the
optional
Phase
4
5,
BIRCH
is
an
incremental
method
that
does
not
require
the
whole
clataset
in
advance,
and
only
scans
the
clataset
once.
3
Background
Assume
that
readers
are
familiar
with
the
terminology
of
vector
spaces,
we
begin
by
defining
centroid,
radius
and
diameter
for
a
cluste~.
Given
N
d-dimensional
data
points
in
a
cluster:
{.Xi}
where
i
=
1,2,
.
.
.
.
N,
the
centroid
XO,
radius
R
and
diameter
D
of
the
cluster
are
defined
as:
f,.
Ztl
~,
AT
(1)
Jx™mt-m™)+
N(NŒ1)
(2)
(3)
R
is
the
average
distance
from
member
points
to
the
centroid.
D
is
the
average
pairwise
distance
within
a
cluster.
They
are
two
alternative
measures
of
the
tightness
of
the
cluster
around
the
centroid.
Next
between
two
clusters,
we
define
5
alternative
distances
for
measuring
their
closeness.
(iiven
the
centroids
of
two
clusters:
X~l
ancl
X_62,
the
centroid
Euclidian
distance
DO
and
centroid
Manhattan
distance
D1
of
the
two
clusters
are
defined
as:
Do
=
((X731
Œ
xi12)™)+
(4)
d
1)1
=
Ixbl
Œ
X7121
=
~
[Xhl(t)
Œ
xb2(t)\
(5)
,=1
(iiven
NI
d-dimensional
data
points
in
a
cluster:
{.~i
}
where
i
=
1,2,
.
.
.
.
N],
and
N2
data
points
in
another
cluster:
{X-™}
where
j
=
N1
+
l,N1
+
2,
.
..)Nl
+
N2,
~he
average
in$er.clust
er
distancs
D%,
average
mtra-cluster
distance
D3
and
variance
increase
distance
D4
of
the
two
clusters
are
defined
as:
(6)
D3
is
actually
D
of
the
merged
cluster.
For
the
sake
of
clarity,
we
treat
X-O,
R
and
D
as
properties
of
a
single
cluster,
and
DO,
D
1,
D2,
D3
and
D4
as
properties
between
two
clusters
and
state
them
separately.
[Jsers
can
optionally
preprocess
data
by
weighting
or
shifting
along
different
dimensions
without
affecting
the
relative
placement.
4
Clustering
Feature
and
CF
Tree
The
concepts
of
Clustering
Feature
and
CF
tree
are
at
the
core
of
BIRCH™S
incremental
clustering.
A
Clustering
Feature
is
a
triple
summarizing
the
information
that
we
maintain
about
a
cluster.
Definition
4.1
Given
N
d-dimensional
data
points
in
a
cluster:
{ii}
where
i
=
1,
2,
.
.
.
.
N,
the
Clustering
Feature
(CF)
vector
of
the
cluster
is
defined
as
a
triple:
CF
=
(N,
L%™,
S™S),
where
N
is
the
number
of
data
points
in
the
cluster,
L~$
is
the
linear
sum
of
the
N
data
points,
i.e.,
~~
~
~~,
and
S™5™
is
the
square
sum
of
the
N
data
points,
i.e.,
~~=1
X-iz.
q
Theorem
4.1
(CF
Additivity
Theorem);
Assu7ne
that
CF1
=
(Nl
,
L~l
,
,$,$1),
a91~
CF2
=
(N2,
L3™2,
,$,$™z)
are
the
CF
vectors
of
two
dw~oint
clusters.
Then
the
CF
vector
of
the
cluster
that
is
formed
by
merging
the
two
disjoint
clusters,
is:
CF1
+
CF2
=
(Nl
+
N2,
L~l
+
L~2,
,$,$1
+
,5™,5™2)
(9)
The
proof
consists
of
straightforward
algebra.
[]
From
the
CF
definition
and
additivity
theorem,
we
know
that
the
CF
vectors
of
clusters
can
be
stored
and
calculated
incrementally
and
accurately
as
clusters
are
merged.
It
is
also
easy
to
prove
that
given
the
CF
vectors
of
clusters,
the
corresponding
XO,
R,
D,
DO,
D1,
D2,
D3
and
D4,
as
well
as
the
usual
quality
rnet,rics
(such
as
weighted
total/average
diameter
of
clusters)
can
all
be
calculated
easily.
One
can
think
of
a
cluster
as
a
set
of
data
points,
but
only
the
CF
vector
stored
as
summary.
This
CF
summary
is
not
only
efficient
because
it
stores
much
less
than
all
the
data
points
in
the
cluster,
hut
also
accurate
because
it
is
sufficient
for
calculating
all
the
measurements
that
we
need
for
making
clustering
decisions
in
BIRCH.
105
4.1
CF
Tree
A
CF
tree
is
a
height-balanced
tree
with
two
pararrl-
et(ers:
branching
factor
B
and
threshold
T.
Each
nonleaf
node
contains
at
most
B
entries
of
the
form
[CFi,
rhddi],
where
r™
=
1,2,...,
B,
ﬁ~hildi™)
is
a
pointer
to
its
i-th
child
node,
and
CF,
is
the
CF
of
the
sub-
clust,
er
represented
by
this
child.
So
a
nonleaf
node
represents
a
cluster
made
up
of
all
the
subclusters
rep-
resented
l>y
its
entries.
A
leaf
node
contains
at
most,
L
entries,
each
of
the
form
[CFi],
where
i
=
1,
2,
.
.
.
.
L,
In
addition,
each
leaf
node
has
two
pointers,
ﬁprevﬂ
and
ﬁnrxtﬂ
which
are
used
to
chain
all
leaf
nodes
together
for
efficient
scans.
A
leaf
node
also
represents
a
clus-
ter
made
up
of
all
the
subclusters
represented
by
its
entries.
But
all
entries
in
a
leaf
node
must]
satisfy
a
thrr.shold
requirement,
with
respect
to
a
thresholci
value
T:
tht-
dtametrr
(or
radtus)
has
to
&
less
than
T.
The
tree
size
is
a
function
of
T.
The
larger
‚T
is,
the
slllaller
the
tree
is.
We
require
a
node
to
it,
in
a
pa~e
of
size
P,
once
the
dimension
d
of
the
data
space
1s
g]veu,
the
sizes
of
leaf
and
nonleaf
entries
are
known,
then
B
and
L
are
determined
by
P.
So
P
can
be
varied
for
performance
tuning.
Such
a
CF
tree
will
be
built
dynamically
as
new
data
ohjectls
are
inserted.
It,
is
used
to
guide
a
new
insertion
illtlo
the
correct
suhcluster
for
clustering
purposes
just
the
same
as
a
B+--tree
is
USd
to
guide
a
new
insertion
il]tu
the
corrert
position
for
sorting
purposes.
The
CF
tree
is
a
very
compact
representation
of
the
dat,aset
I>eralme
each
entry
in
a
leaf
node
is
not
a
single
data
l)oint
hut,
a
subcluster
(which
absorbs
many
data
points
with
diameter
(or
radius)
under
a
specific
threshold
T).
4.2
Insertion
into
a
CF
Tree
We
now
present,
the
algorithm
for
inserting
an
entry
into
a
CF
tree.
(~iveu
entry
ﬁEntﬂ,
it
proceeds
as
helnw:
ldentzf~ymg
the
appropriate
leaf:
Starting
from
the
root,,
it
recursively
descends
the
CF
tree
by
choosing
tile
closest
child
node
according
to
a
chosen
distance
metric:
DO,
D1
,D2,
D3
or
D4
as
defined
in
Sec.
3.
Modtf?ytn~g
the
leaf:
When
it
reaches
a
leaf
node,
it
finc]s
the
rlosest
leaf
entry,
say
L,,
and
then
tests
whether
L!
can
ﬁahsorhﬂ
ﬁ
Entﬂ
withoutv
iolatingthe
threshold
conclitionz.
If
SO,
the
CF
vector
for
Li
is
ul~dated
to
reflect
this,
If
not,,
a
new
entry
for
ﬁEnt,ﬂ
is
addecl
to
the
leaf.
If
there
is
space
on
the
leaf
for
this
new
entry,
we
are
done,
otherwise
we
must,
.sp/it
the
leaf
node.
,Nocle
splitting
is
done
by
choosing
the
farthest
pair
of
entries
as
seeds,
and
redistributing
the
remaining
entries
based
on
the
closest
criteria.
.x.
Modtf;jzn!g
th~
~mth
to
the
leaf:
After
inserting
ﬁEntﬂ
mt,o
a
leaf,
we
must,
update
the
CF
information
for
2Tl}at
is,
the
cluster
n)erged
with
ﬁEntﬂ
and
L,
n]ust
satisfy
the
threshold
condition.
Note
that
the
CF
vector
of
the
new
rluster
cal}
be
eomputecl
from
the
CF
vectors
for
L!
and
ﬁEntﬂ.
each
nonleaf
entry
on
the
path
to
the
leaf.
In
the
absence
of
a
split,
this
simply
involves
adding
CF
vectors
to
reflect
the
addition
of
ﬁEntﬂ.
A
leaf
split,
requires
us
to
insert
a
new
nonleaf
entry
into
the
parent,
node,
to
describe
the
newly
created
leaf,
If
the
parent,
has
space
for
this
entry,
at
all
higher
levels,
we
only
need
to
update
the
CF
vectors
to
reflect,
the
addition
of
ﬁEntﬂ.
In
general,
however,
we
may
have
to
split
the
parent
as
well,
ancl
so
on
up
to
the
root,,
If
the
root
is
split,
the
tree
height
increases
by
one.
4.A
Mergz?~gRefi?lelrte?tt:
Splits
are
caused
hy
the
page
size,
which
is
independent
of
the
clustering
properties
of
the
data.
In
the
presence
of
skewed
data
input,
order
,
this
can
affect
the
clustering
quality,
and
also
recluce
space
utilization.
A
simple
additional
merging
step
often
helps
ameliorate
these
problems:
Suppose
that
there
is
a
leaf
split,
and
the
propagation
of
this
split
stops
at
some
nonleaf
nocle
NJ,
i.e.,
N,T
can
accommodate
the
additional
entry
resulting
from
the
split.
We
now
scan
node
N,T
to
find
the
two
closest
entries.
If
they
are
not
the
pair
corresponding
to
the
split,
we
try
to
merge
them
and
the
corresponding
two
child
nodes.
If
there
are
more
entries
in
the
two
child
nodes
than
one
page
can
hold,
we
split
the
merging
result
again.
During
the
resplitting,
in
case
one
of
the
seed
attracts
enough
merged
entries
to
fill
a
page,
we
just
put
the
rest
entries
with
the
other
seed.
In
summary,
if
the
mergecl
entries
fit,
on
a
single
page,
we
free
a
node
space
for
later
use,
create
one
more
entry
space
in
node
NJ,
thereby
increasing
space
utilization
and
postponing
future
splits;
otherwise
we
improve
the
distribution
of
entries
in
the
closest,
two
children.
Since
each
node
can
only
hold
a
limited
number
of
entries
clue
to
its
size,
it
does
not
always
correspond
to
a
natural
cluster.
occasionally,
two
subclusters
that
should
have
been
in
one
cluster
are
split,
across
nodes.
Depending
upon
the
order
of
data
input
and
the
degree
of
skew,
it
is
also
possible
that
two
subclust,ers
that
should
not
be
in
one
cluster
are
kept
in
the
same
node.
These
infrequent,
but
unclesirahle
anomalies
caused
Ijy
page
size
are
remedied
with
a
global
(or
semi-glo]>al)
algorithm
that
arranges
leaf
entries
across
nodes
(Phase
3
discussed
in
Sec.
5),
Another
undesirable
artifact,
is
that
if
the
same
data
point
is
inserted
twice,
hut
at,
different,
times,
the
two
copies
might
be
entered
into
distinct
leaf
entries.
or,
in
another
word,
occasionally
with
a
skewed
input
order,
a
point,
might
enter
a
leaf
entry
that
it,
should
not
have
entered.
This
problem
can
he
addressed
with
further
refinement
passes
over
the
data
(Phase
4
discussed
in
Ser.
5),
5
The
BIRCH
Clustering
Algorithm
Fig.
1
presents
the
overview
of
BIRCH.
The
main
task
of
Phase
1
is
to
scan
all
data
and
build
an
initial
in-
memory
CF
tree
using
the
given
amount,
of
memory
106
Data
J/
Initial
CF
tree
Phase
2
<optional):
Condense
tit.
desirable
rang.
by
buildkpj
.
sm.11.r
CF
tree
1
.n,
aller
C-F
tree
+
r
F™base.z
3:
Global
Clu
steting
1
Better
clusters
+~
Figure
1:
BIRCH
OvervZcuI
and
recycling
space
on
disk.
This
CF
tree
tries
to
reflect
the
clustering
information
of
the
dataset
as
fine
as
possible
under
the
memory
limit.
With
crowded
data
points
grouped
as
fine
subclusters,
and
sparse
data
points
removed
as
outliers,
this
phase
creates
a
in-
memory
summary
of
the
data.
The
details
of
Phase
1
will
be
discussed
in
Sec.
5.1.
After
Phase
1,
subsequent
computations
in
later
phases
will
be:
1
fast
became
(a)
no
1/0
operations
are
needed,
and
(b)
the
problem
of
clustering
the
original
data
is
reduced
to
a
smaller
problem
of
clustering
the
subclusters
in
the
leaf
entries;
2.
accurate
because
(a)
a
lot
of
outliers
are
eliminated,
ancl
(b)
the
remaining
data
is
reflected
with
the
finest
granularity
that
can
be
achieved
given
the
available
rnernory;
3.
less
order
sensitive
because
the
leaf
entries
of
the
initial
tree
form
an
input
order
containing
better
data
locality
compared
with
the
arbitrary
original
data
input,
order.
Phase
2
is
optional.
We
have
observed
that
the
ex-
isting
global
or
semi-global
clustering
methods
applied
in
Phase
3
have
different
input
size
ranges
within
which
they
perform
well
in
terms
of
both
speed
and
quality.
So
potentially
there
is
a
gap
between
the
size
of
Phase
1
results
and
the
input
range
of
Phase
3.
Phase
2
serves
as
a
cushion
and
bridges
this
gap:
Similar
to
Phase
1,
it
scans
the
leaf
entries
in
the
initial
CF
tree
to
rebuild
a
smaller
CF
tree,
while
removing
more
outliers
and
grouping
crowded
subclusters
into
larger
ones.
The
undesirable
effect
of
the
skewed
input
order,
and
splitting
triggered
by
page
size
(Sec.
4.2)
causes
us
to
be
unfaithful
to
the
actual
clustering
patterns
in
the
data.
This
is
remedied
in
Phase
3
by
using
a
global
or
semi-global
algorithm
to
cluster
all
leaf
entries.
We
observe
that
existing
clustering
algorithms
for
a
set
of
data
points
can
be
readily
adapted
to
work
with
a
set,
of
subclusters,
each
described
by
its
CF
vector.
For
example,
with
the
CF
vectors
known,
(1)
naively,
by
calculating
the
centroid
as
the
representative
of
a
subcluster,
we
can
treat
each
subcluster
as
a
single
point
and
use
an
existing
algorithm
without,
modification;
(2)
or
to
be
a
little
more
sophisticated,
we
can
treat
a
subcluster
of
n
data
points
as
its
cent,
roid
repeating
n
times
and
modify
an
existing
algorithm
slightly
to
take
the
counting
information
into
account;
(3)
or
to
be
general
and
accurate,
we
can
apply
an
existing
algorithm
directly
to
the
subclusters
because
the
information
in
their
CF
vectors
is
usually
sufficient,
for
calculating
most
distance
and
quality
metrics.
In
this
paper,
we
adapted
an
agglomerative
hierarchic-
al
clustering
algorithm
by
applying
it
directly
to
the
subclusters
represented
by
their
CF
vectors.
It
uses
the
accurate
distance
metric
D2
or
D4,
which
can
he
calculated
from
the
CF
vectors,
during
the
whole
clus-
tering,
and
has
a
complexity
of
O(lV2).
It
also
provides
the
flexibility
of
allowing
the
user
to
specify
either
the
desired
number
of
clusters,
or
the
desired
diameter
(or
radius)
threshold
for
clusters.
After
Phase
3,
we
obtain
a
set,
of
clusters
that,
captures
the
major
distribution
pattern
in
the
data,
However
minor
and
localized
inaccuracies
might
exist
because
of
the
rare
misplacement
problem
mentioned
in
Sec.
4.2,
and
the
fact
that
Phase
3
is
applied
on
a
coarse
summary
of
the
data.
Phase
4
is
optional
and
entails
the
cost
of
additional
passes
over
the
data
to
correct
those
inaccuracies
and
refine
the
clusters
further.
Note
that
up
to
this
point,
the
original
data
has
only
been
scanned
once,
although
the
tree
and
outlier
information
may
have
been
scanned
multiple
times.
Phase
4
uses
the
centroids
of
the
clusters
produced
Ly
Phase
3
as
seeds,
and
redistributes
the
data
points
to
its
closest
seed
to
obtain
a
set
of
new
clusters.
Not
only
does
this
allow
points
belonging
to
a
cluster
to
rnigrat,e,
but
also
it
ensures
that
all
copies
of
a
given
data
point
go
to
the
same
cluster.
Phase
4
can
be
extended
with
additional
passes
if
desired
by
the
user,
and
it
has
been
proved
to
converge
to
a
minimum
[G
G92].
As
a
bonus,
during
this
pass
each
data
point
can
be
labeled
with
the
cluster
that
it
belongs
to,
if
we
wish
to
identify
the
data
points
in
each
cluster.
Phase
4
also
provides
us
with
the
option
of
discarding
outliers.
That
is,
a
point
which
is
too
far
from
its
closest,
seed
can
be
treated
as
an
outlier
and
not
included
in
the
result.
5.1
Phase
1
Revisited
Fig.
2
shows
the
details
of
Phase
1.
It
starts
with
an
initial
threshold
value,
scans
the
data,
and
inserts
points
into
the
tree.
If
it
runs
out
of
memory
before
it
finishes
scanning
the
data,
it
increases
the
threshold
value,
rebuilds
a
new,
smaller
CF
tree,
by
re-inserting
the
leaf
entries
of
the
old
tree.
After
the
old
leaf
entries
have
been
re-inserted,
the
scanning
of
the
data
(and
insertion
into
the
new
tree)
is
resumed
from
the
point
at
which
it
was
interrupted.
107
,
f
(.ontmue
scanning
data
and
insert
to
+1
1
out
of
memﬂl-v
Fuush
scamm~
data
.
.
Result?
I
v
(1)
Increase
T.
(2)
Rebudd
(LF
txe
t2
of
new
T
from
(.F
tree
tl:
if.
leaf
entry
of
tl
k
potential
outher
and
disk
space
.wadabks,
write
to
disk;
othewise
use
it
to
mbudd
t2.
(3)
tl
<-
tz
Otherw™,.
e
Out
of
disk
space
c
~
(
Re-absmb
tmtentid
outiiers
into
tl
1
c
1
(
Re-.bamb
potemi.+1
outkrs
mto
tl
I
Figure
2:
(!ontrol
Flou)
of
Phase
1
old
Tree
New
Tree
A-A
OldCurrentPath
NeWClOSeStPath
NewCurrentPath
Figure
3:
Ftebuddtmg
<™F
Tree
5.1.1
Reducibility
Assume
t,
is
a
CF
tree
of
threshold
T,.
Its
height
is
h,
and
its
size
(numl>er
of
nodes)
is
,™i™t,
(iiven
T,+l
>
T,,
we
want
to
use
all
the
leaf
entries
of
tz
to
rebuild
a
CF
tree.
t%+l
,
of
threshold
T,+l
such
that
the
size
of
tt+
1
should
not,
be
larger
than
,$™~.
Following
ifi
the
rebuilding
algorithm
as
well
as
the
consequent
reduril>ility
theorem.
Assome
within
each
node
of
CF
tree
t,,
the
entries
are
labeled
contiguously
from
O
to
nk
Š
1,
where
71k
is
the
number
of
entries
in
that
node,
then
a
path
from
an
entry
in
the
root
(level
1)
to
a
leaf
node
(level
h)
(an
he
uniquely
representeci
by
(il
,
i2,
,..,
i}~_,
),
where
i,)
,
:1
=
ll...,)/
Œ
1
is
the
label
of
the
j-th
level
entry
.(1)
.(1)
‚(™)
)
is
on
that
path.
So
naturally,
path
(tl
,Z2
,
.
.
..zl~_l
befc)re
(or<
)pat,h(i\2),
i$),
.
.
..i~~l)
ifi\l)=i~2)
..,.,
.(1)
=
~(~)
z
,7Œ1
l_l,
and
i~l)
<
iJ2)(0
<j
~
h-l).
It
is
obvious
that,
a
leaf
node
corresponds
to
a
path
uniquely,
and
we
will
use
path
and
leaf
node
interchangeably
from
now
on.
The
algorithm
is
illustrated
in
Fig.
3.
With
the
natural
path
order
defined
above,
it,
scans
and
frees
the
old
tree
path
by
path,
and
at,
the
same
time,
creates
the
new
tree
path
hy
path.
The
new
tree
starts
with
NIJLL,
an[l
ﬁol~l(.~urrentpat
n™ﬂ
starts
with
the
leftmost
path
in
the
old
tree.
For
ﬁold(!urrentpat
h™ﬂ,
the
algorithr~l
proceeds
as
below:
1
2
3,
4.
Creatr
thr
corrrspondL7ig
ﬁNru)(!urre~it[
™atllﬂ
tn
the
new
tree:
nodes
are
added
to
the
new
tree
exactly
the
same
as
in
the
old
tree,
so
that
there
is
no
chance
that
the
new
tree
ever
hecornes
larger
than
the
old
tree.
Insert
leaf
e7itrze.s
tn
ﬁOldCurrmtI)atliﬂ
to
thy
neti)
tree:
with
the
new
threshold,
each
leaf
entry
in
ﬁolCi-
(.~urrentPathﬂ
is
tested
against
the
new
tree
to
see
if
it,
can
fit
3
in
the
ﬁNewC1osest
Pat,
hﬂ
that,
is
found
top
down
with
the
closest
criteria
in
the
new
tree.
If
yes
and
‚LNew(~losestPathﬂ
is
before
ﬁNew(.~urrentPa
t,hﬂ,
then
it
is
inserted
to
ﬁNewClosestPathﬂ,
and
the
space
in
ﬁNewCurrentPathﬂ
is
left
available
for
later
use;
otherwise
it
is
inserted
to
ﬁNew(
;urrent
F™athﬂ
without
creating
any
new
node.
Frer
spare
iIt
ﬁOldCurrentPathﬂ
and
ﬁNe
W(
‚ur7rnt-
Pathﬂ:
Once
all
leaf
entries
in
ﬁOICI(
;urrentF™athﬂ
are
processed,
the
un-needed
nodes
along
ﬁ01[1(
!urrent,-
Path>™
can
be
freed.
It
is
also
likely
that
some
nodes
along
ﬁNewC;urrentPathﬂ
are
empty
because
leaf
en-
tries
that
originally
correspond
to
this
path
are
now
ﬁpushed
forwardﬂ.
In
this
case
the
empty
nodes
can
be
freed
too.
ﬁOldCurre71tPathﬂ
M
set
to
thr
next
pdh
z71
the>
old
tnw
tf
ther-r
rxzsts
071r,
a71d
repeat
the
abmw
stq3s.
From
the
rebuilding
steps,
old
leaf
entries
are
re-
inserted,
but
the
new
tree
can
never
become
larger
than
the
old
tree.
Since
only
nodes
corresponding
to
ﬁOldC!urrent
Pathﬂ
and
ﬁNew(
k~rrentPathﬂ
need
to
exist
simultaneously,
the
maximal
extra
space
needed
for
the
tree
transforrnation
is
h
pages.
So
hy
increasing
the
threshold,
we
can
rebuild
a
smaller
CF
tree
with
a
limited
extra
memory.
Theorem
5.1
(Tkducibilit
y
Theorem:
):
.+!
,ssunt
c
we
rekld
CF
t
me
t
~+1
of
thrr.shold
Ti+
~
from
(ﬁ™F
t,rr~
t,
of
threshold
T%
by
the
about,
al~gorlthm,
and
let
,5™,
GItd
S,+l
be
the
szz~s
oft,
and
t,+l
resprctzuel~j.
If
T,+l
>
T,,
then
,S1+l
<
,S%,
and
the
transforniatzo71
from
t%
to
t,+,
71
eeds
at
Tn
ost
h
ext
r-o
pages
of
7r~
e7n
ory,
71111
we
h
1s
the
Ileaght
oft,.
5.1.2
Threshold
Values
A
good
choice
of
threshold
value
can
greatly
reduce
the
number
of
rebuilds.
Since
the
initial
threshold
value
To
is
increased
dynamically,
we
can
adjust
for
its
lwing
tc)o
low.
But
if
the
initial
TO
is
too
high,
we
will
obtain
a
less
detailed
CF
tree
than
is
feasible
with
the
available
memory.
So
To
should
he
set
conservatively.
BIR(~H
sets
it,
t,o
zero
by
default;
a
knowledgeable
user
could
change
this,
3Eit11er
absorbed
by
an
existing
leaf
entry,
or
created
as
a
IIeW
leaf
entry
witllOut
splitting.
108
Suppose
that
T,
turns
out
to
lx=
too
small,
and
we
subsequently
run
out
of
memory
after
Nt
data
points
have
been
scanneci,
and
~~%
leaf
entries
have
hem
formed
(eaeh
satisfying
the
threshold
condition
wrt.
fi).
Based
on
the
portion
of
the
data
that
we
have
scanned
and
the
tree
that,
we
have
built
up
so
far,
we
need
to
estimate
the
next
threshold
value
T,+l
This
estimation
is
a
diflirult
i>roblemi
and
a
full
solution
is
beyond
the
scope
of
this
paper.
(
!urrently,
we
use
the
following
heuristie
approarh:
1.
We
try
to
choose
2ﬁ%+1
so
that,
N,+l
=
Min(2N,,
N).
That,
is,
whether
N
is
known,
we
choose
to
estirnat,e
T,+
I
at
most
in
proportion
to
the
data
we
have
seen
thus
far.
2.
Intuitivelyj
we
want
to
increase
threshold
based
on
some
measure
of
volrme.
There
are
two
distinct,
notions
of
volume
that
we
use
in
estimating
threshold,
The
first
is
average
volume,
which
is
defineci
as
t~
=
rd
where
r
is
the
average
radius
of
the
root
cluster
in
the
CF
tree,
and
d
is
the
(Iimensionality
of
the
space.
Intuitively,
this
is
a
measure
of
the
space
oeeupied
by
the
portion
oft
he
data
seen
thus
far
(the
ﬁfootprintﬂ
of
seen
data).
A
second
notion
of
volume
packed
tdum~,
which
is
defined
as
Vp
=
(~,
*
T%(i,
where
~1~
is
the
number
of
leaf
entries
and
Tt
d
is
the
maximal
volume
of
a
leaf
entry.
Intuitively,
this
is
a
measure
of
the
actual
volume
occupied
by
the
leaf
clusters.
Since
~~z
is
essentially
the
same
whenever
we
run
out
of
memory
(since
we
work
with
a
fixed
amount,
of
memory),
we
can
approximate
VP
by
Ti
d.
We
make
the
assumption
that
r
grows
with
the
number
of
data
points
Ni.
By
maintaining
a
rerord
of
r
and
the
number
of
points
Ni,
we
ean
estimate
ri+
1
using
least,
squares
linear
regression.
We
define
the
ezpnrmon
factor
f
=
Maz(
1.01
*),
and
use
it
as
a
heuristic
measure
of
how
the
data
footprint
is
growing.
The
use
of
Max
is
motivated
by
our
observation
that
for
most
large
datasets,
the
observeci
footprint
heeomes
a
constant
quite
quickly
(unless
the
input
order
is
skewed).
Similarly,
by
making
the
assumption
that
VT,
grows
linearly
with
Ni,
we
estimate
Ti+
1
using
least
squares
linear
regression.
3.
We
traverse
a
path
from
the
root
to
a
leaf
in
the
CF
tree,
always
going
to
the
child
with
the
most
points
in
a
ﬁgreedyﬂ
attempt
to
find
the
most
crowded
leaf
node.
We
calculate
the
distance
(
~),nin
)
between
the
rlosest
two
entries
on
this
leaf.
If
we
want
to
build
a
more
condensed
tree,
it
is
reasonable
to
expeet
that
we
should
at
least
increase
the
threshold
value
to
D~Zn,
so
that
these
two
entries
can
he
merged.
4.
We
multiplied
the
Ti+l
value
obtained
through
linear
regression
with
the
expansion
factor
f,
ancl
adjustecl
it
using
D~i~
as
follows:
Tt+l
=
Mwr(DTntn,
f
*
T%+,).
To
ensure
that
the
threshold
value
grows
monotonically,
in
the
very
unlikely
case
that,
Ti+
1
obtained
thus
is
less
than
T~
then
we
choose
Tt+l
=
T!
Y
(~)~.
(This
is
equivalent
to
assuming
that
aII
iar:~
points
are
uniformly
distributed
in
a
d-(dimensional
sphere,
and
is
really
just
a
crude
:tI>l]roxiI1l:iti
t)ll,
however,
it
is
rarely
called
for.
)
5.1.3
Outlier-Handling
Option
Optionally,
we
c-an
use
R
bytes
of
disk
space
for
handling
outlters,
which
are
leaf
entries
of
low
density
that
are
judged
to
be
unimportant,
wrt.
the
overall
elllst,ering
pattern.
When
we
rebuild
the
CF
tree
by
re-inserting
the
old
leaf
entries,
the
size
of
the
new
tree
is
reduce(l
in
two
ways.
First,
we
increase
the
threshold
value,
thereby
allowing
each
leaf
entry
to
ﬁahsorhﬂ
more
points.
Second,
we
treat
some
leaf
entries
as
potential
outliers
and
write
them
out
to
disk.
An
old
leaf
entry
1s
considered
to
be
a
potential
outlier
if
it
has
ﬁfar
fewerﬂ
data
points
than
the
average.
ﬁFar
fewerﬂ,
is
of
course
another
heuristics.
Periodically,
the
disk
space
may
run
out,
and
the
potential
outliers
are
scanned
to
see
if
they
can
he
re-
absorbed
into
the
current
tree
without
causing
the
tree
to
grow
in
size.
Š
An
increase
in
the
threshold
value
or
a
change
in
the
distribution
due
to
the
new
(Iat,
a
read
after
a
potential
outlier
is
written
out]
could
well
mean
that
the
potential
outlier
no
longer
qualities
as
an
outlier.
When
all
data
has
been
scanneci,
the
potential
outliers
left,
in
the
disk
space
must
be
scanned
to
verify
if
they
are
indeed
outliers.
If
a
potential
outlier
ran
not
he
absorbed
at
this
last
chance,
it,
is
very
likely
a
real
outlier
and
can
be
removed.
Note
that
the
entire
cycle
Š
insufficient
memory
triggering
a
rebuilding
of
the
tree,
insufficient
disk
spare
triggering
a
re-absorbing
of
outliers,
etc.
Š
could
be
repeated
several
times
before
the
datlasetf
is
fldly
scanned.
This
effort
must
be
considered
in
a[l(lit,ion
t,c
the
cost
of
scanning
the
data
in
order
to
assess
t,he
(-Ost,
of
Phase
1
accurately.
5.1.4
Delay-Split
Option
When
we
run
out
of
main
memory,
it
may
well
he
the
case
that
still
more
data
points
can
fit,
in
the
current,
CF
tree,
without
changing
the
threshold.
However,
some
of
the
data
points
that
we
read
may
require
us
to
sl)lit
a
node
in
the
(3F
tree,
A
simple
idea
is
to
writ,
e
such
data
points
to
disk
(in
a
manner
similar
to
how
outliers
are
written),
and
to
proceed
reading
the
data
until
we
run
out,
of
disk
space
as
well.
The
advantage
of
this
approach
is
that
in
general,
more
data
points
ean
fit
in
the
tree
before
we
have
to
rebuild.
6
Performance
Studies
We
present
a
complexity
analysis,
and
then
discuss
the
experiments
that
we
have
conducted
on
BIRC™H
(an(l
(~LARAN,S)
using
synthetic™
as
well
as
real
dataset,s.
109
6.1
Analysis
First
we
analyze
the
cpu
cost
of
Phase
1.
The
maximal
size
of
the
tree
is
#.
To
insert
a
point,
we
need
to
follow
a
path
from
root
to
leaf,
touching
about
1
+
logB
~
nodes.
At
each
node
we
must
examine
B
entries,
looking
for
the
ﬁclosestﬂ;
the
cost
per
entry
is
proportional
to
the
dimension
d.
So
the
cost
for
inserting
all
data
points
is
O(d
*
N
*
B(l
+
logB
$)).
In
case
we
must
rebuild
the
tree,
let
ES
be
the
CF
entry
size.
There
are
at
most
&
leaf
entries
to
re-insert,
so
the
cost
of
re-
inserting
leaf
entries
is
O(d
*
&
*
B(
1
+
logB
~)).
The
number
of
times
we
have
to
re-builcl
the
tree
depends
upon
our
threshold
heuristics.
Currently,
it
is
about
logz
&
,
where
the
value
2
arises
from
the
fact
that
we
never
estimate
farther
than
twice
of
the
cm-rent
size,
and
NO
is
the
number
of
data
points
loaded
into
memory
with
threshold
To.
So
the
total
CPU
cost
of
Phase
1
is
()(d*N*B(l+logB
*)+log2
~*i*#$*B(l+logB
*)).
The
analysis
of
Phase
2
cpu
cost
is
similar,
and
hence
omitted.
As
for
1/0,
we
scan
the
data
once
in
Phase
1
and
not
at
all
in
Phase
2.
With
the
outlier-handling
and
delay-split
options
on,
there
is
some
cost
associated
with
writing
out
outlier
entries
to
disk
and
reading
them
back
during
a
rebuilt.
Considering
that
the
amount
of
disk
available
for
outlier-handling
(and
delay-split)
is
not
more
than
M,
and
that
there
are
about
log2
~
re-builds,
the
1/0
cost
of
Phase
1
is
not
significantly
different
from
the
cost
of
reading
in
the
dataset.
Based
on
the
above
analysis
Š
which
is
actually
rather
pessimistic,
in
the
light
of
our
experimental
results
Š
the
cost
of
Phases
1
and
2
should
scale
linearly
with
N.
There
is
no
1/0
in
Phase
3.
Since
the
input
to
Phase
3
is
bounded,
the
cpu
cost
of
Phase
3
is
therefore
hounded
hy
a
constant
that
depends
upon
the
maximum
input,
size
and
the
global
algorithm
chosen
for
this
phase.
Phase
4
scans
the
clataset
again
and
puts
each
data
point
into
the
proper
cluster;
the
time
taken
is
proportional
to
IV
*
K.
(However
with
the
newest
ﬁnearest
neighborﬂ
techniques,
it
can
be
improved
[(i(~92]
to
be
almost
linear
wrt.
N.)
6.2
Synthetic
Dataset
Generator
To
study
the
sensitivity
of
BIRCH
to
the
characteristics
of
a
wide
range
of
input
datasets,
we
have
used
a
collection
of
synthetic
datasets
generated
by
a
generator
that,
we
have
developed.
The
data
generation
is
controlled
by
a
set
of
parameters
that
are
summarized
in
Table
1.
Each
dataset
consists
of
1{
clusters
of
2-d
data
points.
A
cluster
is
characterized
by
the
number
of
data
points
in
it,(n),
its
radius(r),
and
its
center(c).
n
is
in
the
range
of
[7~/
,nk],
and
r
is
in
the
range
of
[rl
,rh]4.
once
placed,
the
clusters
cover
a
range
of
values
in
each
4Note
tllat
wl)en
?LL
=
TLh
tl]e
nun)her
of
points
is
fixed
and
WlIeII
rl
=
r,,
tlle
radius
is
fixed,
Paran3eter
Values
or
.
.
.
.
.
.
.
.
~.
.-,
u...
-
Number
of
clusters
h-
4..
256
nt
(Lower
n)
0..
2500
?Lh
(Higher
n)
50..
2500
u
r-~
(Lower
r)
0..
d2
u
Table
1:
Data
Generation
Parameters
and
Thetr
Values
or
Ranges
Experimented
dimension.
We
refer
to
these
ranges
as
the
ﬁoverviewﬂ
of
the
dataset.
The
location
of
the
center
of
each
cluster
is
deter-
mined
by
the
pattern
parameter.
Three
patterns
Š
grzd,
,stne,
and
random
Š
are
currently
supported
by
the
generator.
When
the
gnd
pattern
is
used,
the
clus-
ter
centers
are
placed
on
a
~
x
@
grid.
The
distance
between
the
centers
of
neighboring
clusters
on
the
same
row/column
is
controlled
by
kg,
and
is
set
to
k{+.
This
leads
to
an
overview
of
[O,~kj~]
on
both
dimensions.
The
szne
pattern
places
the
cluster
cen-
ters
on
a
curve
of
sine
function.
The
K
clusters
are
divided
into
71,
groups,
each
of
which
is
placecl
on
a
different
cycle
of
the
sine
function.
The
c
location
of
the
center
of
cluster
i
is
2ni
whereas
the
y
location
is
~
*
sine
(2ni/(~)).
The
overview
of
a
sine
dataset
is
nc
therefore
[0,2mK]
and
[Œ~
,+~]
on
the
x
and
y
di-
rections
respectively.
The
random
pattern
places
the
cluster
centers
randomly.
The
overview
of
the
dataset
is
[O,K]
on
both
dimensions
since
the
the
c
and
y
locations
of
the
centers
are
both
randomly
distributed
within
the
range
[O,
K].
Once
the
characteristics
of
each
cluster
are
deter-
mined,
the
data
points
for
the
cluster
are
generated
ac-
cording
to
a
2-d
independent
normal
distribution
whose
mean
is
the
center
c,
and
whose
variance
in
each
di-
mension
is
$.
Note
that
due
to
the
properties
of
the
normal
distribution,
the
maximum
distance
between
a
point
in
the
cluster
and
the
center
is
unbounded.
In
other
words,
a
point
may
be
arbitrarily
far
from
its
be-
longing
cluster.
So
a
data
point
that
belongs
to
cluster
A
may
be
closer
to
the
center
of
cluster
B
than
to
the
center
of
A,
and
we
refer
to
such
points
as
ﬁoutsidersﬂ.
In
addition
to
the
clustered
data
points,
noise
in
the
form
of
data
points
uniformly
distributed
throughout
the
overview
of
the
dataset
can
be
added
to
the
dataset.
The
parameter
rn
controls
the
percentage
of
data
points
in
the
dataset
that
are
considered
noise.
The
placement
of
the
data
points
in
the
dataset
is
controlled
by
the
order
parameter
o.
When
the
randomized
option
is
used,
the
data
points
of
all
clusters
and
the
noise
are
randomized
throughout
the
entire
110
Scope
Parameter
Default
Value
(;lobal
Memory
(M)
8OX1O24
bytes
Disk
(R)
20%
M
Dista;lc~
clef.
[)2
L
Quality
clef.
::~~,,o,d
for
~
Threshold
clef.
F™hasel
Initial
tbresbold
0.0
Delay-split
011
~age
size
(P)
1024
bytes
outlier-handling
01)
outlier
clef.
Leaf
entry
which
contaias
<
~5Y0
of
the
average
aumt)er
of
pOints
per
leaf
Euclidian
distance
to
the
closest
seed
is
larger
than
twire
of
the
radius
of
that
cluster
u
Table
2:
BIR(?H
Parameters
and
Tlimr
Dclault
Wue.s
dataset.
Whereas
when
the
ordered
option
is
selected,
the
data
points
of
a
cluster
are
placed
together,
the
c-lusters
are
placed
in
the
order
they
are
generated,
and
the
noise
is
placed
at
the
end.
6.3
Parameters
and
Default
Setting
t?
IR.(~H
is
capable
of
working
under
various
settings.
Table
2
lists
the
parameters
of
BIRCH,
their
effecting
scopes
and
their
default,
values.
[Jnless
specified
explicitly
otherwise.
an
experiments
is
conducted
under
this
default
setting.
flf
was
selected
to
he
80
kbytes
whirh
is
about
5%
of
the
dataset
size
in
the
base
workload
used
in
our
experiments.
Since
clisk
space
(R)
is
just
used
for
outliers,
we
assume
that,
R
<
M
and
set
R
=
20%
of
M.
The
experiments
on
the
effects
of
the
5
distance
metrics
in
the
first
3
phases[ZR,L9.5]
indicate
that
(1)
using
D3
in
Phases
I
and
2
results
in
a
much
higher
en(ling
threshold,
and
hence
produces
clusters
of
poorer
quality;
(2)
however,
there
is
no
distinctive
performance
difference
among
the
ot,
hms.
So
we
decided
to
choose
L)2
as
default.
Following
Statistics
tradition,
we
choose
ﬁweighted
average
diameterﬂ
(denoted
as
D)
as
quality
measurement.
The
smaller
~
is,
the
het,ter
the
quality
is.
The
threshold
is
defined
as
the
threshold
for
cluster
diameter
as
default.
In
Phase
1,
the
initial
threshold
is
default
to
O.
Based
on
a
study
of
how
page
size
affects
perforrnance[ZR,
L95],
we
selected
P
=
1024.
The
delay-split,
option
is
on
so
that
given
a
threshold,
the
CF
tree
accepts
more
data
points
and
reaches
a
higher
capacity.
The
outlier-
handling
option
is
on
so
that
BIRL™H
can
remove
outliers
and
concentrate
on
the
dense
places
with
the
given
amount
of
resources.
For
simplicity,
we
treat
a
leaf
entry
of
which
the
number
of
ciatla
points
is
less
than
a
quarter
of
the
average
as
an
outlier.
In
Phase
3,
most
global
algorithms
can
handle
1000
ol>jectls
cluitle
well.
So
we
default,
the
input
range
as
1000.
We
have
chosen
the
adaptecl
H(7
algorithm
to
use
here.
We
deciclecl
to
let
Phase
4
refine
the
clusters
only
once
with
its
ciiscarcl-out,lier
option
off,
so
that
all
(Iat,
a
points
will
be
counteci
in
the
quality
measurement,
for
fair
comparisons.
6.4
Base
Workload
Performance
The
first
set
of
experiments
was
to
evaluate
the
ability
of
J31R(.™H
to
cluster
various
lar~e
datasets,
All
the
times
are
presented
in
second
in
tlus
paper.
Three
synthetir
datasets,
one
for
each
pattern,
were
used.
Table
3
presents
the
generator
settings
for
them.
The
weight,wi
average
diameters
of
the
actual
clusters5
,
~)<l,.t
are
also
inclucie[i
in
the
table.
Fig.
6
visualizes
the
actual
clusters
of
1)S
1
hy
plotting
a
cluster
as
a
circle
whose
center
is
the
centroid,
radius
is
the
cluster
ra[iius,
anti
label
is
the
number
of
points
in
the
cluster,
The
BIR(ﬂ™H
clusters
of
DS
1
are
presented
in
Fig.
7.
We
observe
that
the
BIR(™H
clusters
are
very
similar
to
the
actual
clusters
in
terms
of
location,
number
of
points,
and
raclii.
The
maximal
and
average
difference
between
the
centroids
of
an
actual
cluster
and
its
corresponding
BIRCH
cluster
are
0.17
and
0.07
respectively.
The
number
of
points
in
a
BIll(ﬂ~H
cluster
is
no
more
t$han
4°i1
ciifferentl
from
the
correspon(ling
actual
cluster.
The
radii
of
the
BIR(~H
clusters
(ranging
from
1.25
to
1.40
with
an
average
of
1.32)
are
close
to,
those
of
the
actual
rlusters
(
1.41).
Note
that
all
the
BIR(~H
raclii
are
smaller
than
the
actual
radii.
This
is
because
BIRCH
assigns
the
ﬁoutsidersﬂ
of
an
art,
ual
clusters
to
a
proper
BIRCH
cluster.
Similar
conclusions
can
be
reached
by
analyzing
the
visual
presentations
of
DS2
and
13S3
(but
omitted
here
clue
to
the
lack
of
space).
As
summarized
in
Table
4,
it
took
BIRC™H
less
than
50
seconds
(on
an
HP
9000/720
workstation)
to
cluster
100,000
data
points
of
each
dataset,.
The
pattern
of
the
dataset
haci
almost
no
impact
on
the
clustering
time,
Table
4
also
presents
the
performance
results
for
three
additional
ciatasets
Œ
DS
10,
DS20
and
DS30
Œ
which
correspon(i
to
DS
1,
DS2
anti
13S3,
respectively
exceljtl
that
the
parameter
o
of
the
generator
is
set,
to
ordered,
As
demomtrateci
in
Table
4,
changing
the
order
of
the
data
points
had
almost
no
impact,
on
the
performance
of
BIRCH.
6.5
Sensitivity
to
Parameters
We
studied
the
sensitivity
of
BIRCH™S
performance
to
the
change
of
the
values
of
some
parameters.
Due
to
the
lack
of
space,
here
we
can
only
present
some
major
conclusions
(for
details,
see
[Z
RL95]).
5
From
now
on,
we
refer
to
the
clusters
generated
by
tile
generator
as
the
ﬁactual
clustersﬂ
whereas
the
clusters
identifimi
by
BIRCH
as
ﬁt?I~
CH
clustersﬂ.
111
Dataset
C;enerator
Setting
L)act
[
DSI
grid,
f<™
=
100,
nt
=
71h
=
1000,
r~
=
rh
=
42,
kg
=
4,
r-n
=
070,0
=
randomized
2.00
DS2
sine,
K
=
100)711
=
7Lh
=
1000,
r~
=
r~
=
~~,
n.
=
4,
rn
=
070,
0
=
ra7ut07nized
2.00
DX3
random,
Aﬁ
=
100,
nt
=
O,
n},
=
2000,
rt
=
O,
rh
=
4,
T-n
=
rn
=
0~0,
o
=
randomized
4.18
Table
3:
Datasets
Userl
as
Base
Workload
Initial
threshold:
(1)
BIRCH™S
performance
is
stable
as
long
as
the
initial
threshold
is
not
excessively
high
wrt.
the
dataset.
(2)
To
=
0.0
works
well
with
a
little
extra
running
time.
(3)
If
a
user
does
know
a
good
To,
then
she/he
can
be
rewarded
by
saving
up
to
10%
of
the
time.
Page
Size
P:
In
Phase
1,
smaller
(larger)
P
tends
to
ciecrease
(increase)
the
running
time,
requires
higher
(lower)
ending
threshold,
produces
less
(more)
but
ﬁcoarser
(finer)ﬂ
leaf
entries,
and
hence
degrades
(improves)
the
quality.
However
with
the
refinement
in
Phase
4,
the
experiments
suggest
that
from
P
=
256
to
4096
,
although
the
qualities
at
the
end
of
Phase
3
are
different,
the
final
qualities
after
the
refinement
are
almost
the
same.
Outlier
Options:
BIRCH
was
tested
on
ﬁ
noisy™)
datasets
with
all
the
outlier
options
on,
and
o~.
The
results
show
that
with
all
the
outlier
options
on,
BIRCH
is
not
slower
but
faster,
and
at
the
same
time,
its
quality
is
much
better.
Memory
Size:
In
Phase
1,
as
memory
size
(or
the
maximal
tree
size)
increases,
the
running
time
increases
because
of
processing
a
larger
tree
per
rebuilt,
but
only
slightly
because
it
is
clone
in
memory;
(2)
more
but
finer
subclusters
are
generated
to
feed
the
next
phase,
and
hence
results
in
better
quality;
(3)
the
inaccuracy
caused
hy
insufficient
memory
can
be
compensated
to
some
extent
by
Phase
4
refinements.
In
another
word,
BIRCH
can
tradeoff
between
memory
and
time
to
achieve
similar
final
quality.
6.6
Time
Scalability
Two
distinct
ways
of
increasing
the
clataset
size
are
used
to
test
the
scalability
of
BIRCH.
Increasing
the
Number
of
Points
per
Cluster:
For
each
of
DS
1,
DS2
and
DS3,
we
create
a
range
of
clatasets
by
keeping
the
generator
settings
the
same
except
for
changing
711
and
nk
to
change
71,
and
hence
N.
The
running
time
for
the
first
3
phases,
as
well
as
for
all
4
phases
are
plotted
against
the
dataset
size
N
in
Fig.
4.
Both
of
them
are
shown
to
grow
linearly
wrt.
N
consistently
for
all
three
patterns.
Increasing
the
Number
of
Clwst
ers:
For
each
of
DS
1,
DS2
and
DS3,
we
create
a
range
of
datasets
by
keeping
the
generator
settings
the
same
except
for
changing
1{
to
change
N.
The
running
time
for
the
first
3
phases,
as
well
as
for
all
4
phases
are
plotted
against
the
dataset
size
N
in
Fig.
5.
Since
both
N
and
K
are
growing,
and
Phase
4™s
complexity
is
now
0(1<
*N)
(can
be
improved
to
be
almost
linear
in
the
future),
the
total
Dataset
Time
D
Dataset
Time
L)
DS1
47.1
1.87
DS1O
47™.4
1.87
DS2
47.5
1.99
DS20
46.4
1.99
DS3
49.5
3.39
DS30
48.4
3.26
Table
4
Time,
~
arid
171put
Order
4:
BIRCH
Performance
on
Base
Workload
wrt.
.-.
ﬂ
.
II
.
.me
D
I
™10
II
1525.7
I
1
(3.75
Dataset
Time
D
Dat==-~
II
T;,>
DS1
&39.5
2.11
DS
11
I
DS™2
777.5
2.56
DS20
1405.8
179.;:3
DS3
15™20.2
3.36
D%30
2:390.5
6.9:3
Table
5:
CLA
RANS
Performance
on
Base
Workload
wrt.
Ttme,
~
and
Input
Order
time
is
not
exactly
linear
wrt.
N.
However
the
running
time
for
the
first
3
phases
is
again
confirmed
to
grow
linearly
wrt.
N
consistently
for
all
three
patterns.
6.7
Comparison
of
BIRCH
and
CLARANS
In
this
experiment
we
compare
the
performance
of
C™LARAN,$
and
BIRCH
on
the
base
workload.
First
CLARANS
assumes
that
the
memory
is
enough
for
holding
the
whole
clataset,
so
it
needs
much
more
memory
than
BIRCH
does.
In
order
for
CLARAN,S
to
stop
after
an
acceptable
running
time,
we
set
its
7naxnezghbor
value
to
be
the
larger
of
50
(instead
of
250)
and
1.2.5~o
of
K(N-K),
but
no
more
than
100
(newly
enforced
upper
limit
recommended
by
Ng).
Its
numlocal
value
is
still
2.
Fig.
8
visualizes
the
CLA
RANS™
clusters
for
DS
1.
Comparing
them
with
the
actual
clusters
for
DSI
we
can
observe
that:
(1)
The
pattern
of
the
location
of
the
cluster
centers
is
distorted.
(2)
The
number
of
data
points
in
a
CLARAN,S
cluster
can
be
as
many
as
57%
different
from
the
number
in
the
actual
cluster.
(3)
The
radii
of
CLA
RANS
clusters
varies
largely
from
1.15
to
1.94
with
an
average
of
1.44
(larger
than
those
of
the
actual
clusters,
1.41).
Similar
behaviors
can
be
observed
the
visualization
of
CLARAN,S
clusters
for
DS2
and
DS3
(but
omitted
here
due
to
the
lack
of
space).
Table
5
summarizes
the
performance
of
(™LA
RAN,$.
For
all
three
datasets
of
the
base
workload,
(1)
(TLARAN,5™
is
at
least
15
times
slower
than
BIRCH,
and
is
sensi-
tive
to
the
pattern
of
the
dataset.
(2)
The
~
value
for
the
C™LA
RANS
clusters
is
much
larger
than
that
for
the
BIRCH
clusters.
(3)
The
results
for
DS
10,
DS20,
and
DS30
show
that
when
the
data
points
are
ordered,
the
time
and
quality
of
CLARAN,S
degrade
dramati-
cally.
In
conclusion,
for
the
base
workload,
BIRCH
uses
much
less
memory,
hut
is
faster,
more
accurate,
and
less
order-sensitive
compared
with
C™LARAN,S,
112
DS1
:
Phase
1-3
~
D
S2:
P
base
1-3
-----I+----
DS3:
Phase
1-3
.
.
.
...=
.
DS1
:
Phase
1-4
G
D
S2:
Phase
1-4
---------
DS3:
Phase
1-4
------
1
0
L
I
0
100000
200000
Number
of
Tuples
(N)
Figure
4:
scalability
wrt.
I?tcreasing
ILL,
n},
140
120
100
g
~
80
i%
.Š
l--
c
z
60
40
20
0
DS1
:
Phase
1-3
Š
~
~
D
S2:
P
base
i
-3
-----u----
,.;2
DS3:
Phase
1-3
------=
-.;
j™\
DS1
:
Phase
1-4
-
ok,ﬂ
D
S2:
P
base
1-4
----7~~
DS3:
Phase
1-4
--;
~,.J™-
,#™
/
,/.™ﬂ
./
,
/
,/
I
o
100000
200000
Number
of
Tuples
(N)
Figure
5:
,Scalabilit~j
7mt.
[nmmsing
K
6.8
Application
to
Real
Datasets
BIli(ﬂ™H
has
been
used
for
filtering
real
images,
Fig.
9
are
two
similar
images
of
trees
with
a
partly
cloudy
sky
as
the
background,
taken
in
two
different
wavelengt
11s.
The
top
one
is
in
near-infrared
hand
(NIR),
and
the
bottom
one
is
in
visible
wavelength
band
(VIS).
Each
image
contains
512xl1324
pixels,
and
each
pixel
act)llally
has
a
pair
of
brightness
values
corresponding
to
NIR,
and
VIS.
Soil
scientists
receive
hundreds
of
such
image
pairs
and
try
to
first
filter
the
trees
from
the
background,
and
then
filter
the
trees
into
sunlit
leaves,
shadows
and
branches
for
statistical
analysis.
We
applied
BIRCH
to
the
(NIR,,
VIS)
value
pairs
for
all
pixels
in
an
image
(512X
1024
2-d
tuples)
hy
using
400
khytes
of
rnernory
(about
5%,
of
the
dataset
size)
and
80
khytes
of
disk
space
(about
20%
of
the
rnernory
size),
0
1.
20
m
4,
Figure
6:
Actual
C™lustmx
of
D,5™1
Figure
7:
BIRCH
Clusters
of
DiSl
and
weighting
NI
R
and
VIS
values
equally.
We
obtained
5
clust,
ers
that,
correspond
to
(
1)
very
bright
part
of
sl{y,
(2)
ordinary
part
of
sky,
(3)
clouds,
(4)
sunlit
leaves
(5)
tree
branches
and
shadows
on
the
trees.
This
step
took
284
seconds.
However
the
branches
and
shadows
were
too
sinlilar
to
be
distinguished
from
each
other,
although
we
COU1[l
separate
them
from
the
other
[™luster
categories,
So
we
pulled
out
the
part
of
the
data
corresponding
to
(.5)
(
146707
2-d
tuples)
and
used
BIRCH
again,
But,
this
time,
(1)
NIR
was
weighted
10
times
heavier
than
VIS
because
we
observed
that
branches
and
shadows
were
easier
to
tell
apart
from
the
NIR
image
than
from
the
VIS
image;
(2)
BIRCH
ended
with
a
finer
threshold
because
it
processed
a
smaller
dataset
wit,h
the
same
amount,
of
memory.
The
two
clusters
corresponding
to
branches
and
shadows
were
obtained
with
71
secon(ls,
Fig.
10
shows
the
parts
of
image
that,
correspond
to
113
Figure
9:
The
ima~ges
taken
in
NIR
and
VIS
.h.
dt...
.
,:.<..,
,,,
=
,
.
.
.
.
.
.
.
)..
..,..
Figure
10:
The
sunlit
leaves,
branches
and
shadows
sunlit
leaves,
tree
branches
and
shadows
on
the
trees,
obtained
hy
clustering
using
BIRCH.
Visually,
we
can
see
that,
it
is
asatisfactory
filteringof
the
original
image
according
to
the
user™s
intention.
7
Summary
and
Future
Research
BIli(~H
is
a
clustering
method
for
very
large
datasets.
It
makes
a
large
clustering
problem
tractable
hy
con-
centrating
on
densely
occupied
portions,
and
using
a
compact
summary.
It
utilizes
measurements
that
cap-
ture
the
natural
closeness
of
data.
These
measurements
can
he
stored
and
updated
incrementally
in
a
height-
balanced
tree.
BIRCH
can
work
with
any
given
amount
of
rnernory,
and
the
1/()
complexity
is
a
little
more
than
one
scan
of
data.
Experimentally,
BIRCH
is
shown
to
perform
very
well
on
several
large
datasetsj
and
is
signif-
icantly
superior
to
CLARANS
in
terms
of
quality,
speed
and
order-sensitivity.
Proper
parameter
setting
is
important
to
BIR(;H™s
diiciency.
In
the
near
future,
we
will
concentrate
on
studying
(1)
more
reasonable
ways
of
increasing
the
threshold
dynamically,
(2)
the
dynamic
adjustment
of
outlier
criteria,
(3)
more
accurate
quality
rneasure-
me.nts,
and
(4)
data
parameters
that
are
good
indica-
tors
of
how
well
BIRCH
is
likely
to
perform.
We
will
explore
BIRCH™S
architecture
for
opportunities
of
par-
allel
executions
as
well
as
interactive
learnings.
As
an
incremental
algorithrr,
BIRCH
will
be
able
to
read
data
directly
from
a
tape
drive,
or
from
network
by
matchi-
ng
its
clustering
speed
with
the
data
reading
speed.
We
will
also
study
how
to
make
use
of
the
clustering
infor-
mation
obtained
to
help
solve
problems
such
as
storage
or
query
optimization,
and
data
compression.
References
[CKS88]
Peter
(Ubeeseman,
James
Kelly,
Matthew
Self,
et
al.,
Auto
Class
:
A
Bayesian
(Ylassijlcation
SUstem,
Proc.
of
the
5tb
Int™1
Couf.
on
Machine
Learning,
Morgan
Kaufmau,
Jun.
1988.
[DH7:3]
Richard
Duda,
and
Peter
E.
Hart,
Patter,L
C/aSSijiCatiOn
and
Sce7ze
Analysis,
Wiley,
1973.
[DJ80]
f%.
Dubes,
and
A.K.
Jaiu,
G™Imter-ing
Methodologies
in
Ezplorator~
Data
Anczlgsis
Advances
in
(.~omputers,
Edited
by
M.C.
Yovits,
Vol.
19,
Academic
F™ress,
New
York,
19/30.
[EKX95a]
Martin
Ester,
Haus-Peter
Kriegel,
aud
Xiaowei
Xu,
A
Database
Interface
for
Clustering
in
Largr
Spatial
Databases,
Proc.
of
1st
Int™1
Conf.
ou
Kuowledge
Discuvery
aud
Data
Miuiug,
1995.
[EKX95b]
Martiu
Ester,
Haus-Peter
Kriegel,
and
Xiaowei
Xu,
Knowl?dgr
Discouery
in
Larg?
Spatial
f)atabas~s:
Focusing
Techniques
for
Eficie~~t
~lass
Identijlcation,
Proc.
of
4tb
Int™1
Symposium
ou
Large
Spatial
Databases,
F)ortlaud,
Maine,
I-J.
S.
A.,
1995.
[Fis87]
Douglas
H.
Fisher,
Knowledge
Acqui~itzon
uia
lncr.mew
f~l
(Tone-rptual
Clustering,
Machine
Leamiug,
2(2),
1987
[Fis95]
Douglas
H.
Fisher,
Iterative
(optimization
and
Simp[ijica-
tion
of
Hierarchical
Clusterings,
Technical
Report
(}
S-95-01,
Dept.
of
Computer
Science,
Vanderbilt
IIuiver-sity,
Nashville,
TN
:372:35.
[(1(+92]
A.
Gersbo
and
R.
Gray,
Vector
quantization
and
signal
compression,
Boston,
Ma.:
Kluwer
Academic
F)ublishers,
1992.
[KR90]
Leonard
Kaufman,
and
Peter
J.
Rousseeuw,
Finding
Groups
in
Data
-
An
I?Ltroductio?L
to
Cluster
Analysis,
Wiley
Series
in
F™robability
and
Mathematical
Statistics,
1990.
[Leb87]
Michael
Lebowitz,
Experiment.
with
Incremental
(;on-
cept
Formation
:
UNIMEM,
Machine
Learuiug,
1987.
[Lee81]
R.
C.
T.Lee,
Clustering
anrdgsis
ar,d
its
application.,
A+
vauces
in
Information
Systems
,Science,
Edited
by
J
,T.
Toum,
Vol.
8,
pp.
169-292,
Plenum
Press,
New
York,
1981,
[Mur8:3]
F.
Murtagb,
A
Survey
of
Recent
Advance.
in
Hierarchi-
cal
6™lustering
A~g
Orithms,
The
(.;
omputer
Jourmal,
1%3:3.
[NH94]
Raymond
T.
Ng
and
Jiawei
Hau,
Ejficimt
and
,Eflectiue
(blustering
Methods
for
Spatial
Data
Mining,
F™roe,
of
VLDB,
1994.
[01s9:3]
Clark
F.
L31son,
Parallel
Algorithms
for
Hierarchical
C™[usteri?~g,
Technical
Report,
Computer
Scieuce
Divisiou,
IIuiv.
of
California
at
Berkeley,
Dec.,1993.
[ZRL95]
Tian
Zbau.g,
Ragbu
Rau)akt-ishuan,
aud
Mirou
Liv,,y,
BIRCH:
An
Ef%cient
Data
Clustering
Mtthod
for
VPTV
Largr
Databases,
Technical
Report,
Computer
Scieuces
Dept.,
(Juiv.
of
Wisconsiu-Madison,
1995,
114
