Squeeze-and-ExcitationNetworks
JieHu
1

LiShen
2

GangSun
1
hujie@momenta.ailishen@robots.ox.ac.uksungang@momenta.ai
1
Momenta
2
Department ofEngineeringScience,UniversityofOxford
Abstract
Convolutionalneuralnetworksarebuiltuponthecon-
volutionoperation,whichextractsinformativefeatures by

fusingspatial andchannel-wiseinformationtogetherwithin

localreceptiveelds.Inordertoboosttherepresenta-

tionalpowerofanetwork,severalrecentapproacheshave

shownthebenet ofenhancingspatialencoding.Inthis

work,wefocusonthe channelrelationshipandpropose

anovelarchitecturalunit,whichwetermthe Ô¨ÅSqueeze-

and-ExcitationÔ¨Ç(SE)block,thatadaptivelyrecalibrates

channel-wise featureresponsesby explicitlymodellingin-

terdependencies betweenchannels.Wedemonstratethatby

stackingtheseblockstogether,wecanconstructSENetar-

chitecturesthatgeneraliseextremelywellacrosschalleng-

ingdatasets.Crucially,wendthatSEblocksproduce

signicantperformanceimprovementsforexistingstate-of-

the-artdeeparchitecturesatminimaladditionalcomputa-

tionalcost.SENetsformedthefoundationofourILSVRC

2017classicationsubmissionwhichwonrst placeand

signicantlyreducedthetop-5errorto
2
:
251%
,achiev-
inga
Àò
25%
relativeimprovementoverthewinningen-
tryof2016.Codeand modelsareavailableat
https:
//github.com/hujie-frank/SENet
.
1.Introduction
Convolutionalneuralnetworks(CNNs) haveprovento
beeffectivemodels fortacklingavarietyofvisualtasks

[21
,
27
,
33
,
45
].Foreachconvolutionallayer,asetof
lters are learnedtoexpresslocalspatialconnectivitypat-

ternsalonginputchannels.Inotherwords,convolutional

ltersareexpectedtobeinformativecombinationsbyfus-

ingspatial andchannel-wiseinformation togetherwithinlo-

calreceptiveelds.Bystackingaseriesofconvolutional

layersinterleavedwithnon-linearitiesanddownsampling,

CNNsarecapableofcapturinghierarchicalpatternswith

globalreceptiveeldsaspowerfulimagedescriptions.Re-

centworkhasdemonstratedthattheperformanceofnet-

workscanbeimprovedbyexplicitlyembeddinglearning

Equalcontribution.
mechanismsthathelpcapturespatialcorrelationswithout

requiringadditionalsupervision.Onesuchapproachwas

popularisedbytheInceptionarchitectures[
16
,
43
],which
showedthatthenetworkcanachievecompetitiveaccuracy

byembeddingmulti-scaleprocessesinitsmodules.More

recentworkhassoughttobetter modelspatialdependence

[1
,
31
] andincorporatespatialattention[
19
].
Inthispaper,weinvestigateadifferentaspectofarchi-
tecturaldesign-thechannelrelationship,byintroducinga

newarchitecturalunit,whichwetermtheÔ¨Å
Squeeze-and-
Excitation
Ô¨Ç(SE)block.Ourgoalistoimprovetherep-
resentationalpowerofanetworkbyexplicitlymodelling

theinterdependenciesbetweenthechannelsofitsconvolu-

tionalfeatures.Toachievethis,weproposeamechanism

thatallowsthenetworktoperform
featurerecalibration
,
through whichitcanlearntouseglobalinformationtose-

lectivelyemphasiseinformativefeaturesand suppress less

usefulones.
Thebasicstructureof theSEbuildingblockisillustrated
inFig.
1
. Foranygiventransformation
F
tr
:
X
!
U
,
X
2
R
H
0 

W
0 

C
0 
;
U
2
R
H

W

C
,(e.g.aconvolution
orasetofconvolutions),wecanconstructacorrespond-

ingSEblocktoperformfeaturerecalibrationasfollows.

Thefeatures
U
arerstpassedthrougha
squeeze
opera-
tion,whichaggregatesthefeaturemaps acrossspatialdi-

mensionsH

W
toproduceachanneldescriptor.This
descriptorembedstheglobaldistributionofchannel-wise

featureresponses,enablinginformationfromtheglobalre-

ceptiveeldofthenetworktobeleveragedbyitslowerlay-

ers.Thisisfollowedbyan
excitation
operation,inwhich
sample-specic activations,learnedfor eachchannelbya

self-gatingmechanismbasedonchanneldependence,gov-

erntheexcitationofeachchannel.Thefeaturemaps
U
arethenreweightedtogeneratetheoutputoftheSEblock

whichcan thenbefed directlyintosubsequentlayers.
AnSEnetworkcanbe generatedbysimplystackinga
collectionofSEbuildingblocks.SEblockscanalsobe

usedasadrop-inreplacement forthe originalblockat
any
depth
inthe architecture.However,whilethetemplatefor
thebuilding blockisgeneric,asweshowinSec.
6.4
,the
roleitperformsatdifferentdepthsadaptstotheneedsofthe

network.Intheearlylayers,itlearnstoexciteinformative
7132

Figure1:ASqueeze-and-Excitationblock.
featuresinaclassagnosticmanner,bolsteringthequalityof

thesharedlowerlevel representations.In later layers,the

SEblockbecomes increasinglyspecialised, andresponds

todifferentinputsinahighly
class-specic
manner.Con-
sequently,thebenetsof featurerecalibrationconductedby

SEblockscanbeaccumulatedthroughtheentirenetwork.
ThedevelopmentofnewCNNarchitecturesisa chal-
lengingengineeringtask,typicallyinvolvingtheselection

ofmanynewhyperparametersandlayercongurations.By

contrast,thedesign oftheSEblockoutlinedaboveissim-

ple,andcanbeuseddirectlywithexistingstate-of-the-art

architectureswhosemodulescanbestrengthenedbydirect

replacementwiththeirSEcounterparts.
Moreover,asshowninSec.
4
,SEblocksarecomputa-
tionallylightweightandimpose onlyaslightincreasein

modelcomplexityandcomputationalburden.Tosupport

these claims,wedevelopseveralSENetsandprovidean

extensiveevaluationontheImageNet2012dataset[
34
].
Todemonstratetheirgeneralapplicability,wealsopresent

results beyondImageNet,indicatingthattheproposed ap-

proachisnotrestrictedtoaspecicdatasetoratask.
UsingSENets,wewontherstplaceintheILSVRC
2017classicationcompetition.Ourtopperformingmodel

ensembleachievesa
2
:
251%
top-5erroronthetestset
1
.
Thisrepresentsa
Àò
25%
relativeimprovementincompari-
sontothewinnerentryofthepreviousyear(withatop-5

errorof
2
:
991%
).
2.RelatedWork

Deeparchitectures.
VGGNets[
39
]andInceptionmod-
els[
43
]demonstratedthe benetsofincreasingdepth.
Batchnormalization(BN)[
16
]improvedgradientpropa-
gationbyinsertingunitstoregulatelayerinputs,stabilis-

ingthelearningprocess. ResNets[
10
,
11
]showedtheef-
fectivenessoflearningdeepernetworksthrough theuseof

identity-basedskipconnections.Highwaynetworks[
40
]
employedagatingmechanismtoregulateshortcutconnec-

tions.Reformulationsoftheconnectionsbetweennetwork

layers[
5
,
14
]havebeen showntofurther improvethelearn-
ingandrepresentationalpropertiesofdeepnetworks.
Analternativelineof researchhasexplored waystotune
thefunctionalformofthemodularcomponentsofanet-

work.Groupedconvolutionscanbeusedtoincreasecar-
1
http://image-net.org/challenges/LSVRC/2017/results
dinality(thesizeofthesetoftransformations)[
15
,
47
].
Multi-branchconvolutionscan beinterpretedasa generali-

sationofthisconcept,enablingmoreexiblecompositions

ofoperators[
16
,
42
,
43
,
44
].Recently,compositionswhich
havebeenlearnedinanautomatedmanner[
26
,
54
,
55
]
haveshowncompetitiveperformance.Cross-channelcor-

relationsaretypicallymappedasnewcombinations offea-

tures,eitherindependentlyofspatialstructure[
6
,
20
]or
jointlybyusingstandardconvolutionallters[
24
]with
1

1
convolutions.Muchofthisworkhasconcentratedonthe

objectiveofreducingmodelandcomputationalcomplexity,

reectinganassumptionthatchannelrelationshipscanbe

formulatedasa compositionofinstance-agnosticfunctions

withlocalreceptiveelds. Incontrast,weclaimthatprovid-

ingtheunitwithamechanismtoexplicitlymodeldynamic,

non-lineardependenciesbetweenchannelsusingglobalin-

formationcaneasethelearningprocess,andsignicantly

enhance therepresentationalpowerofthenetwork.

Attention andgatingmechanisms.
Attentioncanbe
viewed,broadly,asatooltobiastheallocationofavailable

processingresourcestowardsthemostinformativecompo-

nentsofaninputsignal[
17
,
18
,
22
,
29
,
32
].Thebenets
ofsuchamechanismhavebeenshownacrossarangeof

tasks,fromlocalisationandunderstandinginimages[
3
,
19
]
tosequence-basedmodels[
2
,
28
].Itistypicallyimple-
mentedincombinationwithagatingfunction(e.g.asoft-

maxorsigmoid)andsequentialtechniques[
12
,
41
].Re-
centworkhasshownitsapplicabilitytotaskssuchasim-

agecaptioning[
4
,
48
]andlipreading[
7
].Intheseappli-
cations,itisoftenusedontopofoneormorelayersrep-

resentinghigher-levelabstractionsforadaptationbetween

modalities.Wangetal.[
46
]introduceapowerfultrunk-
and-maskattentionmechanismusinganhourglassmodule

[31
].Thishighcapacityunitisinsertedintodeepresid-
ualnetworksbetweenintermediatestages.Incontrast,our

proposedSE block isalightweightgatingmechanism,spe-

cialisedto modelchannel-wiserelationshipsinacomputa-

tionallyefcientmanneranddesignedtoenhancetherepre-

sentationalpowerofbasicmodulesthroughoutthenetwork.

3.Squeeze-and-ExcitationBlocks
The
Squeeze-and-Excitation
blockisacomputational
unitwhichcanbeconstructedforanygiventransformation

F
tr
:
X
!
U
;
X
2
R
H
0

W
0

C
0
;
U
2
R
H

W

C
.For
7133

simplicity,inthenotationthatfollows wetake
F
tr
tobea
convolutionaloperator. Let
V
=[
v
1
;
v
2
;:::;
v
C
]
denote
thelearnedsetoflterkernels,where
v
c
referstothepa-
rametersofthe
c
-thlter.Wecanthenwritetheoutputsof
F
tr
as
U
=[
u
1
;
u
2
;:::;
u
C
]
,where
u
c
=
v
c

X
=
C
0 
X
s
=1
v
s
c

x
s
:
(1)
Here

denotesconvolution,
v
c
=[
v
1
c
;
v
2
c
;:::;
v
C
0 
c
]
and
X
=[
x
1
;
x
2
;:::;
x
C
0 
]
(tosimplifythenotation,biasterms
areomitted),while
v
s
c
isa
2
Dspatialkernel,andtherefore
representsasinglechannelof
v
c
whichactsonthecorre-
spondingchannelof
X
.Sincetheoutputisproducedby
asummationthroughallchannels,thechanneldependen-

ciesareimplicitlyembeddedin
v
c
,butthesedependencies
areentangledwiththespatialcorrelationcapturedbythe

lters.Ourgoalistoensurethatthenetworkisabletoin-

creaseitssensitivitytoinformativefeaturessothattheycan

beexploitedbysubsequenttransformations,and tosuppress

lessusefulones.Weproposetoachievethisbyexplicitly

modellingchannelinterdependenciestorecalibratelterre-

sponsesintwo steps,
squeeze
and
excitation
,beforetheyare
fedintonexttransformation. AdiagramofanSEbuilding

blockisshowninFig.
1
.
3.1.Squeeze:GlobalInformationEmbedding
Inorderto tackletheissueofexploitingchanneldepen-
dencies,werstconsiderthesignaltoeachchannelinthe

outputfeatures.Eachofthe learnedltersoperates witha

localreceptiveeldandconsequentlyeachunitofthetrans-

formationoutput
U
isunabletoexploitcontextualinforma-
tionoutsideofthisregion.Thisisanissue thatbecomes

moresevereinthelowerlayersof thenetworkwhosere-

ceptiveeldsizesaresmall.
Tomitigatethisproblem,weproposeto
squeeze
global
spatial informationinto achanneldescriptor.Thisis

achievedbyusingglobalaveragepoolingtogenerate

channel-wisestatistics.Formally,astatistic
z
2
R
C
isgen-
eratedbyshrinking
U
throughspatialdimensions
H

W
,
wherethe
c
-thelementof
z
iscalculatedby:
z
c
=
F
sq
(
u
c
)=
1
H

W
H
X
i
=1
W
X
j
=1
u
c
(
i;j
)
:
(2)
Discussion.
Thetransformationoutput
U
canbein-
terpretedasa collectionofthelocaldescriptorswhose

statisticsareexpressiveforthewholeimage.Exploiting

suchinformationisprevalentinfeatureengineeringwork

[35
,
38
,
49
].Weoptforthesimplest,globalaveragepool-
ing,notingthatmoresophisticatedaggregationstrategies

couldbeemployedhereaswell.

3.2.Excitation:Adaptive Recalibration
Tomakeuseofthe informationaggregatedinthe
squeeze
operation,wefollowitwithasecondoperationwhichaims
tofullycapture channel-wisedependencies.Tofullthis

objective,thefunctionmustmeettwocriteria:rst,it must

be exible(inparticular,itmustbe capable of learning a

nonlinear interactionbetweenchannels)andsecond,itmust

learn anon-mutually-exclusiverelationshipsincewewould

liketoensurethatmultiplechannelsareallowedtobeem-

phasisedopposedtoone-hotactivation.Tomeet thesecri-

teria,weopttoemployasimplegatingmechanismwitha

sigmoidactivation:
s
=
F
ex
(
z
;
W
)=
Àô
(
g 
(
z
;
W
))=
Àô
(
W
2

(
W
1
z
))
;
(3)
where

referstotheReLU[
30
]function,
W
1
2
R
C
r

C
and
W
2
2
R
C

C
r
.Tolimitmodelcomplexityandaidgenerali-
sation,weparameterisethegatingmechanismbyforminga

bottleneckwithtwofullyconnected(FC)layersaroundthe

non-linearity,i.e.adimensionality-reductionlayerwithpa-

rametersW
1
withreductionratio
r
(thisparameterchoice
isdiscussedinSec.
6.4
),aReLUandthenadimensionality-
increasinglayerwithparameters
W
2
.The naloutputof
theblockisobtainedbyrescalingthetransformation output

U
withtheactivations:
e
x
c
=
F
scale
(
u
c
;s
c
)=
s
c

u
c
;
(4)
where
e
X
=[
e
x
1
;
e
x
2
;:::;
e
x
C
]
and
F
scale
(
u
c
;s
c
)
refersto
channel-wisemultiplicationbetweenthefeaturemap
u
c
2
R
H

W
andthescalar
s
c
.
Discussion.
Theactivationsactaschannelweights
adaptedtotheinput-specicdescriptor
z
.Inthisregard,
SEblocksintrinsicallyintroducedynamicsconditionedon

theinput,helpingtoboostfeaturediscriminability.

3.3.Exemplars:and
Itisstraightforward toapplytheSEblocktoAlexNet
[
21
]andVGGNet[
39
].TheexibilityoftheSEblock
meansthatitcanbedirectlyappliedtotransformationsbe-

yondstandardconvolutions.Toillustratethispoint,wede-

velopSENetsbyintegratingSEblocksintomodernarchi-

tectureswithsophisticateddesigns.
Fornon-residualnetworks,suchasInceptionnetwork,
SEblocksareconstructedforthe networkby takingthe

transformationF
tr
tobeanentireInceptionmodule(see
Fig.
2
).Bymakingthischangefor eachsuchmodule
inthearchitecture,weconstructan
SE-Inception
network.
Moreover,SEblocksaresufcientlyexibletobeusedin

residualnetworks. Fig.
3
depictstheschemaofan
SE-
ResNet
module.Here,theSEblocktransformation
F
tr
istakentobethenon-identitybranchofaresidualmod-

ule.Squeeze
and
excitation
bothactbeforesummation
withtheidentitybranch.Morevariantsthatintegratewith

ResNeXt[
47
],Inception-ResNet[
42
],MobileNet[
13
]and
ShufeNet[
52
]canbeconstructedbyfollowingthesimi-
larschemes.WedescribethearchitectureofSE-ResNet-50

andSE-ResNeXt-50inTable
1
.
7134

Inception
Global pooling
FC
!"
#$%&'()*+%,-+./0'
FC
Inception
√©
$%&'()*+%,-+./0'
√©
Sigmoid
Scale
ReLU
*H
W
H
C
sHsH
sHsH
sHsH
sHsH
N
sHsH
N
*H
W
H
C
Figure2:The schemaoftheoriginalInceptionmodule(left)and

theSE-Inceptionmodule(right).
!"
#$%&'%()*+,-%
+
Global pooling
FC
ReLU
+
$%&'%()*+,-%
√©
√©
Sigmoid
sHsH
sHsH
N
sHsH
sHsH
Scale
*H
W
H
C
*H
W
H
*H
W
H
Residual
Residual
FC
sHsH
N
Figure3:TheschemaoftheoriginalResidualmodule(left)and

theSE-ResNet module(right).

4.Model andComputationalComplexity
FortheproposedSEblocktobeviableinpractice,it
mustprovideaneffectivetrade-offbetweenmodelcom-

plexityandperformancewhichisimportantforscalability.

We setthereductionratio
r
tobe16inallexperiments,ex-
ceptwherestatedotherwise(morediscussioncanbefound

inSec.
6.4
).Toillustratethecostofthemodule,wetake
thecomparisonbetweenResNet-50andSE-ResNet-50as

anexample, wherethe accuracyofSE-ResNet-50issupe-

riorto ResNet-50andapproachesthatofadeeper ResNet-

101network(showninTable
2
).ResNet-50requires
Àò
3
:
86
GFLOPsinasingleforwardpassfora
224

224
pixelin-
putimage.EachSEblockmakesuseof aglobalaverage

poolingoperationin the
squeeze
phaseandtwosmallfully
connectedlayersinthe
excitation
phase,followedby an
inexpensivechannel-wisescalingoperation.In aggregate,

SE-ResNet-50requires
Àò
3
:
87
GFLOPs,correspondingtoa
0
:
26%
relativeincrease overtheoriginalResNet-50.
Inpractice,withatrainingmini-batchof
256
images,
asinglepassforwardsandbackwardsthroughResNet-50

takes
190
ms,comparedto
209
msforSE-ResNet-50(both
timingsareperformedonaserverwith
8
NVIDIATitanX
GPUs).Wearguethatthisrepresentsareasonableoverhead

particularlysince globalpoolingandsmallinner-product

operationsarelessoptimisedinexistingGPUlibraries.

Moreover,duetoitsimportanceforembeddeddeviceap-

plications,wealsobenchmarkCPUinferencetimeforeach

model:fora
224

224
pixelinputimage,ResNet-50takes
164
ms,comparedto
167
msforSE-ResNet-50.Thesmall
additionalcomputationaloverheadrequired bytheSEblock

isjustiedbyitscontributiontomodelperformance.
Next,weconsiderthe additionalparametersintroduced
bytheproposedblock.Allof themarecontainedinthetwo

FClayersofthegatingmechanism,whichconstituteasmall

fractionofthetotalnetwork capacity.Moreprecisely,the

numberofadditionalparametersintroducedisgivenby:
2
r
S
X
s
=1
N
s

C
s
2
(5)
where
r
denotesthereductionratio,
S
referstothenum-
berofstages(whereeachstagereferstothecollectionof

blocksoperatingonfeaturemapsofacommonspatialdi-

mension),C
s
denotesthedimensionoftheoutputchannels
and
N
s
denotestherepeatedblocknumberforstage
s
.SE-
ResNet-50introduces
Àò
2
:
5
millionadditionalparameters
beyondthe
Àò
25
millionparametersrequiredbyResNet-50,
correspondingtoa
Àò
10%
increase.The majorityofthese
parameterscomefromthelaststageofthenetwork, where

excitationisperformedacrossthegreatestchanneldimen-

sions.However,wefoundthatthecomparativelyexpensive

nalstageofSEblocks couldberemovedatamarginalcost

inperformance (
<
0
:
1%
top-1erroronImageNet)toreduce
therelativeparameterincreaseto
Àò
4%
,whichmayprove
usefulincaseswhereparameterusageisakeyconsidera-

tion(seefurtherdiscussioninSec.
6.4
).
5.Implementation
EachplainnetworkanditscorrespondingSEcounter-
part aretrainedwithidenticaloptimisationschemes.Dur-

ingtrainingonImageNet,wefollowstandardpracticeand

performdataaugmentationwithrandom-sizecropping[
43
]
to
224

224
pixels(
299

299
forInception-ResNet-v2
[
42
]andSE-Inception-ResNet-v2)andrandomhorizontal
ipping.Inputimagesarenormalisedthrough mean chan-

nelsubtraction.Inaddition,weadoptthedatabalancing

strategydescribedin[
36
]formini-batchsampling.The
networksaretrainedonourdistributedlearningsystem

Ô¨ÅROCSÔ¨Çwhichisdesignedtohandleefcientparalleltrain-

ingoflargenetworks.Optimisationisperformedusingsyn-

chronousSGDwith momentum
0
:
9
andamini-batchsize
of
1024
.Theinitial learningrateissetto
0
:
6
anddecreased
byafactorof
10
every
30
epochs.Allmodelsaretrained
7135

Outputsize
ResNet-50
SE-ResNet-50
SE-ResNeXt-50(
32

4
d)
112

112
conv,
7

7
,
64
,stride
2
56

56
maxpool,
3

3
,stride
2
2

4
conv
;
1

1
;
64
conv
;
3

3
;
64
conv
;
1

1
;
256
3

5

3
2

6

6

4
conv
;
1

1
;
64
conv
;
3

3
;
64
conv
;
1

1
;
256
fc;
[16
;
256]
3

7

7

5

3
2

6

6

4
conv
;
1

1
;
128
conv
;
3

3
;
128
C
=32
conv
;
1

1
;
256
fc;
[16
;
256]
3

7

7

5

3
28

28
2

4
conv
;
1

1
;
128
conv
;
3

3
;
128
conv
;
1

1
;
512
3

5

4
2

6

6

4
conv
;
1

1
;
128
conv
;
3

3
;
128
conv
;
1

1
;
512
fc;
[32
;
512]
3

7

7

5

4
2

6

6

4
conv
;
1

1
;
256
conv
;
3

3
;
256
C
=32
conv
;
1

1
;
512
fc;
[32
;
512]
3

7

7

5

4
14

14
2

4
conv
;
1

1
;
256
conv
;
3

3
;
256
conv
;
1

1
;
1024
3

5

6
2

6

6

4
conv
;
1

1
;
256
conv
;
3

3
;
256
conv
;
1

1
;
1024
fc;
[64
;
1024]
3

7

7

5

6
2

6

6

4
conv
;
1

1
;
512
conv
;
3

3
;
512
C
=32
conv
;
1

1
;
1024
fc;
[64
;
1024]
3

7

7

5

6
7

7
2

4
conv
;
1

1
;
512
conv
;
3

3
;
512
conv
;
1

1
;
2048
3

5

3
2

6

6

4
conv
;
1

1
;
512
conv
;
3

3
;
512
conv
;
1

1
;
2048
fc;
[128
;
2048]
3

7

7

5

3
2

6

6

4
conv
;
1

1
;
1024
conv
;
3

3
;
1024
C
=32
conv
;
1

1
;
2048
fc;
[128
;
2048]
3

7

7

5

3
1

1
globalaveragepool,
1000
-d
fc
,softmax
Table1:(
Left
)ResNet-50.(
Middle
)SE-ResNet-50.(
Right
)SE-ResNeXt-50witha32

4dtemplate.Theshapesandoperationswith
specicparametersettingsofaresidualbuildingblock arelistedinsidethebracketsandthenumberofstackedblocksinastageispresented

outside.Theinnerbracketsfollowingby
fc
indicatestheoutputdimensionofthetwofullyconnected layersinan SEmodule.
original
re-implementation
SENet
top-1err.
top-5err.
top-1err.
top-5err.
GFLOPs
top-1err.
top-5err.
GFLOPs
ResNet-50[
10
]
24
:
7
7
:
8
24
:
80
7
:
48
3
:
86
23
:
29
(1
:
51)
6
:
62
(0
:
86)
3
:
87
ResNet-101[
10
]
23
:
6
7
:
1
23
:
17
6
:
52
7
:
58
22
:
38
(0
:
79)
6
:
07
(0
:
45)
7
:
60
ResNet-152[
10
]
23
:
0
6
:
7
22
:
42
6
:
34
11
:
30
21
:
57
(0
:
85)
5
:
73
(0
:
61)
11
:
32
ResNeXt-50[
47
]
22
:
2
-
22
:
11
5
:
90
4
:
24
21
:
10
(1
:
01)
5
:
49
(0
:
41)
4
:
25
ResNeXt-101[
47
]
21
:
2
5
:
6
21
:
18
5
:
57
7
:
99
20
:
70
(0
:
48)
5
:
01
(0
:
56)
8
:
00
VGG-16[
39
]
-
-
27
:
02
8
:
81
15
:
47
25
:
22
(1
:
80)
7
:
70
(1
:
11)
15
:
48
BN-Inception[
16
]
25
:
2
7
:
82
25
:
38
7
:
89
2
:
03
24
:
23
(1
:
15)
7
:
14
(0
:
75)
2
:
04
Inception-ResNet-v2 [
42
]
19
:
9
y
4
:
9
y
20
:
37
5
:
21
11
:
75
19
:
80
(0
:
57)
4
:
79
(0
:
42)
11
:
76
Table2: Single-croperrorrates(%)ontheImageNetvalidationsetandcomplexitycomparisons.The
original
columnreferstotheresults
reportedintheoriginalpapers.Toenableafaircomparison,were-trainthebaselinemodelsandreportthescoresinthe
re-implementation
column.The
SENet
columnreferstothecorrespondingarchitecturesinwhichSE blockshavebeenadded.Thenumbersinbracketsdenote
theperformanceimprovementoverthere-implementedbaselines.
y
indicatesthatthemodelhasbeenevaluatedonthenon-blacklisted
subsetofthevalidationset(thisisdiscussedinmoredetailin[
42
]),whichmayslightlyimproveresults.VGG-16andSE-VGG-16are
trainedwithbatchnormalization.

for100
epochsfromscratch,usingtheweightinitialisation
strategydescribedin[
9
].
Whentesting,weapplyacentrecropevaluationonthe
validationset,where
224

224
pixelsarecropped fromeach
imagewhoseshorteredgeisrstresizedto
256
(
299

299
fromeachimagewhoseshorteredgeisrstresizedto
352
forInception-ResNet-v2andSE-Inception-ResNet-v2).

6.Experiments

6.1.ImageNetClassication
TheImageNet2012datasetiscomprisedof
1
:
28
mil-
liontrainingimagesand
50
Kvalidationimagesfrom
1000
classes.Wetrainnetworksonthetrainingsetandreportthe

top-1andthetop-5errors.
Networkdepth.
Werst compare theSE-ResNetagainst
ResNetarchitectureswithdifferentdepths.Theresultsin

Table
2
showsthatSEblocksconsistentlyimproveperfor-
manceacrossdifferentdepths with anextremely smallin-

creaseincomputationalcomplexity.
Remarkably,SE-ResNet-50achievesasingle-croptop-5
validationerrorof6.62%,exceedingResNet-50(7.48%)

by0.86%andapproachingtheperformanceachievedby

themuchdeeperResNet-101network (6.52%top-5error)

withonlyhalfofthe computationaloverhead(
3
:
87
GFLOPs
vs.
7
:
58
GFLOPs).Thispatternisrepeatedatgreater
depth,whereSE-ResNet-101(
6
:
07%
top-
5
error)notonly
matches,butoutperformsthedeeperResNet-152network

(6.34%top-5error)by
0
:
27%
.Fig.
4
depictsthetraining
andvalidationcurvesofSE-ResNet-50andResNet-50(the
7136

original
re-implementation
SENet
top-1
err.
top-5
err.
top-1
err.
top-5
err.
MFLOPs
Million
Parameters
top-1
err.
top-5
err.
MFLOPs
Million
Parameters
MobileNet[
13
]
29
:
4
-
29
:
1
10
:
1
569
4
:
2
25
:
3
(3
:
8)
7
:
9
(2
:
2)
572
4
:
7
ShufeNet[
52
]
34
:
1
-
33
:
9
13
:
6
140
1
:
8
31
:
7
(2
:
2)
11
:
7
(1
:
9)
142
2
:
4
Table3:Single-croperrorrates(%)ontheImageNetvalidationset andcomplexitycomparisons.Here,MobileNetreferstoÔ¨Å1.0

MobileNet-224Ô¨Çin[
13
]andShufeNetreferstoÔ¨ÅShufeNet
1

(
g
=3)
Ô¨Çin[
52
].
Figure4:TrainingcurvesofResNet-50andSE-ResNet-50onIm-

ageNet.
curves of morenetworksareshowninsupplementarymate-

rial).Whileitshouldbenotedthatthe SEblocksthemselves

add depth,theydosoinanextremelycomputationallyef-

cientmannerandyieldgoodreturnsevenatthepointat

whichextendingthedepth ofthebasearchitectureachieves

diminishingreturns.Moreover, weseethattheperformance

improvementsareconsistentthroughtrainingacrossarange

ofdifferentdepths, suggestingthattheimprovementsin-

ducedbySEblockscanbeusedincombinationwithin-

creasingthedepthofthebasearchitecture.

Integrationwithmodernarchitectures.
Wenextinves-
tigatetheeffectofcombiningSEblocks withanothertwo

state-of-the-artarchitectures,Inception-ResNet-v2[
42
]and
ResNeXt(usingthesettingof
32

4
d)[
47
],whichboth
introducepriorstructuresinmodules.
WeconstructSENetequivalentsofthesenetworks,SE-
Inception-ResNet-v2andSE-ResNeXt(the congurationof

SE-ResNeXt-50isgiveninTable
1
).TheresultsinTa-
ble
2
illustratethesignicantperformanceimprovement
inducedbySEblockswhenintroducedintobotharchi-

tectures.Inparticular,SE-ResNeXt-50hasatop-5er-

ror of
5
:
49
%whichissuperior tobothitsdirectcounter-
partResNeXt-50(
5
:
90%
top-5error)aswellasthedeeper
ResNeXt-101(
5
:
57%
top-5 error),amodelwhichhasal-
mostdouble thenumberof parametersandcomputational

overhead.Asfortheexperimentsof Inception-ResNet-

v2,weconjecturethedifferenceofcroppingstrategymight

leadtothegapbetweentheirreportedresultandourre-

implementedone,astheir originalimagesizehasnotbeen

clariedin[
42
]whilewecropthe
299

299
regionfrom
arelativelylargerimage(wheretheshorteredgeisresized
224

224
320

320
/
299

299
top-1
err.
top-5
err.
top-1
err.
top-5
err.
ResNet-152[
10
]
23
:
0
6
:
7
21
:
3
5
:
5
ResNet-200[
11
]
21
:
7
5
:
8
20
:
1
4
:
8
Inception-v3[
44
]
-
-
21
:
2
5
:
6
Inception-v4[
42
]
-
-
20
:
0
5
:
0
Inception-ResNet-v2[
42
]
-
-
19
:
9
4
:
9
ResNeXt-101(64

4d)[
47
]
20
:
4
5
:
3
19
:
1
4
:
4
DenseNet-264[
14
]
22
:
15
6
:
12
-
-
Attention-92[
46
]
-
-
19
:
5
4
:
8
VeryDeepPolyNet[
51
]
y
-
-
18
:
71
4
:
25
PyramidNet-200[
8
]
20
:
1
5
:
4
19
:
2
4
:
7
DPN-131[
5
]
19
:
93
5
:
12
18
:
55
4
:
16
SENet-154
18.68
4.47
17.28
3.79
NASNet-A(6@4032)[
55
]
y
-
-
17
:
3
z
3
:
8
z
SENet-154(post-challenge)
-
-
16.88
z
3.58
z
Table4:Single-croperrorratesofstate-of-the-artCNNsonIm-

ageNetvalidationset.The sizeoftestcropis
224

224
and
320

320
/
299

299
asin[
11
].
y
denotesthemodelwitha
largercrop
331

331
.
z
denotesthepost-challengeresult.SENet-
154(post-challenge)istrainedwithalarger inputsize
320

320
comparedtotheoriginalone withtheinputsize
224

224
.
to
352
).SE-Inception-ResNet-v2(
4
:
79%
top-5error)out-
performsourreimplementedInception-ResNet-v2(
5
:
21%
top-5error)by
0
:
42%
(arelativeimprovementof
8
:
1%
)as
wellasthereportedresultin[
42
].
WealsoassesstheeffectofSEblockswhenoperating
on
non-residual
networksbyconductingexperimentswith
theVGG-16[
39
]andBN-Inceptionarchitecture[
16
].Asa
deepnetworkistrickytooptimise[
16
,
39
],tofacilitatethe
trainingofVGG-16 fromscratch,weaddaBatchNormal-

izationlayeraftereachconvolution.Weapplytheidentical

schemefortrainingSE-VGG-16.Theresultsofthecompar-

isonareshowninTable
2
,exhibitingthesamephenomena
thatemergedintheresidualarchitectures.
Finally,weevaluateontworepresentativeefcientar-
chitectures,MobileNet[
13
] andShufeNet[
52
]inTable
3
,showingthat SEblockscanconsistently improvetheac-
curacybyalargemarginatminimalincreasesincomputa-

tionalcost.Theseexperimentsdemonstratethatimprove-

mentsinducedbySEblockscanbeusedincombination

withawiderangeofarchitectures.Moreover,thisresult

holdsfor bothresidualandnon-residualfoundations.
7137

top-1err.
top-5err.
Places-365-CNN[
37
]
41
:
07
11
:
48
ResNet-152(ours)
41
:
15
11
:
61
SE-ResNet-152
40.37
11.01
Table5:Single-croperrorrates(%)onPlaces365validationset.
ResultsonILSVRC2017ClassicationCompetition.

SENetsformedthefoundationofoursubmissiontothe

competitionwherewewonrstplace.Ourwinningen-

trycomprisedasmallensembleof SENetsthatemployed

astandard multi-scale andmulti-cropfusionstrategytoob-

taina
2
:
251%
top-5erroron thetestset.Oneofourhigh-
performingnetworks,whichweterm
SENet-154
,wascon-
structedbyintegratingSEblockswithamodiedResNeXt

[47
](detailsareprovidedinsupplementalmaterial),the
goalofwhichistoreachthebestpossibleaccuracywith

lessemphasisonmodelcomplexity.Wecompareitwith

thetop-performingpublishedmodelsontheImageNetval-

idationsetinTable
4
.Ourmodelachievedatop-1error
of
18
:
68%
andatop-5errorof
4
:
47%
usinga
224

224
centrecropevaluation. Toenableafaircomparison,we

providea
320

320
centre cropevaluation,showingasig-
nicantperformanceimprovementonpriorwork.Afterthe

competition,wetrainanSENet-154withalargerinputsize

320
320
,achievingthelowererrorrateunderboththe
top-1(
16
:
88%
)andthetop-5(
3
:
58%
)errormetrics.
6.2.SceneClassication
We conductexperimentsonthePlaces365-Challenge
dataset[
53
]forsceneclassication.Itcomprises
8
million
trainingimagesand
36
;
500
validationimagesacross
365
categories.Relativetoclassication,the taskofsceneun-

derstandingcanprovideabetterassessmentof theabilityof

amodeltogeneralisewellandhandleabstraction,sinceit

requiresthecaptureofmorecomplexdataassociationsand

robustnesstoagreaterlevelofappearancevariation.
WeuseResNet-152asastrongbaselinetoassesstheef-
fectivenessofSEblocksandfollowthetrainingand eval-

uationprotocolsin[
37
].Table
5
showstheresultsof
ResNet-152andSE-ResNet-152.Specically,SE-ResNet-

152(
11
:
01%
top-5error)achievesalowervalidationerror
thanResNet-152(
11
:
61%
top-5error), providingevidence
thatSEblockscanperformwellondifferentdatasets.This

SENetsurpassesthepreviousstate-of-the-artmodelPlaces-

365-CNN[
37
]whichhasatop-5errorof
11
:
48%
onthis
task.

6.3.ObjectDetectiononCOCO
WefurtherevaluatethegeneralisationofSEblockson
objectdetectiontaskusing COCOdataset[
25
]whichcon-
tains
80
ktrainingimagesand
40
kvalidationimages,fol-
lowing [
10
].WeuseFasterR-CNN[
33
]asthedetec-
tionmethodandfollowthebasicimplementationin[
10
].
AP@IoU=
0
:
5
AP
ResNet-50
45.2
25.1
SE-ResNet-50
46.8
26.4
ResNet-101
48.4
27.2
SE-ResNet-101
49.2
27.9
Table6:ObjectdetectionresultsontheCOCO
40
kvalidationset
byusingthebasicFasterR-CNN.

Hereourintentionistoevaluatethebenetofreplacingthe

basearchitectureResNetwithSE-ResNet,sothattheim-

provementscanbeattributedtobetterrepresentations.Ta-

ble6
showstheresultsbyusingResNet-50,ResNet-101
andtheirSEcounterparts onthevalidationset,respectively.

SE-ResNet-50outperformsResNet-50by
1
:
3%
(arelative
5
:
2%
improvement)on COCO'sstandardmetricAPand
1
:
6%
onAP@IoU=
0
:
5
.Importantly,SEblocksarecapable
of benetingthedeeperarchitectureResNet-101by
0
:
7%
(arelative
2
:
6%
improvement)ontheAPmetric.
6.4.Analysisand Interpretation

Reductionratio.
Thereductionratio
r
introducedin
Eqn. (
5
) isanimportanthyperparameterwhichallowsusto
varythe capacity andcomputationalcostoftheSEblocks

inthemodel.Toinvestigatethisrelationship,weconduct

experimentsbasedonSE-ResNet-50 forarangeofdiffer-

entr
values.ThecomparisoninTable
7
revealsthatper-
formancedoesnotimprovemonotonicallywithincreased

capacity.ThisislikelytobearesultofenablingtheSE

blocktoovertthechannel interdependenciesofthe train-

ingset.Inparticular,wefoundthatsetting
r
=16
achieved
a goodtradeoffbetweenaccuracyandcomplexityandcon-

sequently,we usedthisvalueforallexperiments.

TheroleofExcitation.
WhileSEblockshavebeenempir-
icallyshowntoimprovenetworkperformance,wewould

alsoliketounderstandhowtheself-gating
excitation
mech-
anismoperatesinpractice.Toprovideaclearerpictureof

thebehaviourofSEblocks,inthissectionwestudyexam-

pleactivationsfromtheSE-ResNet-50modelandexamine

theirdistributionwithrespecttodifferentclassesatdifferent

blocks.Specically,wesamplefourclassesfromtheIma-

geNetdatasetthatexhibitsemanticandappearancediver-
Ratio
r
top-1err.
top-5err.
MillionParameters
4
23
:
21
6
:
63
35
:
7
8
23
:
19
6
:
64
30
:
7
16
23
:
29
6
:
62
28
:
1
32
23
:
40
6
:
77
26
:
9
original
24
:
80
7
:
48
25
:
6
Table7:Single-croperrorrates(%)onImageNetvalidationset

andparametersizesforSE-ResNet-50atdifferentreductionratios

r
.Here
original
referstoResNet-50.
7138

(a)SE
2
3
(b)SE
3
4
(c)SE
4
6
(d)SE
5
1
(e)SE
5
2
(f)SE
5
3
Figure5:Activationsinducedby
Excitation
inthedifferentmodulesofSE-ResNet-50onImageNet.Themoduleisnamedas
Ô¨Å
SE
stageID
blockID
Ô¨Ç.
sity,namely
goldsh
,
pug
,
plane
and
cliff
(exampleimages
fromtheseclassesareshowninsupplementalmaterial).We

thendrawftysamplesforeachclassfromthevalidation

setandcomputetheaverageactivationsforftyuniformly

sampledchannelsinthelastSEblockineachstage(imme-

diatelypriortodownsampling)andplottheirdistributionin

Fig.5
.Forreference,wealsoplotthedistributionofaver-
ageactivationsacrossall
1000
classes.
Wemakethefollowingthreeobservationsabouttherole
of
Excitation
.First, thedistributionacrossdifferentclasses
isnearly identicalinlowerlayers,e.g.SE
2
3.Thissug-
geststhattheimportanceoffeaturechannelsislikelyto

beshared bydifferentclassesintheearlystagesofthe

network.Interestinglyhowever,thesecondobservationis

thatatgreater depth,thevalueofeach channelbecomes

muchmoreclass-specicasdifferentclassesexhibitdiffer-

entpreferencestothediscriminativevalueoffeatures,e.g.

SE4
6andSE
5
1.Thetwoobservationsareconsistent
withndingsinpreviouswork[
23
,
50
],namelythatlower
layerfeaturesaretypicallymoregeneral(i.e.classagnostic

inthe contextofclassication)whilehigherlayerfeatures

havegreaterspecicity.Asaresult,representationlearning

benetsfromtherecalibrationinducedbySE blockswhich

adaptivelyfacilitatesfeatureextractionandspecialisationto

theextentthatitisneeded.Finally,weobserveasome-

whatdifferentphenomenainthe laststageofthe network.

SE5
2exhibitsaninterestingtendencytowardsasaturated
stateinwhich mostoftheactivationsarecloseto
1
andthe
remainderiscloseto
0
.Atthepointatwhichallactivations
takethevalue
1
,thisblockwouldbecomeastandardresid-
ualblock.AttheendofthenetworkintheSE
5
3(whichis
immediatelyfollowedbyglobal poolingpriorbeforeclas-

siers),asimilarpatternemergesover differentclasses,up

toaslightchangeinscale(whichcouldbetunedbythe
classiers).ThissuggeststhatSE
5
2andSE
5
3areless
importantthanpreviousblocksinprovidingrecalibrationto

thenetwork.Thisndingisconsistentwiththeresult ofthe

empiricalinvestigationinSec.
4
whichdemonstratedthat
theoverallparametercountcouldbesignicantlyreduced

byremovingtheSE blocksforthelaststagewithonlya

marginallossofperformance.

7.Conclusion
In thispaperweproposedtheSEblock,anovelarchitec-
turalunitdesignedtoimprovetherepresentationalcapacity

ofanetworkbyenablingittoperformdynamicchannel-

wisefeaturerecalibration.Extensiveexperimentsdemon-

stratetheeffectivenessofSENetswhichachievestate-of-

the-artperformanceonmultipledatasets.Inaddition,they

provide someinsightintothelimitationsofpreviousarchi-

tectures inmodelling channel-wisefeaturedependencies,

whichwehopemayproveusefulforothertasksrequiring

strongdiscriminativefeatures.Finally,thefeatureimpor-

tanceinducedbySEblocksmaybehelpfultorelatedelds

suchasnetworkpruningforcompression.

Acknowledgements.
WewouldliketothankProfessorAndrew
ZissermanforhishelpfulcommentsandSamuelAlbanieforhis

discussionsandwritingeditforthepaper.Wewouldliketo

thankChaoLiforhiscontributionsinthetrainingsystem. Li

ShenissupportedbytheOfceoftheDirectorofNationalIntelli-

gence(ODNI),IntelligenceAdvancedResearchProjectsActivity

(IARPA),viacontractnumber2014-14071600010.Theviewsand

conclusionscontainedhereinarethoseoftheauthorandshould

notbeinterpretedasnecessarilyrepresentingtheofcial policies

orendorsements,eitherexpressedorimplied,ofODNI,IARPA,or

theU.S.Government.TheU.S.Governmentisauthorizedtore-

produceanddistributereprintsforGovernmentalpurposenotwith-

standinganycopyrightannotationthereon.
7139

References
[1]S.Bell,C.L.Zitnick,K.Bala,andR.Girshick.Inside-
outsidenet: Detectingobjectsincontextwithskip pooling

andrecurrentneuralnetworks.In
CVPR
, 2016.
1
[2]T.Bluche.Jointlinesegmentationandtranscriptionforend-
to-endhandwrittenparagraphrecognition.In
NIPS
,2016.
2
[3]C.Cao,X.Liu,Y.Yang,Y.Yu,J.Wang,Z.Wang,Y. Huang,
L.Wang,C. Huang,W.Xu,D.Ramanan,andT.S.Huang.

Lookandthinktwice:Capturingtop-down visualatten-

tionwithfeedbackconvolutionalneuralnetworks.In
ICCV
,
2015.
2
[4]L.Chen,H.Zhang,J.Xiao,L.Nie,J.Shao,W.Liu,and
T.Chua.SCA-CNN:Spatialandchannel-wiseattention

inconvolutionalnetworksforimagecaptioning. In
CVPR
,
2017.
2
[5]Y.Chen,J.Li,H.Xiao,X.Jin,S.Yan,andJ.Feng. Dual
pathnetworks.In
NIPS
, 2017.
2
,
6
[6]F.Chollet.Xception:Deeplearningwithdepthwisesepara-
bleconvolutions.In
CVPR
, 2017.
2
[7]J.S.Chung,A.Senior,O.Vinyals,andA.Zisserman.Lip
readingsentencesinthewild.In
CVPR
, 2017.
2
[8]D.Han,J.Kim,andJ.Kim.Deeppyramidalresidualnet-
works.In
CVPR
, 2017.
6
[9]K.He,X.Zhang,S.Ren,andJ.Sun.Delving deepintorec-
tiers:Surpassinghuman-levelperformanceonImageNet

classication.In
ICCV
, 2015.
5
[10]K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearning
forimagerecognition.In
CVPR
, 2016.
2
,
5
,
6
,
7
[11]K.He,X.Zhang,S.Ren,andJ.Sun.Identitymappingsin
deepresidualnetworks.In
ECCV
, 2016.
2
,
6
[12]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Neuralcomputation
, 1997.
2
[13]A.G.Howard,M.Zhu,B.Chen,D.Kalenichenko,W.Wang,
T.Weyand,M.Andreetto, andH.Adam.Mobilenets:Ef-

cientconvolutionalneuralnetworksformobilevisionappli-

cations.arXiv:1704.04861
, 2017.
3
,
6
[14]G.Huang,Z.Liu,K.Q.Weinberger,andL.Maaten.Densely
connectedconvolutionalnetworks.In
CVPR
, 2017.
2
,
6
[15]Y.Ioannou,D.Robertson,R.Cipolla,andA.Criminisi.
Deeproots:ImprovingCNNefciencywithhierarchicall-

tergroups.In
CVPR
, 2017.
2
[16]S.IoffeandC. Szegedy.Batchnormalization:Accelerating
deepnetworktrainingbyreducinginternalcovariateshift.In

ICML, 2015.
1
,
2
,
5
,
6
[17]L.IttiandC.Koch. Computational modellingofvisualat-
tention.
Naturereviewsneuroscience
, 2001.
2
[18]L.Itti,C.Koch,andE.Niebur.Amodelofsaliency-based
visualattention forrapidsceneanalysis.
IEEETPAMI
,1998.
2
[19]M.Jaderberg, K.Simonyan,A.Zisserman,and
K.Kavukcuoglu.Spatialtransformernetworks.In

NIPS, 2015.
1
,
2
[20]M.Jaderberg, A.Vedaldi,andA.Zisserman.Speedingup
convolutionalneuralnetworkswithlowrankexpansions.In

BMVC, 2014.
2
[21]A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNet
classicationwithdeepconvolutionalneuralnetworks.In

NIPS, 2012.
1
,
3
[22]H.LarochelleandG.E.Hinton.Learningtocombinefoveal
glimpseswithathird-orderboltzmannmachine.In
NIPS
,
2010.
2
[23]H.Lee,R.Grosse,R.Ranganath,andA.Y.Ng.Convolu-
tionaldeepbeliefnetworksforscalableunsupervisedlearn-

ingofhierarchicalrepresentations.In
ICML
, 2009.
8
[24]M.Lin,Q.Chen,and S.Yan.Networkinnetwork.
arXiv:1312.4400
, 2013.
2
[25]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Dollar,andC.L.Zitnick.Microsoftcoco:Com-

monobjectsincontext.
ECCV
, 2014.
7
[26]H.Liu, K.Simonyan,O.Vinyals,C.Fernando,and
K.Kavukcuoglu. Hierarchicalrepresentations for efcient

architecturesearch.
arXiv:1711.00436
, 2017.
2
[27]J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutional
networksforsemanticsegmentation.In
CVPR
, 2015.
1
[28]A.Miech,I.Laptev,andJ.Sivic.Learnablepoolingwith
contextgatingforvideoclassication.
arXiv:1706.06905
,
2017.
2
[29]V.Mnih,N.Heess,A.Graves,andK.Kavukcuoglu.Recur-
rentmodelsofvisualattention.In
NIPS
, 2014.
2
[30]V.NairandG.E.Hinton.Rectiedlinearunitsimprovere-
stricted boltzmannmachines.In
ICML
, 2010.
3
[31]A.Newell,K.Yang,andJ.Deng.Stackedhourglassnet-
worksforhumanposeestimation.In
ECCV
, 2016.
1
,
2
[32]B.A.Olshausen,C.H.Anderson,andD.C.V.Essen.A
neurobiologicalmodelofvisualattentionandinvariantpat-

ternrecognitionbasedondynamicroutingofinformation.

JournalofNeuroscience
, 1993.
2
[33]S.Ren,K. He,R.Girshick,andJ.Sun.FasterR-CNN:To-
wardsreal-time objectdetectionwithregionproposalnet-

works.In
NIPS
, 2015.
1
,
7
[34]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,

A.C.Berg,andL.Fei-Fei.ImageNetlargescalevisual

recognitionchallenge.
IJCV
, 2015.
2
[35]J.Sanchez,F.Perronnin,T.Mensink, andJ.Verbeek.Im-
ageclassicationwiththeshervector:Theoryandpractice.

RR-8209,INRIA
, 2013.
3
[36]L.Shen,Z.Lin,andQ.Huang.Relaybackpropagationfor
effectivelearningofdeepconvolutionalneuralnetworks.In

ECCV, 2016.
4
[37]L.Shen,Z.Lin,G.Sun,andJ.Hu.Places401andplaces365
models.
https://github.com/lishen-shirley/
Places2-CNNs
, 2016.
7
[38]L.Shen,G.Sun,Q.Huang,S.Wang,Z.Lin,andE.Wu.
Multi-leveldiscriminativedictionarylearning withapplica-

tiontolargescaleimage classication.
IEEETIP
, 2015.
3
[39]K.SimonyanandA.Zisserman.Verydeepconvolutional
networksfor large-scaleimagerecognition.In
ICLR
,2015.
2
,
3
,
5
,
6
[40]R.K.Srivastava,K.Greff,andJ. Schmidhuber.Training
verydeepnetworks.In
NIPS
, 2015.
2
[41]M.F.Stollenga,J.Masci,F.Gomez,andJ.Schmidhuber.
Deepnetworkswithinternalselectiveattentionthroughfeed-

backconnections.In
NIPS
, 2014.
2
[42]C.Szegedy,S.Ioffe,V.Vanhoucke,andA.Alemi. Inception-
v4,inception-resnetandtheimpactofresidualconnections

onlearning.In
ICLRWorkshop
, 2016.
2
,
3
,
4
,
5
,
6
[43]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,
D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.

Goingdeeperwithconvolutions.In
CVPR
, 2015.
1
,
2
,
4
[44]C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.
Rethinkingtheinception architecture forcomputervision.In
7140

CVPR
, 2016.
2
,
6
[45]A.ToshevandC.Szegedy.DeepPose:Humanposeestima-
tionviadeepneuralnetworks.In
CVPR
, 2014.
1
[46]F.Wang,M.Jiang,C.Qian,S.Yang,C.Li,H.Zhang,
X.Wang,andX.Tang.Residualattentionnetworkforimage

classication.In
CVPR
, 2017.
2
,
6
[47]S.Xie,R.Girshick,P.Dollar,Z.Tu,andK.He.Aggregated
residualtransformationsfordeepneuralnetworks.In
CVPR
,
2017.
2
,
3
,
5
,
6
,
7
[48]K.Xu,J.Ba, R.Kiros,K.Cho,A.Courville,R.Salakhudi-
nov,R.Zemel,andY.Bengio.Show,attendandtell:Neural

imagecaptiongenerationwithvisualattention.In
ICML
,
2015.
2
[49]J.Yang,K.Yu,Y.Gong,andT.Huang.Linearspatialpyra-
midmatchingusingsparsecodingforimageclassication.

InCVPR
, 2009.
3
[50]J.Yosinski,J. Clune,Y. Bengio,andH.Lipson.Howtrans-
ferablearefeaturesindeepneuralnetworks?In
NIPS
,2014.
8
[51]X.Zhang,Z.Li,C.C.Loy,andD.Lin.Polynet: Apursuitof
structuraldiversityinverydeepnetworks.In
CVPR
, 2017.
6
[52]X.Zhang,X.Zhou,M.Lin,andJ.Sun.Shufenet:An
extremelyefcientconvolutionalneuralnetworkformobile

devices.
arXiv:1707.01083
, 2017.
3
,
6
[53]B.Zhou,A.Lapedriza,A.Khosla, A.Oliva, andA.Torralba.
Places:A10millionimagedatabaseforscenerecognition.

IEEETPAMI
, 2017.
7
[54]B.ZophandQ.V. Le.Neuralarchitecturesearch withrein-
forcementlearning.In
ICLR
, 2017.
2
[55]B.Zoph,V.Vasudevan,J.Shlens,andQ.V.Le.Learn-
ingtransferablearchitecturesforscalableimagerecognition.

arXiv:1707.07012
, 2017.
2
,
6
7141

