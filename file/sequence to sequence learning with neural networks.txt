SequencetoSequenceLearning
withNeuralNetworks
IlyaSutskever
Google
ilyasu@google.com
OriolVinyals
Google
vinyals@google.com
QuocV.Le
Google
qvl@google.com
Abstract
DeepNeuralNetworks(DNNs)arepowerfulmodelsthathaveac
hievedexcel-
lentperformanceondifcultlearningtasks.AlthoughDNNs
workwellwhenever
largelabeledtrainingsetsareavailable,theycannotbeus
edtomapsequencesto
sequences.Inthispaper,wepresentageneralend-to-endap
proachtosequence
learningthatmakesminimalassumptionsonthesequencestr
ucture.Ourmethod
usesamultilayeredLongShort-TermMemory(LSTM)tomapthe
inputsequence
toavectorofaxeddimensionality,andthenanotherdeepLS
TMtodecodethe
targetsequencefromthevector.Ourmainresultisthatonan
EnglishtoFrench
translationtaskfromtheWMT-14dataset,thetranslationsp
roducedbytheLSTM
achieveaBLEUscoreof34.8ontheentiretestset,wheretheL
STM'sBLEU
scorewaspenalizedonout-of-vocabularywords.Additiona
lly,theLSTMdidnot
havedifcultyonlongsentences.Forcomparison,aphrase-
basedSMTsystem
achievesaBLEUscoreof33.3onthesamedataset.Whenweusedt
heLSTM
torerankthe1000hypothesesproducedbytheaforementione
dSMTsystem,its
BLEUscoreincreasesto36.5,whichisclosetothepreviouss
tateoftheart.The
LSTMalsolearnedsensiblephraseandsentencerepresentat
ionsthataresensitive
towordorderandarerelativelyinvarianttotheactiveandt
hepassivevoice.Fi-
nally,wefoundthatreversingtheorderofthewordsinallso
urcesentences(but
nottargetsentences)improvedtheLSTM'sperformancemark
edly,becausedoing
sointroducedmanyshorttermdependenciesbetweenthesour
ceandthetarget
sentencewhichmadetheoptimizationproblemeasier.
1Introduction

DeepNeuralNetworks(DNNs)areextremelypowerfulmachine
learningmodelsthatachieveex-
cellentperformanceondifcultproblemssuchasspeechrec
ognition[13,7]andvisualobjectrecog-
nition[19,6,21,20].DNNsarepowerfulbecausetheycanper
formarbitraryparallelcomputation
foramodestnumberofsteps.Asurprisingexampleofthepowe
rofDNNsistheirabilitytosort
NN
-bitnumbersusingonly2hiddenlayersofquadraticsize[27
].So,whileneuralnetworksare
relatedtoconventionalstatisticalmodels,theylearnani
ntricatecomputation.Furthermore,large
DNNscanbetrainedwithsupervisedbackpropagationwhenev
erthelabeledtrainingsethasenough
informationtospecifythenetwork'sparameters.Thus,ift
hereexistsaparametersettingofalarge
DNNthatachievesgoodresults(forexample,becausehumans
cansolvethetaskveryrapidly),
supervisedbackpropagationwillndtheseparametersands
olvetheproblem.
Despitetheirexibilityandpower,DNNscanonlybeapplied
toproblemswhoseinputsandtargets
canbesensiblyencodedwithvectorsofxeddimensionality
.Itisasignicantlimitation,since
manyimportantproblemsarebestexpressedwithsequencesw
hoselengthsarenotknowna-priori.
Forexample,speechrecognitionandmachinetranslationar
esequentialproblems.Likewise,ques-
tionansweringcanalsobeseenasmappingasequenceofwords
representingthequestiontoa
1
sequenceofwordsrepresentingtheanswer.Itisthereforec
learthatadomain-independentmethod
thatlearnstomapsequencestosequenceswouldbeuseful.

SequencesposeachallengeforDNNsbecausetheyrequiretha
tthedimensionalityoftheinputsand
outputsisknownandxed.Inthispaper,weshowthatastraig
htforwardapplicationoftheLong
Short-TermMemory(LSTM)architecture[16]cansolvegener
alsequencetosequenceproblems.
TheideaistouseoneLSTMtoreadtheinputsequence,onetime
stepatatime,toobtainlargexed-
dimensionalvectorrepresentation,andthentouseanother
LSTMtoextracttheoutputsequence
fromthatvector(g.1).ThesecondLSTMisessentiallyarec
urrentneuralnetworklanguagemodel
[28,23,30]exceptthatitisconditionedontheinputsequen
ce.TheLSTM'sabilitytosuccessfully
learnondatawithlongrangetemporaldependenciesmakesit
anaturalchoiceforthisapplication
duetotheconsiderabletimelagbetweentheinputsandtheir
correspondingoutputs(g.1).
Therehavebeenanumberofrelatedattemptstoaddressthege
neralsequencetosequencelearning
problemwithneuralnetworks.Ourapproachiscloselyrelat
edtoKalchbrennerandBlunsom[18]
whowerethersttomaptheentireinputsentencetovector,a
ndisverysimilartoChoetal.[5].
Graves[10]introducedanoveldifferentiableattentionme
chanismthatallowsneuralnetworksto
focusondifferentpartsoftheirinput,andanelegantvaria
ntofthisideawassuccessfullyapplied
tomachinetranslationbyBahdanauetal.[2].TheConnectio
nistSequenceClassicationisanother
populartechniqueformappingsequencestosequenceswithn
euralnetworks,althoughitassumesa
monotonicalignmentbetweentheinputsandtheoutputs[11]
.
Figure1:
OurmodelreadsaninputsentenceﬁABCﬂandproducesﬁWXYZﬂasth
eoutputsentence.The
modelstopsmakingpredictionsafteroutputtingtheend-of-sentencetoke
n.NotethattheLSTMreadsthe
inputsentenceinreverse,becausedoingsointroducesmanyshortte
rmdependenciesinthedatathatmakethe
optimizationproblemmucheasier.

Themainresultofthisworkisthefollowing.OntheWMT'14Eng
lishtoFrenchtranslationtask,
weobtainedaBLEUscoreof
34.81
bydirectlyextractingtranslationsfromanensembleof5de
ep
LSTMs(with380Mparameterseach)usingasimpleleft-to-ri
ghtbeam-searchdecoder.Thisis
byfarthebestresultachievedbydirecttranslationwithla
rgeneuralnetworks.Forcomparison,
theBLEUscoreofaSMTbaselineonthisdatasetis33.30[29].
The34.81BLEUscorewas
achievedbyanLSTMwithavocabularyof 80kwords,sothescor
ewaspenalizedwheneverthe
referencetranslationcontainedawordnotcoveredbythese
80k.Thisresultshowsthatarelatively
unoptimizedneuralnetworkarchitecturewhichhasmuchroo
mforimprovementoutperformsa
maturephrase-basedSMTsystem.

Finally,weusedtheLSTMtorescorethepubliclyavailable1
000-bestlistsoftheSMTbaselineon
thesametask[29].Bydoingso,weobtainedaBLEUscoreof36.
5,whichimprovesthebaseline
by3.2BLEUpointsandisclosetothepreviousstate-of-the-
art(whichis37.0[9]).
Surprisingly,theLSTMdidnotsufferonverylongsentences
,despitetherecentexperienceofother
researcherswithrelatedarchitectures[26].Wewereablet
odowellonlongsentencesbecausewe
reversedtheorderofwordsinthesourcesentencebutnotthe
targetsentencesinthetrainingandtest
set.Bydoingso,weintroducedmanyshorttermdependencies
thatmadetheoptimizationproblem
muchsimpler(seesec.2and3.3).Asaresult,SGDcouldlearn
LSTMsthathadnotroublewith
longsentences.Thesimpletrickofreversingthewordsinth
esourcesentenceisoneofthekey
technicalcontributionsofthiswork.

AusefulpropertyoftheLSTMisthatitlearnstomapaninputs
entenceofvariablelengthinto
axed-dimensionalvectorrepresentation.Giventhattran
slationstendtobeparaphrasesofthe
sourcesentences,thetranslationobjectiveencouragesth
eLSTMtondsentencerepresentations
thatcapturetheirmeaning,assentenceswithsimilarmeani
ngsareclosetoeachotherwhiledifferent
2
sentencesmeaningswillbefar.Aqualitativeevaluationsu
pports thisclaim,showingthatourmodel
isawareofwordorderandisfairlyinvarianttotheactivean
dpassivevoice.
2Themodel

TheRecurrentNeuralNetwork(RNN)[31,28]isanaturalgene
ralizationoffeedforwardneural
networkstosequences.Givenasequenceofinputs
(
x
1
;:::;x
T
)
,astandardRNNcomputesa
sequenceofoutputs
(
y
1
;:::;y
T
)
byiteratingthefollowingequation:
h
t
=sigm
W
hx
x
t
+
W
hh
h
t

1

y
t
=
W
yh
h
t
TheRNNcaneasilymapsequencestosequenceswhenevertheal
ignmentbetweentheinputsthe
outputsisknownaheadoftime.However,itisnotclearhowto
applyanRNNtoproblemswhose
inputandtheoutputsequenceshavedifferentlengthswithc
omplicatedandnon-monotonicrelation-
ships.

Asimplestrategyforgeneralsequencelearningistomapthe
inputsequencetoaxed-sizedvector
usingoneRNN,andthentomapthevectortothetargetsequenc
ewithanotherRNN(thisapproach
hasalsobeentakenbyChoetal.[5]).Whileitcouldworkinpri
nciplesincetheRNNisprovided
withalltherelevantinformation,itwouldbedifculttotr
aintheRNNsduetotheresultinglong
termdependencies[14,4](gure1)[16,15].However,theLo
ngShort-TermMemory(LSTM)[16]
isknowntolearnproblemswithlongrangetemporaldependen
cies,soanLSTMmaysucceedin
thissetting.

ThegoaloftheLSTMistoestimatetheconditionalprobabili
ty
p
(
y
1
;:::;y
T
0
j
x
1
;:::;x
T
)
where
(
x
1
;:::;x
T
)
isaninputsequenceand
y
1
;:::;y
T
0
isitscorrespondingoutputsequencewhoselength
T
0
maydifferfrom
T
.TheLSTMcomputesthisconditionalprobabilitybyrstobt
ainingthexed-
dimensionalrepresentation
v
oftheinputsequence
(
x
1
;:::;x
T
)
givenbythelasthiddenstateofthe
LSTM,andthencomputingtheprobabilityof
y
1
;:::;y
T
0
withastandardLSTM-LMformulation
whoseinitialhiddenstateissettotherepresentation
v
of
x
1
;:::;x
T
:
p
(
y
1
;:::;y
T
0
j
x
1
;:::;x
T
)=
T
0
Y
t
=1
p
(
y
t
j
v;y
1
;:::;y
t

1
)
(1)
Inthisequation,each
p
(
y
t
j
v;y
1
;:::;y
t

1
)
distributionisrepresentedwithasoftmaxoverallthe
wordsinthevocabulary.WeusetheLSTMformulationfromGra
ves[10].Notethatwerequirethat
eachsentenceendswithaspecialend-of-sentencesymbolﬁ
<
EOS
>
ﬂ,whichenablesthemodelto
deneadistributionoversequencesofallpossiblelengths
.Theoverallschemeisoutlinedingure
1,wheretheshownLSTMcomputestherepresentationofﬁAﬂ,ﬁ
Bﬂ,ﬁCﬂ,ﬁ
<
EOS
>
ﬂandthenuses
thisrepresentationtocomputetheprobabilityofﬁWﬂ,ﬁXﬂ,ﬁ
Yﬂ,ﬁZﬂ,ﬁ
<
EOS
>
ﬂ.
Ouractualmodelsdifferfromtheabovedescriptioninthree
importantways.First,weusedtwo
differentLSTMs:onefortheinputsequenceandanotherfort
heoutputsequence,becausedoing
soincreasesthenumbermodelparametersatnegligiblecomp
utationalcostandmakesitnaturalto
traintheLSTMonmultiplelanguagepairssimultaneously[1
8].Second,wefoundthatdeepLSTMs
signicantlyoutperformedshallowLSTMs,sowechoseanLST
Mwithfourlayers.Third,wefound
itextremelyvaluabletoreversetheorderofthewordsofthe
inputsentence.Soforexample,instead
ofmappingthesentence
a;b;c
tothesentence
;;
,theLSTMisaskedtomap
c;b;a
to
;;
,
where
;;
isthetranslationof
a;b;c
.Thisway,
a
isincloseproximityto

,
b
isfairlycloseto

,andsoon,afactthatmakesiteasyforSGDtoﬁestablishcomm
unicationﬂbetweentheinputand
theoutput.Wefoundthissimpledatatransformationtogrea
tlyboosttheperformanceoftheLSTM.
3Experiments

WeappliedourmethodtotheWMT'14EnglishtoFrenchMTtaskin
twoways.Weuseditto
directlytranslatetheinputsentencewithoutusingarefer
enceSMTsystemandweittorescorethe
n-bestlistsofanSMTbaseline.Wereporttheaccuracyofthe
setranslationmethods,presentsample
translations,andvisualizetheresultingsentencerepres
entation.
3
3.1Datasetdetails

WeusedtheWMT'14EnglishtoFrenchdataset.Wetrainedourmo
delsonasubsetof12Msen-
tencesconsistingof348MFrenchwordsand304MEnglishword
s,whichisacleanﬁselectedﬂ
subsetfrom[29].Wechosethistranslationtaskandthisspe
cictrainingsetsubsetbecauseofthe
publicavailabilityofatokenizedtrainingandtestsettog
etherwith1000-bestlistsfromthebaseline
SMT[29].

Astypicalneurallanguagemodelsrelyonavectorrepresent
ationforeachword,weusedaxed
vocabularyforbothlanguages.Weused160,000ofthemostfr
equentwordsforthesourcelanguage
and80,000ofthemostfrequentwordsforthetargetlanguage
.Everyout-of-vocabularywordwas
replacedwithaspecialﬁUNKﬂtoken.

3.2DecodingandRescoring

ThecoreofourexperimentsinvolvedtrainingalargedeepLS
TMonmanysentencepairs.We
traineditbymaximizingthelogprobabilityofacorrecttra
nslation
T
giventhesourcesentence
S
,
sothetrainingobjectiveis
1
=
jSj
X
(
T;S
)
2S
log
p
(
T
j
S
)
where
S
isthetrainingset.Oncetrainingiscomplete,weproducetr
anslationsbyndingthemost
likelytranslationaccordingtotheLSTM:
^
T
=argmax
T
p
(
T
j
S
)
(2)
Wesearchforthemostlikelytranslationusingasimpleleft
-to-rightbeamsearchdecoderwhich
maintainsasmallnumber
B
ofpartialhypotheses,whereapartialhypothesisisaprex
ofsome
translation.Ateachtimestepweextendeachpartialhypoth
esisinthebeamwitheverypossible
wordinthevocabulary.Thisgreatlyincreasesthenumberof
thehypothesessowediscardallbut
the
B
mostlikelyhypothesesaccordingtothemodel'slogprobabi
lity.Assoonastheﬁ
<
EOS
>
ﬂ
symbolisappendedtoahypothesis,itisremovedfromthebea
mandisaddedtothesetofcomplete
hypotheses.Whilethisdecoderisapproximate,itissimplet
oimplement.Interestingly,oursystem
performswellevenwithabeamsizeof1,andabeamofsize2pro
videsmostofthebenetsofbeam
search(Table1).

WealsousedtheLSTMtorescorethe1000-bestlistsproduced
bythebaselinesystem[29].To
rescoreann-bestlist,wecomputedthelogprobabilityofev
eryhypothesiswithourLSTMandtook
anevenaveragewiththeirscoreandtheLSTM'sscore.

3.3ReversingtheSourceSentences

WhiletheLSTMiscapableofsolvingproblemswithlongtermde
pendencies,wediscoveredthat
theLSTMlearnsmuchbetterwhenthesourcesentencesarerev
ersed(thetargetsentencesarenot
reversed).Bydoingso,theLSTM'stestperplexitydroppedf
rom5.8to4.7,andthetestBLEU
scoresofitsdecodedtranslationsincreasedfrom25.9to30
.6.
Whilewedonothaveacompleteexplanationtothisphenomenon
,webelievethatitiscausedby
theintroductionofmanyshorttermdependenciestothedata
set.Normally,whenweconcatenatea
sourcesentencewithatargetsentence,eachwordinthesour
cesentenceisfarfromitscorresponding
wordinthetargetsentence.Asaresult,theproblemhasalar
geﬁminimaltimelagﬂ[17].By
reversingthewordsinthesourcesentence,theaveragedist
ancebetweencorrespondingwordsin
thesourceandtargetlanguageisunchanged.However,ther
stfewwordsinthesourcelanguage
arenowveryclosetotherstfewwordsinthetargetlanguage
,sotheproblem'sminimaltimelagis
greatlyreduced.Thus,backpropagationhasaneasiertimeﬁ
establishingcommunicationﬂbetween
thesourcesentenceandthetargetsentence,whichinturnre
sultsinsubstantiallyimprovedoverall
performance.

Initially,webelievedthatreversingtheinputsentencesw
ouldonlyleadtomorecondentpredic-
tionsintheearlypartsofthetargetsentenceandtolesscon
dentpredictionsinthelaterparts.How-
ever,LSTMstrainedonreversedsourcesentencesdidmuchbe
tteronlongsentencesthanLSTMs
4
trainedontherawsourcesentences(seesec.3.7),whichsug
geststhatreversingtheinputsentences
resultsinLSTMswithbettermemoryutilization.

3.4Trainingdetails

WefoundthattheLSTMmodelsarefairlyeasytotrain.Weused
deepLSTMswith4layers,
with1000cellsateachlayerand1000dimensionalwordembed
dings,withaninputvocabulary
of160,000andanoutputvocabularyof80,000.WefounddeepL
STMstosignicantlyoutperform
shallowLSTMs,whereeachadditionallayerreducedperplex
itybynearly10%,possiblyduetotheir
muchlargerhiddenstate.Weusedanaivesoftmaxover80,000
wordsateachoutput.Theresulting
LSTMhas380Mparametersofwhich64Marepurerecurrentconn
ections(32Mfortheﬁencoderﬂ
LSTMand32MfortheﬁdecoderﬂLSTM).Thecompletetrainingd
etailsaregivenbelow:

WeinitializedalloftheLSTM'sparameterswiththeuniform
distributionbetween-0.08
and0.08

Weusedstochasticgradientdescentwithoutmomentum,with
axedlearningrateof0.7.
After5epochs,webegunhalvingthelearningrateeveryhalf
epoch.Wetrainedourmodels
foratotalof7.5epochs.

Weusedbatchesof128sequencesforthegradientanddivided
itthesizeofthebatch
(namely,128).

AlthoughLSTMstendtonotsufferfromthevanishinggradien
tproblem,theycanhave
explodinggradients.Thusweenforcedahardconstraintont
henormofthegradient[10,
25]byscalingitwhenitsnormexceededathreshold.Foreach
trainingbatch,wecompute
s
=
k
g
k
2
,where
g
isthegradientdividedby128.If
s>
5
,weset
g
=
5
g
s
.

Differentsentenceshavedifferentlengths.Mostsentence
sareshort(e.g.,length20-30)
butsomesentencesarelong(e.g.,length
>
100),soaminibatchof128randomlychosen
trainingsentenceswillhavemanyshortsentencesandfewlo
ngsentences,andasaresult,
muchofthecomputationintheminibatchiswasted.Toaddres
sthisproblem,wemade
surethatallsentenceswithinaminibatchwereroughlyofth
esamelength,whicha2x
speedup.
3.5Parallelization

AC++implementationofdeepLSTMwiththecongurationfrom
theprevioussectiononasin-
gleGPUprocessesaspeedofapproximately1,700wordsperse
cond.Thiswastooslowforour
purposes,soweparallelizedourmodelusingan8-GPUmachin
e.EachlayeroftheLSTMwas
executedonadifferentGPUandcommunicateditsactivation
stothenextGPU(orlayer)assoon
astheywerecomputed.Ourmodelshave4layersofLSTMs,each
ofwhichresidesonaseparate
GPU.Theremaining4GPUswereusedtoparallelizethesoftma
x,soeachGPUwasresponsible
formultiplyingbya
1000

20000
matrix.Theresultingimplementation achievedaspeedof6,
300
(bothEnglishandFrench)wordspersecondwithaminibatchs
izeof128.Trainingtookaboutaten
dayswiththisimplementation.

3.6ExperimentalResults

WeusedthecasedBLEUscore[24]toevaluatethequalityofou
rtranslations.Wecomputedour
BLEUscoresusing
multi-bleu.pl
1
onthe
tokenized
predictionsandgroundtruth.Thisway
ofevaluatingtheBELUscoreisconsistentwith[5]and[2],a
ndreproducesthe33.3scoreof[29].
However,ifweevaluatethestateoftheartsystemof[9](who
sepredictionscanbedownloaded
from
statmt.org\matrix
)inthismanner,weget37.0,whichisgreaterthanthe35.8re
ported
by
statmt.org\matrix
.
Theresultsarepresentedintables1and2.Ourbestresultsa
reobtainedwithanensembleof
LSTMsthatdifferintheirrandominitializationsandinthe
randomorderofminibatches.Whilethe
decodedtranslationsoftheLSTMensembledonotbeatthesta
teoftheart,itisthersttimethat
apureneuraltranslationsystemoutperformsaphrase-base
dSMTbaselineonalargeMTtaskby
1
ThereseveralvariantsoftheBLEUscore,andeachvariantisdene
dwithaperlscript.
5
Method
testBLEUscore(ntst14)
Bahdanauetal.[2]
28.45
BaselineSystem[29]
33.30
SingleforwardLSTM,beamsize12
26.17
SinglereversedLSTM,beamsize12
30.59
Ensembleof5reversedLSTMs,beamsize1
33.00
Ensembleof2reversedLSTMs,beamsize12
33.27
Ensembleof5reversedLSTMs,beamsize2
34.50
Ensembleof5reversedLSTMs,beamsize12
34.81
Table1:TheperformanceoftheLSTMonWMT'14EnglishtoFrenc
htestset(ntst14).Notethat
anensembleof5LSTMswithabeamofsize2ischeaperthanofas
ingleLSTMwithabeamof
size12.
Method
testBLEUscore(ntst14)
BaselineSystem[29]
33.30
Choetal.[5]
34.54
Stateoftheart[9]
37.0
Rescoringthebaseline1000-bestwithasingleforwardLSTM
35.61
Rescoringthebaseline1000-bestwithasinglereversedLSTM
35.85
Rescoringthebaseline1000-bestwithanensembleof5reversedLSTM
s
36.5
OracleRescoringoftheBaseline1000-bestlists
˘
45
Table2:MethodsthatuseneuralnetworkstogetherwithanSM
TsystemontheWMT'14English
toFrenchtestset(ntst14).

asizeablemargin,despiteitsinabilitytohandleout-of-v
ocabularywords.TheLSTMiswithin0.5
BLEUpointsofthepreviousstateoftheartbyrescoringthe1
000-bestlistofthebaselinesystem.
3.7Performanceonlongsentences

WeweresurprisedtodiscoverthattheLSTMdidwellonlongse
ntences,whichisshownquantita-
tivelyingure3.Table3presentsseveralexamplesoflongs
entencesandtheirtranslations.
3.8ModelAnalysis
-8
-6
-4
-2
0
2
4
6
8
10
-6
-5
-4
-3
-2
-1
0
1
2
3
4
John respects Mary
Mary respects John
John admires Mary
Mary admires John
Mary is in love with John
John is in love with Mary
-15
-10
-5
0
5
10
15
20
-20
-15
-10
-5
0
5
10
15
I gave her a card in the garden
In the garden , I gave her a card
She was given a card by me in the garden
She gave me a card in the garden
In the garden , she gave me a card
I was given a card by her in the garden
Figure2:
Thegureshowsa2-dimensionalPCAprojectionoftheLSTMhiddenstate
sthatareobtained
afterprocessingthephrasesinthegures.Thephrasesareclustere
dbymeaning,whichintheseexamplesis
primarilyafunctionofwordorder,whichwouldbedifculttocapturewitha
bag-of-wordsmodel.Noticethat
bothclustershavesimilarinternalstructure.

Oneoftheattractivefeaturesofourmodelisitsabilitytot
urnasequenceofwordsintoavector
ofxeddimensionality.Figure2visualizessomeofthelear
nedrepresentations.Thegureclearly
showsthattherepresentationsaresensitivetotheorderof
words,whilebeingfairlyinsensitivetothe
6
Type
Sentence
Ourmodel
UlrichUNK,membreduconseild'administrationduconstructeurautomob
ileAudi,
afrmequ'ils'agitd'unepratiquecourantedepuisdesann
´
eespourquelest
´
el
´
ephones
portablespuissent
‹
etrecollect
´
esavantlesr
´
eunionsduconseild'administrationanqu'ils
nesoientpasutilis
´
escommeappareilsd'
´
ecoute
˚
adistance.
Truth
UlrichHackenberg,membreduconseild'administrationduconstructeu
rautomobileAudi,
d
´
eclarequelacollectedest
´
el
´
ephonesportablesavantlesr
´
eunionsduconseil,anqu'ils
nepuissentpas
‹
etreutilis
´
escommeappareilsd'
´
ecoute
˚
adistance,estunepratiquecourante
depuisdesann
´
ees.
Ourmodel
ﬁLest
´
el
´
ephonescellulaires,quisontvraimentunequestion,nonseulementp
arcequ'ils
pourraientpotentiellementcauserdesinterf
´
erencesaveclesappareilsdenavigation,mais
noussavons,selonlaFCC,qu'ilspourraientinterf
´
ereraveclestoursdet
´
el
´
ephonecellulaire
lorsqu'ilssontdansl'airﬂ,ditUNK.
Truth
ﬁLest
´
el
´
ephonesportablessontv
´
eritablementunprobl
˚
eme,nonseulementparcequ'ils
pourraient
´
eventuellementcr
´
eerdesinterf
´
erencesaveclesinstrumentsdenavigation,mais
parcequenoussavons,d'apr
˚
eslaFCC,qu'ilspourraientperturberlesantennes-relaisde
t
´
el
´
ephoniemobiles'ilssontutilis
´
es
˚
abordﬂ,ad
´
eclar
´
eRosenker.
Ourmodel
Aveclacr
´
emation,ilyaunﬁsentimentdeviolencecontrelecorpsd'un
‹
etrecherﬂ,
quiseraﬁr
´
eduit
˚
aunepiledecendresﬂentr
˚
espeudetempsaulieud'unprocessusde
d
´
ecompositionﬁquiaccompagnerales
´
etapesdudeuilﬂ.
Truth
Ilya,aveclacr
´
emation,ﬁuneviolencefaiteaucorpsaim
´
eﬂ,
quiva
‹
etreﬁr
´
eduit
˚
auntasdecendresﬂentr
˚
espeudetemps,etnonapr
˚
esunprocessusde
d
´
ecomposition,quiﬁaccompagneraitlesphasesdudeuilﬂ.
Table3:AfewexamplesoflongtranslationsproducedbytheL
STMalongsidethegroundtruth
translations.Thereadercanverifythatthetranslationsa
resensibleusingGoogletranslate.
4
7
8
12
17
22
28
35
79
test sentences sorted by their length
20
25
30
35
40
BLEU score
LSTM  (34.8)
baseline (33.3)
0
500
1000
1500
2000
2500
3000
3500
test sentences sorted by average word frequency rank
20
25
30
35
40
BLEU score
LSTM  (34.8)
baseline (33.3)
Figure3:
Theleftplotshowstheperformanceofoursystemasafunctionofsen
tencelength,wherethe
x-axiscorrespondstothetestsentencessortedbytheirlengthandismar
kedbytheactualsequencelengths.
Thereisnodegradationonsentenceswithlessthan35words,thereisonly
aminordegradationonthelongest
sentences.TherightplotshowstheLSTM'sperformanceonsentenc
eswithprogressivelymorerarewords,
wherethex-axiscorrespondstothetestsentencessortedbytheirﬁave
ragewordfrequencyrankﬂ.
replacementofan activevoicewithapassivevoice.Thetwo-
dimensionalprojectionsareobtained
usingPCA.

4Relatedwork

Thereisalargebodyofworkonapplicationsofneuralnetwor
kstomachinetranslation.Sofar,
thesimplestandmosteffectivewayofapplyinganRNN-Langu
ageModel(RNNLM)[23]ora
7
FeedforwardNeuralNetworkLanguageModel(NNLM)[3]toanM
Ttaskisbyrescoringthen-
bestlistsofastrongMTbaseline[22],whichreliablyimpro
vestranslationquality.
Morerecently,researchershavebeguntolookintowaysofin
cludinginformationaboutthesource
languageintotheNNLM.ExamplesofthisworkincludeAuliet
al.[1],whocombineanNNLM
withatopicmodeloftheinputsentence,whichimprovesresc
oringperformance.Devlinetal.[8]
followedasimilarapproach,buttheyincorporatedtheirNN
LMintothedecoderofanMTsystem
andusedthedecoder'salignmentinformationtoprovidethe
NNLMwiththemostusefulwordsin
theinputsentence.Theirapproachwashighlysuccessfulan
ditachievedlargeimprovementsover
theirbaseline.

OurworkiscloselyrelatedtoKalchbrennerandBlunsom[18]
,whowerethersttomaptheinput
sentenceintoavectorandthenbacktoasentence,althought
heymapsentencestovectorsusing
convolutionalneuralnetworks,whichlosetheorderingoft
hewords.Similarlytothiswork,Choet
al.[5]usedanLSTM-likeRNNarchitecturetomapsentencesi
ntovectorsandback,althoughtheir
primaryfocuswasonintegratingtheirneuralnetworkintoa
nSMTsystem.Bahdanauetal.[2]also
attempteddirecttranslationswithaneuralnetworkthatus
edanattentionmechanismtoovercome
thepoorperformanceonlongsentencesexperiencedbyChoet
al.[5]andachievedencouraging
results.Likewise,Pouget-Abadieetal.[26]attemptedtoa
ddressthememoryproblemofChoet
al.[5]bytranslatingpiecesofthesourcesentenceinwayth
atproducessmoothtranslations,which
issimilartoaphrase-basedapproach.Wesuspectthattheyc
ouldachievesimilarimprovementsby
simplytrainingtheirnetworksonreversedsourcesentence
s.
End-to-endtrainingisalsothefocusofHermannetal.[12],
whosemodelrepresentstheinputsand
outputsbyfeedforwardnetworks,andmapthemtosimilarpoi
ntsinspace.However,theirapproach
cannotgeneratetranslationsdirectly:togetatranslatio
n,theyneedtodoalookupforclosestvector
inthepre-computeddatabaseofsentences,ortorescorease
ntence.
5Conclusion

Inthiswork,weshowedthatalargedeepLSTMwithalimitedvo
cabularycanoutperformastan-
dardSMT-basedsystemwhosevocabularyisunlimitedonalar
ge-scaleMTtask.Thesuccessof
oursimpleLSTM-basedapproachonMTsuggeststhatitshould
dowellonmanyothersequence
learningproblems,providedtheyhaveenoughtrainingdata
.
Weweresurprisedbytheextentoftheimprovementobtainedb
yreversingthewordsinthesource
sentences.Weconcludethatitisimportanttondaprobleme
ncodingthathasthegreatestnumber
ofshorttermdependencies,astheymakethelearningproble
mmuchsimpler.Inparticular,while
wewereunabletotrainastandardRNNonthenon-reversedtra
nslationproblem(showning.1),
webelievethatastandardRNNshouldbeeasilytrainablewhe
nthesourcesentencesarereversed
(althoughwedidnotverifyitexperimentally).

WewerealsosurprisedbytheabilityoftheLSTMtocorrectly
translateverylongsentences.We
wereinitiallyconvincedthattheLSTMwouldfailonlongsen
tencesduetoitslimitedmemory,
andotherresearchersreportedpoorperformanceonlongsen
tenceswithamodelsimilartoours
[5,2,26].Andyet,LSTMstrainedonthereverseddatasethad
littledifcultytranslatinglong
sentences.

Mostimportantly,wedemonstratedthatasimple,straightf
orwardandarelativelyunoptimizedap-
proachcanoutperformamatureSMTsystem,sofurtherworkwi
lllikelyleadtoevengreatertrans-
lationaccuracies.Theseresultssuggestthatourapproach
willlikelydowellonotherchallenging
sequencetosequenceproblems.

6Acknowledgments

WethankSamyBengio,JeffDean,MatthieuDevin,GeoffreyHinton,Nal
Kalchbrenner,ThangLuong,Wolf-
gangMacherey,RajatMonga,VincentVanhoucke,PengXu,Wojciec
hZaremba,andtheGoogleBrainteam
forusefulcommentsanddiscussions.
8
References
[1]M.Auli,M.Galley,C.Quirk,andG.Zweig.Jointlanguageandtransla
tionmodelingwithrecurrent
neuralnetworks.In
EMNLP
,2013.
[2]D.Bahdanau,K.Cho,andY.Bengio.Neuralmachinetranslationb
yjointlylearningtoalignandtranslate.
arXivpreprintarXiv:1409.0473
,2014.
[3]Y.Bengio,R.Ducharme,P.Vincent,andC.Jauvin.Aneuralpro
babilisticlanguagemodel.In
Journalof
MachineLearningResearch
,pages1137Œ1155,2003.
[4]Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdepen
dencieswithgradientdescentisdifcult.
IEEETransactionsonNeuralNetworks
,5(2):157Œ166,1994.
[5]K.Cho,B.Merrienboer,C.Gulcehre,F.Bougares,H.Schwen
k,andY.Bengio.Learningphraserepresen-
tationsusingRNNencoder-decoderforstatisticalmachinetranslation.In
ArxivpreprintarXiv:1406.1078
,
2014.
[6]D.Ciresan,U.Meier,andJ.Schmidhuber.Multi-columndeepneur
alnetworksforimageclassication.
In
CVPR
,2012.
[7]G.E.Dahl,D.Yu,L.Deng,andA.Acero.Context-dependentpr
e-traineddeepneuralnetworksforlarge
vocabularyspeechrecognition.
IEEETransactionsonAudio,Speech,andLanguageProcessing-Sp
ecial
IssueonDeepLearningforSpeechandLanguageProcessing
,2012.
[8]J.Devlin,R.Zbib,Z.Huang,T.Lamar,R.Schwartz,andJ.Makh
oul.Fastandrobustneuralnetwork
jointmodelsforstatisticalmachinetranslation.In
ACL
,2014.
[9]NadirDurrani,BarryHaddow,PhilippKoehn,andKennethHeaeld
.Edinburgh'sphrase-basedmachine
translationsystemsforwmt-14.In
WMT
,2014.
[10]A.Graves.Generatingsequenceswithrecurrentneuralnetwo
rks.In
ArxivpreprintarXiv:1308.0850
,
2013.
[11]A.Graves,S.Fern
´
andez,F.Gomez,andJ.Schmidhuber.Connectionisttemporalclass
ication:labelling
unsegmentedsequencedatawithrecurrentneuralnetworks.In
ICML
,2006.
[12]K.M.HermannandP.Blunsom.Multilingualdistributedrepresentatio
nswithoutwordalignment.In
ICLR
,2014.
[13]G.Hinton,L.Deng,D.Yu,G.Dahl,A.Mohamed,N.Jaitly,A.Senio
r,V.Vanhoucke,P.Nguyen,
T.Sainath,andB.Kingsbury.Deepneuralnetworksforacousticmod
elinginspeechrecognition.
IEEE
SignalProcessingMagazine
,2012.
[14]S.Hochreiter.Untersuchungenzudynamischenneuronalenne
tzen.
Master'sthesis,InstitutfurInfor-
matik,TechnischeUniversitat,Munchen
,1991.
[15]S.Hochreiter,Y.Bengio,P.Frasconi,andJ.Schmidhuber.Gr
adient owinrecurrentnets:thedifculty
oflearninglong-termdependencies,2001.
[16]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
NeuralComputation
,1997.
[17]S.HochreiterandJ.Schmidhuber.LSTMcansolvehardlongtimela
gproblems.1997.
[18]N.KalchbrennerandP.Blunsom.Recurrentcontinuoustransla
tionmodels.In
EMNLP
,2013.
[19]A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNetclass
icationwithdeepconvolutionalneural
networks.In
NIPS
,2012.
[20]Q.V.Le,M.A.Ranzato,R.Monga,M.Devin,K.Chen,G.S.Cor
rado,J.Dean,andA.Y.Ng.Building
high-levelfeaturesusinglargescaleunsupervisedlearning.In
ICML
,2012.
[21]Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-base
dlearningappliedtodocumentrecognition.
ProceedingsoftheIEEE
,1998.
[22]T.Mikolov.
StatisticalLanguageModelsbasedonNeuralNetworks
.PhDthesis,BrnoUniversityof
Technology,2012.
[23]T.Mikolov,M.Kara
´
at,L.Burget,J.Cernock
˚
y,andS.Khudanpur.Recurrentneuralnetworkbased
languagemodel.In
INTERSPEECH
,pages1045Œ1048,2010.
[24]K.Papineni,S.Roukos,T.Ward,andW.J.Zhu.BLEU:amethod
forautomaticevaluationofmachine
translation.In
ACL
,2002.
[25]R.Pascanu,T.Mikolov,andY.Bengio.Onthedifcultyoftrainingr
ecurrentneuralnetworks.
arXiv
preprintarXiv:1211.5063
,2012.
[26]J.Pouget-Abadie,D.Bahdanau,B.vanMerrienboer,K.Cho,
andY.Bengio.Overcomingthe
curseofsentencelengthforneuralmachinetranslationusingautomatics
egmentation.
arXivpreprint
arXiv:1409.1257
,2014.
[27]A.Razborov.Onsmalldepththresholdcircuits.In
Proc.3rdScandinavianWorkshoponAlgorithm
Theory
,1992.
[28]D.Rumelhart,G.E.Hinton,andR.J.Williams.Learningrepresenta
tionsbyback-propagatingerrors.
Nature
,323(6088):533Œ536,1986.
[29]H.Schwenk.Universitylemans.
http://www-lium.univ-lemans.fr/
Ÿ
schwenk/cslm_
joint_paper/
,2014.[Online;accessed03-September-2014].
[30]M.Sundermeyer,R.Schluter,andH.Ney.LSTMneuralnetwor
ksforlanguagemodeling.In
INTER-
SPEECH
,2010.
[31]P.Werbos.Backpropagationthroughtime:whatitdoesandhowto
doit.
ProceedingsofIEEE
,1990.
9
