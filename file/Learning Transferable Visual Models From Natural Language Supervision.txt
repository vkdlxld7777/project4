Learning Transferable Visual Models From Natural Language Supervision
Alec Radford
* 1
Jong Wook Kim
* 1
Chris Hallacy
1
Aditya Ramesh
1
Gabriel Goh
1
Sandhini Agarwal
1
Girish Sastry
1
Amanda Askell
1
Pamela Mishkin
1
Jack Clark
1
Gretchen Krueger
1
Ilya Sutskever
1
Abstract
SOTA computer vision systems are trained to pre-
dict a xed set of predetermined object categories.
This restricted form of supervision limits their
generality and usability since additional labeled
data is needed to specify any other visual con-
cept. Learning directly from raw text about im-
ages is a promising alternative which leverages a
much broader source of supervision. We demon-
strate that the simple pre-training task of predict-
ing which caption goes with which image is an
efcient and scalable way to learn SOTA image
representations from scratch on a dataset of 400
million (image, text) pairs collected from the inter-
net. After pre-training, natural language is used to
reference learned visual concepts (or describe new
ones) enabling zero-shot transfer of the model to
downstream tasks. We study performance on over
30 different computer vision datasets, spanning
tasks such as OCR, action recognition in videos,
geo-localization, and many types of ne-grained
object classication. The model transfers non-
trivially to most tasks and is often competitive
with a fully supervised baseline without the need
for any dataset specic training. For instance, we
match the accuracy of the original ResNet50 on
ImageNet zero-shot without needing to use any of
the 1.28 million training examples it was trained
on. We release our code and pre-trained model
weights at
 https://github.com/OpenAI/CLIP
.
1. Introduction and Motivating Work
Pre-training methods which learn directly from raw text
have revolutionized NLP over the last few years (
Dai & Le
,
2015
;
 Peters et al.
,
 2018
;
 Howard & Ruder
,
 2018
;
 Radford
et al.
,
 2018
;
 Devlin et al.
,
 2018
;
 Raffel et al.
,
 2019
). The
development of ﬁtext-to-textﬂ as a standardized input-output
*
Equal contribution
1
OpenAI, San Francisco, CA 94110, USA.
Correspondence to:
<
f
alec, jongwook
g
@openai.com
>
.
Proceedings of the
38
th
International Conference on Machine
Learning
, PMLR 139, 2021. Copyright 2021 by the author(s).
interface (
McCann et al.
,
 2018
;
 Radford et al.
,
 2019
;
 Raffel
et al.
,
 2019
) has enabled task-agnostic architectures to zero-
shot transfer to downstream datasets. Flagship systems like
GPT-3 (
Brown et al.
,
 2020
) are now competitive across
many tasks with bespoke models while requiring little to no
dataset specic training data.
These results suggest that the aggregate supervision acces-
sible to modern pre-training methods within web-scale col-
lections of text surpasses that of high-quality crowd-labeled
NLP datasets. However, in other elds such as computer
vision it is still standard practice to pre-train models on
crowd-labeled datasets such as ImageNet (
Deng et al.
,
 2009
).
Could scalable pre-training methods which learn directly
from web text result in a similar breakthrough in computer
vision? Prior work is encouraging.
Joulin et al.
 (
2016
) demonstrated that CNNs trained to pre-
dict words in image cap tions can learn representations com-
petitive with ImageNet training.
 Li et al.
 (
2017
) then ex-
tended this approach to predicting phrase n-grams in ad-
dition to individual words and demonstrated the ability of
their system to zero-shot transfer to other image classi-
cation datasets. Adopting more recent architectures and
pre-training approaches, VirTex (
Desai & Johnson
,
 2020
),
ICMLM (
Bulent Sariyildiz et al.
,
 2020
), and ConVIRT
(
Zhang et al.
,
 2020
) have recently demonstrated the po-
tential of transformer-based language modeling, masked
language modeling, and contrastive objectives to learn im-
age representations from text.
However, the aforementioned models still under-perform
current SOTA computer vision models such as Big Trans-
fer (
Kolesnikov et al.
,
 2019
) and the weakly supervised
ResNeXt (
Mahajan et al.
,
 2018
). A crucial difference is
scale. While
 Mahajan et al.
 (
2018
) and
 Kolesnikov et al.
(
2019
) trained for accelerator years on millions to billions
of images, VirTex, ICMLM, and ConVIRT trained for ac-
celerator days on one to two hundred thousand images. We
close this gap and study the behaviors of image models
trained from natural language supervision at large scale. We
demonstrate that a simplied version of ConVIRT trained
from scratch, which we call CLIP, for Contrastive Language-
Image Pre-training, is an efcient and scalable method of
learning from natural language supervision. We nd that
Learning Transferable Visual Models From Natural Language Supervision
Figure 1.
Summary of our approach. While standard image models jointly train an image feature extractor and a linear classier to predict
some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training
examples. At test time the learned text encoder synthesizes a zero-shot linear classier by embedding the names or descriptions of the
target dataset's classes.
CLIP learns to perform a wide set of tasks during pre-
training including OCR, geo-localization, action recogni-
tion, and outperforms the best publicly available ImageNet
model while being more computationally efcient. We also
nd that zero-shot CLIP models are much more robust than
equivalent accuracy supervised ImageNet models.
2. Approach
At the core of our work is the idea of learning perception
from the supervision contained in natural language paired
with images. In the following subsections we detail our
specic approach.
2.1. Creating a Sufciently Large Dataset
Existing work has mainly used three datasets, MS-COCO
(
Lin et al.
,
 2014
), Visual Genome (
Krishna et al.
,
 2017
), and
YFCC100M (
Thomee et al.
,
 2016
). While MS-COCO and
Visual Genome are high quality crowd-labeled datasets, they
are small by modern standards with approximately 100,000
training photos each. By comparison, other computer vision
systems are trained on up to 3.5
billion
Instagram photos
(
Mahajan et al.
,
 2018
). YFCC100M, at 100 million photos,
is a possible alternative, but the metadata for each image is
sparse and of varying quality. Many images use automati-
cally generated lenames like
20160716
113957.JPG
as ﬁtitlesﬂ or contain ﬁdescriptionsﬂ of camera exposure
settings. After ltering to keep only images with natural
language titles and/or descriptions in English, the dataset
shrunk by a factor of 6 to only 15 million photos. This is
approximately the same size as ImageNet.
A major motivation for natural language supervision is the
large quantities of data of this form available publicly on
the internet. To test this we constructed a new dataset of
400 million (image, text) pairs collected form a variety of
publicly available sources on the Internet. To attempt to
cover as broad a set of visual concepts as possible, we
search for (image, text) pairs as part of the construction
process whose text includes one of a set of 500,000 queries.
We approximately class balance the results by including
up to 20,000 (image, text) pairs per query. The resulting
dataset has a similar total word count as the WebText dataset
used to train GPT-2. We refer to this dataset as WIT for
WebImageText.
1
2.2. Selecting an Efcient Pre-Training Method
Our initial approach, similar to VirTex, jointly trained an
image CNN and text transformer from scratch to predict
the caption of an image. However, we encountered difcul-
ties efciently scaling this method. In Figure
 2
 we show
that a 63 million parameter transformer language model,
which alread y uses twice the compute of its ResNet50 im-
age encoder, learns to recognize ImageNet classes three
times slower than an approach similar to
 Joulin et al.
 (
2016
)
that predicts a bag-of-words encoding of the same text.
Recent work in contrastive representation learning has found
that contrastive objectives can outperform the equivalent
predictive objective (
Tian et al.
,
 2019
). Noting this nding,
1
The base query list is all words occurring at least 100 times in
the English version of Wikipedia. This is augmented with bi-grams
with high pointwise mutual information for the pair (
Church &
Hanks
,
 1990
) as well as the names of all Wikipedia articles above a
certain search volume. Finally all WordNet (
Miller
,
 1995
) synsets
not already in the query list are added.
Learning Transferable Visual Models From Natural Language Supervision
Figure 2.
CLIP is much more efcient at zero-shot transfer
than our image caption baseline.
Although highly expressive,
we found that transformer-based language models are relatively
weak at zero-shot ImageNet classication. Here, we see that it
learns 3x slower than a baseline which predicts a bag-of-words
(BoW) encoding of the text (
Joulin et al.
,
 2016
). Swapping the
prediction objective for the contrastive objective of CLIP further
improves efciency another 4x.
we explored training a system to solve the potentially eas-
ier proxy task of predicting only which text
as a whole
is
paired with which image and not the exact words of that text.
Starting with the same bag-of-words encoding baseline, we
swapped the predictive objective for a contrastive objective
in Figure
 2
, o bserved a further 4x efciency improvement
in the rate of zero-shot transfer to ImageNet.
Given a batch of
N
(image, text) pairs, CLIP is trained to
predict which of the
N

N
possible (image, text) pairings
across a batch actually occurred. To do this, CLIP learns a
multi-modal embedding space by jointly training an image
encoder and text encoder to maximize the cosine similar-
ity of the image and text embeddings of the
N
real pairs
in the batch while minimizing the cosine similarity of the
embeddings of the
N
2

N
incorrect pairings. We optimize
a symmetric cross entropy loss over these similarity scores.
In Figure 3 we include pseudocode for the core of an imple-
mentation of CLIP. This batch construction technique and
objective was rst introduced as the
multi-class N-pair loss
Sohn
 (
2016
) and was recently adapted for contrastive (text,
image) representation learning in the d omain of medical
imaging by
 Zhang et al.
 (
2020
).
Since over-tting is not a major concern, the details of train-
ing CLIP are simplied compared to
 Zhang et al.
 (
2020
).
We train CLIP from scratch instead of initializing with pre-
trained weights. We remove the non-linear projection be-
tween the representation and the contrastive embedding
space. We use only a linear projection to map from each en-
coder's representation to the multi-modal embedding space.
Figure 3.
Numpy-like pseudocode for the core of an implementa-
tion of CLIP.
We also remove the text transformation function
t
u
which
samples a single sentence at uniform from the text since
many of the (image, text) pairs in CLIP's pre-training dataset
are only a single sentence. We also simplify the image trans-
formation function
t
v
. A random square crop from resized
images is the only data augmentation used during training.
Finally, the temperature parameter which controls the range
of the logits in the softmax,
˝
, is directly optimized during
training as a log-parameterized multiplicative scalar to avoid
turning as a hyper-parameter.
2.3. Choosing and Scaling a Model
We consider two different architectures for the image en-
coder. For the rst, we use ResNet50 (
He et al.
,
 2016a
)
as the base architecture for the image encoder due to its
widespread adoption and proven performance. We make sev-
eral modications to the original version using the ResNetD
improvements from
 He et al.
 (
2019
) and the antialiased
rect-2 blur pooling from
 Zhang
 (
2019
). We also replace
the global average pooling layer with an attention pooling
mechanism. The attention pooling is implemented as a sin-
gle layer of ﬁtransformer-styleﬂ multi-head QKV attention
where the query is conditioned on the global average-pooled
representation of the image. For the second architecture, we
experiment with the recently introduced Vision Transformer
(ViT) (
Dosovitskiy et al.
,
 2020
). We closely follow their
implementation with only the minor modication of adding
an additional layer normalization to the combined patch
and position embeddings before the transformer and use a
slightly different initialization scheme.
Learning Transferable Visual Models From Natural Language Supervision
The text encoder is a Transformer (
Vaswani et al.
,
 2017
)
with the architecture modications described in
 Radford
et al.
 (
2019
). As a base size we use a 12-layer 512-wide
model with 8 attention heads. The transformer operates on a
lower-cased byte pair encoding (BPE) representation of the
text (
Sennrich et al.
,
 2015
). The text sequence is bracketed
with
[SOS]
and
[EOS]
tokens and the activations of the
highest layer of the transformer at the
[EOS]
token are
used as th e feature representation of the text which is layer
normalized and then linearly projected into the multi-modal
embedding space. Masked self-attention was used in the text
encoder to preserve the ability to add language modeling as
an auxiliary objective, though exploration of this is left as
future work.
While previous computer vision research has often scaled
models by increasing the width (
Mahajan et al.
,
 2018
) or
depth (
He et al.
,
 2016a
) in isolation, for the ResNet image
encoders we adapt the approach of
 Tan & Le
 (
2019
) which
found that allocating additional compute across all of width,
depth, and resolution outperforms allocating it to only one
dimension. We use a simple variant which allocates addi-
tional compute equally to increasing the w idth, depth, and
resolution of the model. For the text encoder, we only scale
the width of the model to be proportional to the calculated
increase in width of the ResNet and do not scale the depth
at all, as we found CLIP's performance to be less sensitive
to the text encoder.
2.4. Pre-training
We train a series of 5 ResNets and 3 Vision Transformers.
For the ResNets we train a ResNet50, a ResNet101, and
then 3 more which follow EfcientNet-style model scaling
and use approximately 4x, 16x, and 64x the compute of a
ResNet50. They are denoted as RN50x4, RN50x16, and
RN50x64 respectively. For the Vision Transformers we
train a ViT-B/32, a ViT-B/16, and a ViT-L/14. The largest
ResNet model, RN50x64, took 18 days to train on 592 V100
GPUs while the largest Vision Transformer took 12 days on
256 V100 GPUs. For the ViT-L/14 we also pre-train at a
higher 336 pixel resolution for one additional epoch to boost
performance similar to FixRes (
Touvron et al.
,
 2019
). We
denote this model as ViT-L/14@336px. Unless otherwise
specied, all results reported in this paper as ﬁCLIPﬂ use
this model which we found to perform best. Full model
hyperparameters and details are in supplementary material.
2.5. Using CLIP
CLIP is pre-trained to predict if an image and a text snip-
pet are paired together in WIT. To apply CLIP to down-
stream tasks, we reuse this capability and study the zero-
shot transfer performance of CLIP on standard computer
vision datasets. Similar to
 Radford et al.
 (
2019
) we motivate
aYahoo ImageNet SUN
Visual N-Grams 72.4 11.5 23.0
CLIP
98.4 76 .2 58.5
Table 1.
Comparing CLIP to prior zero-shot transfer image classi-
cation work. CLIP improves performance on all three datasets by
a large amount. This improvement reects many diff erences since
the development of Visual N-Grams (
Li et al.
,
 2017
).
this as a way of measuring the
task learning
capability of a
system (as opposed to its
representation learning
capability).
For each dataset, we use the names of all the classes in the
dataset as the set of potential text pairings and predict the
most probable (image, text) pair according to CLIP. We addi-
tionally experiment with providing CLIP with text prompts
to help specify the task as well as ensembling multiple of
these templates in order to boost performance. However,
since the vast majority of unsupervised and self-supervised
computer vision research focuses on representation learning,
we also investigate this for CLIP using the common linear
probe protocol.
3. Analysis
3.1. Initial Comparison to Visual N-Grams
To our knowledge, Visual N-Grams (
Li et al.
,
 2017
) rst
studied zero-shot transfer to existing image classication
datasets in the manner described above. It is also the only
other work we are aware of that has studied zero-shot trans-
fer to standard image classication datasets using a task
agnostic pre-trained model. In Table
 1
 we compare Visual
N-Grams to CLIP. The best CLIP model improves accuracy
on ImageNet from a proof of concept 11.5% to 76.2% and
matches the performance of the original ResNet50 despite
using none of the 1.28 million crowd-labeled training exam-
ples. Additionally, the top-5 accuracy of CLIP models are
noticeably higher and this model has a 95% top-5 accuracy,
matching Inception-V4 (
Szegedy et al.
,
 2016
). The abil-
ity to match the performance of a strong, fully supervised
baseline in a zero-shot setting suggests CLIP is a signi-
cant step towards exible and practical zero-shot computer
vision classiers. This comparison is not direct because
many differences between CLIP and Visual N-Grams were
not controlled for. As a closer comparison, we trained a
CLIP ResNet50 on the same YFCC100M dataset that Vi-
sual N-Grams was trained on and found it matched their
reported ImageNet performance within a V100 GPU day.
This baseline was also trained from scratch instead of being
initialized from pre-trained ImageNet weights as in Visual
N-Grams.
Learning Transferable Visual Models From Natural Language Supervision
Figure 4.
Zero-shot CLIP is competitive with a fully super-
vised baseline.
Across a 27 dataset eval suite, a zero-shot CLIP
classier outperforms a fully supervised linear classier tted on
ResNet50 features on 16 datasets, including ImageNet.
3.2. Zero-Shot Performance
In computer vision, zero-shot learning usually refers to the
study of generalizing to unseen object categories in image
classication (
Lampert et al.
,
 2009
). We instead use the
term in a broader sense and study generalization to unseen
datasets. We motivate this as a proxy for performing un-
seen tasks, as aspired to in the zero-data learning paper of
Larochelle et al.
 (
2008
). While much research in the eld of
unsupervised learning focuses on the
representation learn-
ing
capabilities of machine learning systems, we motivate
studying zero-shot transfer as a way of measuring the
task-
learning
capabilities of machine learning systems. In this
view, a dataset evaluates performance on a task on a spe-
cic distribution. However, many popular computer vision
datasets were created by the research community primarily
as benchmarks to guide the development of generic image
classication methods rather than measuring performance
on a specic task. To our knowledge, Visual N-Grams (
Li
et al.
,
 2017
) rst studied zero-shot transfer to existing image
classication datasets in the manner described above
To conduct a more comprehensive analysis, we implement
a much larger evaluation suite detailed in the supplementary
material. In total we expand from the 3 datasets reported in
Visual N-Grams to include over 30 datasets and compare to
over 50 existing computer vision systems to contextualize
results. To start, we look at how well CLIP's zero-shot clas-
siers perform when compared to the a simple off-the-shelf
baseline: tting a fully supervised, regularized, logistic re-
gression classier on the features of the canonical ResNet50.
In Figure
 4
 we show this comparison across 27 datasets.
Zero-shot CLIP outperforms this baseline slightly and wins
on 16 of the 27 datasets. The dataset zero-shot CLIP im-
proves by the most is STL10, a dataset designed to en-
courage unsupervised learning by containing only a limited
number of labeled examples. Zero-shot CLIP, without using
any training examples, achieves 99.3% on this dataset which
appears to be a new SOTA. On ne-grained classication
tasks, we observe a wide spread in performance. On two of
these datasets, Stanford Cars and Food101, zero-shot CLIP
outperforms logistic regression on ResNet50 features by
over 20% while on Flowers102 and FGVCAircraft, zero-
shot CLIP underperforms by over 10%. We suspect these
differences are primarily due to varying amounts of per-task
supervision between WIT and ImageNet. On ﬁgeneralﬂ ob-
ject classication datasets such as ImageNet, CIFAR10, and
PascalVOC2007 performance is relatively similar with a
slight advantage for zero-shot CLIP. Zero-shot CLIP sig-
nicantly outperforms a ResNet50 on two datasets measur-
ing action recognition in videos. On Kinetics700, CLIP
outperforms a ResNet50 by 14.5%. Zero-shot CLIP also
outperforms a ResNet50's features by 7.7% on UCF101.
We speculate this is due to natural language providing wider
supervision for visual concepts involving verbs, compared
to the noun-centric object supervision in ImageNet.
Looking at where zero-shot CLIP notably underperforms,
we see that zero-shot CLIP is quite weak on several spe-
cialized, complex, or abstract tasks such as satellite image
classication (EuroSAT and RESISC45), lymph node tumor
detection (PatchCamelyon), counting objects in synthetic
scenes (CLEVRCounts), self-driving related tasks such as
German trafc sign recognition (GTSRB), recognizing dis-
tance to the nearest car (KITTI Distance). These results
highlight the poor capability of zero-shot CLIP on more
complex tasks. By contrast, non-expert humans can robustly
perform several of these tasks, such as counting, satellite
image classication, and trafc sign recognition, suggesting
signicant room for improvement. However, we caution
that it is unclear whether measuring zero-shot transfer, as
opposed to few-shot transfer, is a meaningful evaluation for
difcult tasks that a learner has no prior experience with,
such as lymph node tumor classication for almost all hu-
mans (and possibly CLIP).
While comparing zero-shot performance to fully supervised
models contextualizes the task-learning capabilities of CLIP,
comparing to few-shot methods is a more direct comparison,
since zero-shot is its limit. In Figure
 5
, we visualize how
zero-shot CLIP compares to few-shot logistic regression on
Learning Transferable Visual Models From Natural Language Supervision
Figure 5.
Zero-shot CLIP outperforms few-shot linear probes.
Zero-shot CLIP matches the average performance of a 4-shot linear
classier trained on the same feature space and nearly matches the
best results of a 16-shot linear classier across publicly available
models. For both BiT-M and SimCLRv2, the best performing
model is highlighted. Light gray lines are other models in the eval
suite. The 20 datasets with at least 16 examples per class were
used in this analysis.
the features of many image models including the best pub-
licly available ImageNet models, self-supervised learning
methods, and CLIP itself. While one might expect zero-shot
to underperform one-shot, we instead nd that zero-shot
CLIP matches the performance of 4-shot logistic regression
on the same feature space. This is likely due to a key dif-
ference between the zero-shot and few-shot approach. First,
CLIP's zero-shot classier is generated via natural language
which allows for visual concepts to be directly specied
(ﬁcommunicatedﬂ). By contrast, ﬁnormalﬂ supervised learn-
ing must infer concepts indirectly from training examples.
Context-less example-based learning has the drawback that
many different hypotheses can be consistent with the data,
especially in the one-shot case. A single image often con-
tains many different visual concepts. Although a capable
learner is able to exploit visual cues and heuristics, such as
assuming that the concept being demonstrated is the primary
object in an image, there is no guarantee.
When comparing zero-shot CLIP to few-shot logistic re-
gression on the features of other models, zero-shot CLIP
roughly matches the performance of the best performing
16-shot classier in our evaluation suite, which uses the fea-
tures of a BiT-M ResNet152x2 trained on ImageNet-21K.
We are certain that a BiT-L model trained on JFT-300M
would perform even better but these models have not been
publicly released. That a BiT-M ResNet152x2 performs
best in a 16-shot setting is somewhat surprising since, as
analyzed in Section
 3.3
, the Noisy Student EfcientNet-L2
outperforms it in a fully supervised setting by almost 5% on
average across 27 datasets.
3.3. Representation Learning
While we have focused on studying the task-learning capa-
bilities of CLIP through zero-shot transfer, it is more com-
mon to study the representation learning capabilities of a
model. We use a linear probe evaluation protocol because it
requires minimal hyper-parameter tuning and has standard-
ized evaluation procedures. Please see the supplementary
material for further details on evaluation.
Figure
 6
 summarizes our ndings. To minimize selection
effects that could raise concerns of conrmation or reporting
bias, we rst study performance on the 12 dataset evalua-
tion suite from
 Kornblith et al.
 (
2019
). Models trained with
CLIP scale very well with compute and our largest model
slightly outperforms the best existing model (a Noisy Stu-
dent EfcientNet-L2) on both overall score and compute
efciency. We also nd that CLIP vision transformers are
about 3x more compute ef cient than CLIP ResNets, which
allows higher overall performance within our compute bud-
get. These results replicate the ndings of
 Dosovitskiy et al.
(
2020
) which reported that vision transformers are more
compute efcient than convnets when trained on suf ciently
large datasets. Our best overall model ViT-L/14@336px
outperforms the best existing model across this evaluation
suite by an average of 2.6%.
CLIP models learn a wider set of tasks than has previously
been demonstrated in a single computer vision model trained
end-to-end from random initializatio n. These tasks include
geo-localization, optical character recognition, facial emo-
tion recognition, and action recognition. None of these
tasks are measured in the evaluation suite of
 Kornblith et al.
(
2019
). This could be argued to be a form of selection bias
in
 Kornblith et al.
 (
2019
)'s study towards tasks that overlap
with ImageNet. To address this, we also measure perfor-
mance on a broader 27 dataset evaluation suite. This eval-
uation suite, detailed in Appendix
 A
 includes datasets rep-
resenting the aforementioned tasks, German Trafc Signs
Recognition Benchmark (
Stallkamp et al.
,
 2011
), as well
as several other datasets adapted from VTAB (
Zhai et al.
,
2019
). On this broader evaluation suite, the benets of CLIP
are more clear. All CLIP models, regardless of scale, outper-
form all evaluated systems in terms of compute efciency.
The improvement in average score of the best model over
previous systems increases from 2.6% to 5%.
3.4. Robustness to Natural Distribution Shift
In 2015, it was announced that a deep learning model ex-
ceeded human performance on the ImageNet test set (
He
et al.
,
 2015
). However, research in the subsequent years has
repeatedly found that these models still make many simple
mistakes (
Dodge & Karam
,
 2017
;
 Geirhos et al.
,
 2018
;
 Al-
corn et al.
,
 2019
), and new benchmarks testing these systems
has often found their performance to be much lower than
Learning Transferable Visual Models From Natural Language Supervision
Figure 6.
Linear probe performance of CLIP models in comparison with SOTA computer vision models
, including EfcientNet
(
Tan & Le
,
 2019
;
 Xie et al.
,
 2020
), MoCo (
Chen et al.
,
 2020b
), Instagram-pretrained ResNeXt models (
Mahajan et al.
,
 2018
;
 Touvron
et al.
,
 2019
), BiT (
Kolesnikov et al.
,
 2019
), ViT (
Dosovitskiy et al.
,
 2020
), SimCLRv2 (
Chen et al.
,
 2020a
), BYOL (
Grill et al.
,
 2020
), and
the original ResNet models (
He et al.
,
 2016b
). (Left) Scores are averaged over 12 datasets studied by
 Kornblith et al.
 (
2019
). (Right)
Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ne-tuned or evaluated on
images at a higher-resolution than pre-training. Please see supplementary material for individual model scores for each dataset.
both human accuracy and ImageNet performance (
Recht
et al.
,
 2019
;
 Barbu et al.
,
 2019
).
 Taori et al.
 (
2020
) is a re-
cent comprehensive study moving towards quantifying and
understanding this for ImageNet models.
 Taori et al.
 (
2020
)
study how the performance of ImageNet models change
when evaluated on
natural distribution shifts
. They measure
performance on a set of 7 distribution shifts.
 Taori et al.
(
2020
) nd that accuracy under distribution shift increases
predictably with ImageNet accuracy and is well modeled
as a linear function of logit-transformed accuracy.
 Taori
et al.
 (
2020
) use this nding to propose that robustness
analysis should distinguish between
effective
and
relative
robustness. Effective robustness measures improvements
in accuracy under distribution shift above what is predicted
by the documented relationship between in-distribution and
out-of-distribution accuracy. Relative robustness captures
any improvement in out-of-distribution accuracy.
 Taori et al.
(
2020
) argue that robustness techniques should aim to im-
prove both effective robustness and relative robustness.
However, almost all models studied in
 Taori et al.
 (
2020
) are
trained or ne-tuned on the ImageNet dataset. Is training
or adapting to the ImageNet dataset distribution the cause
of the observed robustness gap? Intuitively, a zero-shot
model should not be able to exploit spurious correlations
or patterns that hold only on a specic distribution, since it
is not trained on that distribution. Thus it is possible that
zero-shot models exhibit higher effective robustness. In
Figure
 7
, we compare the performance of zero-shot CLIP
with existing ImageNet models on natural distribution shifts.
All zero-shot CLIP models improve effective robustness
by a large amount and reduce the gap between ImageNet
accuracy and accuracy under distribution shift by up to
75%. Zero-shot CLIP models trace a completely distinct
robustness frontier from all 204 prior models studied in
Taori et al.
 (
2020
). These results suggest that the recent shift
towards large-scale task and dataset agnostic pre-training
combined with a reorientation towards zero-shot transfer
evaluation (as advocated by
 Yogatama et al.
 (
2019
) and
Linzen
 (
2020
)) promotes the development of more robust
systems and provides a more accurate assessment of true
model performance.
Learning Transferable Visual Models From Natural Language Supervision
Figure 7.
Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models.
(Left) An ideal robust model
(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink
this ﬁrobustness gapﬂ by up to 75%. Linear ts on logit transformed values are shown with bootstrap estimated 95% condence intervals.
(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of
the best zero-shot CLIP model is compared with a model that has the same performance on the ImageNet validation set, ResNet101.
4. Data Overlap Analysis
A concern with pre-training on a very large internet dataset
is unintentional overlap with downstream evals. We con-
ducted de-duplication analysis to investigate this with full
details in the supplementary material. Out of 35 datasets
studied, 9 datasets have no detected overlap at all. There is
a median overlap of 2.2% and an average overlap of 3.2%.
Due to this small amount of overlap, overall accuracy is
rarely shifted by more than 0.1% with only 7 datasets above
this threshold. Of these, only 2 are statistically signicant
after Bonferroni correction. The max detected improve-
ment is only 0.6% on Birdsnap. This echos the ndings of
similar duplicate analysis in previous work on large scale
pre-training.
 Mahajan et al.
 (
2018
) and
 Kolesnikov et al.
(
2019
) detected similar overlap rates for their models and
also observed minimal changes in overall performance.
5. Broader Impacts
CLIP allows people to design their own classiers and re-
moves the need for task-specic training data. How these
classes are designed heavily inuences both model per-
formance and model biases. For example, we nd that
when given a set of labels including Fairface race labels
(
K
¨
arkk
¨
ainen & Joo
,
 2019
) and a handful of egregious terms
such as ﬁcriminalﬂ and ﬁanimalﬂ the model tends to classify
images of people aged 0Œ20 in the egregious category at a
rate of 32.3%. However, when we add the class ﬁchildﬂ to
the list of possible classes, this behaviour drops to 8.7%.
We also found discrepancies across gender and race for peo-
ple categorized into the `crime' and `non-human' categories,
highlighting the potential for disparate impact even when
extreme care is taken for thoughtful class design.
Additionally, given that CLIP does not need task-specic
training data, it can unlock certain niche tasks with greater
ease. Some of these tasks may raise privacy or surveillance
related risks, which we explore by testing CLIP's perfor-
mance on celebrity identication using the CelebA dataset
(
Liu et al.
,
 2018
). CLIP has a top-1 accuracy of 59.2% for
ﬁin the wildﬂ celebrity image classication when choosing
from 100 candidates and of 43.3% when choosing from
1000 possible choices. Although it's noteworthy to achieve
these results with task agnostic pre-training, this perfor-
mance is not competitive with widely available production
level models. We explore challenges that CLIP poses in our
supplemental materials and hope that this work motivates
future research on the characterization of the capabilities,
shortcomings, and biases of such models.
6. Limitations
The performance of zero-shot CLIP is often just compet-
itive with the supervised baseline of a linear classier on
ResNet-50 features. This baseline is now well below the
overall SOTA. Signicant work is still needed to improve
the task learning and transfer capabilities of CLIP. We es-
timate around a 1000x increase in compute is required for
zero-shot CLIP to reach overall SOTA performance across
our evaluation suite. This is infeasible to train with cur-
rent hardware. Further research into improving upon the
computational and data efciency of CLIP will be necessary.
Despite our emphasis on zero-shot transfer, we repeatedly
Learning Transferable Visual Models From Natural Language Supervision
queried performance on validation sets to guide develop-
ment. This is unrealistic for true zero-shot scenarios. Similar
concerns have been raised in the eld of semi-supervised
learning (
Oliver et al.
,
 2018
). Another potential issue is our
selection of evaluation datasets. While we report results
on
 Kornblith et al.
 (
2019
)'s 12 dataset evaluation suite as a
standardized collection, our main analysis uses a somewhat
haphazard collection of 27 datasets that is undeniably co-
adapted with the capabilities of CLIP. A new benchmark of
tasks designed to evaluate broad zero-shot transfer capabili-
ties would help address this issue.
We emphasize that specifying image classiers through nat-
ural language is a exible interface but this has its own
limitations. Many complex tasks can be difcult to specify
just through text. Actual training examples are undeniably
useful but CLIP does not optimize for few-shot performance
directly. We fall back to tting linear classiers on top of
CLIP's features. This results in a counter-intuitive drop
in performance when transitioning from a zero-shot to a
few-shot setting.
7. Related Work
The idea of learning to perform computer vision tasks from
natural language supervision is by no means new. Rather,
our main contribution is studying its behavior at large scale.
Over 20 years ago
 Mori et al.
 (
1999
) explored improving
content based image retrieval by training a model to predict
the nouns and adjectives in text paired with images.
 Quat-
toni et al.
 (
2007
) demonstrated it was possible to learn more
data ef cient image representations via manifold learning in
the weight space of classiers trained to predict words in im-
age captions.
 Srivastava & Salakhutdinov
 (
2012
) explored
deep representation learning by training multimodal Deep
Boltzmann Machines on top of low-level image and text tag
features. More recent work inspiring CLIP is described in
the Introduction.
Learning from collections of internet images is commonly
investigated in webly supervised learning with
 Fergus et al.
(
2005
) demonstrating the ability to train competitive com-
puter vision classiers by treating image search engine re-
sults as supervision. Of this line of work,
Learning Every-
thing about Anything: Webly-Supervised Visual Concept
Learning
(
Divvala et al.
,
 2014
) has a notably similar ambi-
tion and goal as CLIP.
Developments in zero-shot computer vision (
Larochelle
et al.
,
 2008
;
 Lampert et al.
,
 2009
) were essential for CLIP.
Socher et al.
 (
2013a
) demonstrated that connecting image
and language representations enabled zero-shot transfer to
unseen classes on CIFAR10 and
 Frome et al.
 (
2013
) im-
proved and scaled this nding to ImageNet. The idea of
generating a classier from natural language dates back to
at least
 Elhoseiny et al.
 (
2013
) and a form similar to CLIP's
zero-shot classier was explored in
 Lei Ba et al.
 (
2015
).
Natural language supervision has also been explored for
tasks beyond image classication including video under-
standing (
Ramanathan et al.
,
 2013
;
 Miech et al.
,
 2019
), Re-
inforcement Learning (
Hermann et al.
,
 2017
), and a burst of
recent work on learning joint models of vision and language
(
Lu et al.
,
 2019
;
 Tan & Bansal
,
 2019
;
 Chen et al.
,
 2019
;
 Li
et al.
,
 2020b
;
 Yu et al.
,
 2020
) for complex joint tasks beyond
those studied here including visual question answering.
8. Conclusion
We have investigated whether it is possible to transfer the
success of task-agnostic web-scale pre-training in NLP to
another domain. We nd that adopting this formula re-
sults in similar behaviors emerging in the eld of computer
vision and discuss the social implications of this line of
research. In order to optimize their training objective, CLIP
models learn to perform a wide variety of tasks during pre-
training. This task learning can then be leveraged via natural
language prompting to enable zero-shot transfer to many
existing datasets. At sufcient scale, the performance of this
approach can be competitive with task-specic supervised
models although there is still room for much improvement.
A
C K N O W L E D G M E N T S
We'd like to thank the millions of people involved in creating
the data CLIP is trained on. We'd also like to thank Susan
Zhang for her work on image conditional language models
while at OpenAI, Ishaan Gulrajani for catching an error in
the pseudocode, and Irene Solaiman, Miles Brundage, and
Gillian Hadeld for their thoughtful feedback on the broader
impacts section of the paper. We are also grateful to the
Acceleration and Supercomputing teams at OpenAI for their
critical work on software and hardware infrastructure this
project used. Finally, we'd also like to thank the developers
of the many software packages used throughout this project
including, but not limited, to Numpy (
Harris et al.
,
 2020
),
SciPy (
Virtanen et al.
,
 2020
), ftfy (
Speer
,
 2019
), Tensor-
Flow (
Abadi et al.
,
 2016
), PyTorch (
Paszke et al.
,
 2019
),
pandas (
pandas development team
,
 2020
), and scikit-learn
(
Pedregosa et al.
,
 2011
).
References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
Tensorow: A system for large-scale machine learning. In
12th
f
USENIX
g
symposium on operating systems design
and implementation (
f
OSDI
g
16)
, pp. 265Œ283, 2016.
Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi
´
c,
Learning Transferable Visual Models From Natural Language Supervision
R., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,
and Zisserman, A. Self-supervised multimodal versatile
networks.
arXiv preprint arXiv:2006.16228
, 2020.
Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-
S., and Nguyen, A. Strike (with) a pose: Neural networks
are easily fooled by strange poses of familiar objects. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition
, pp. 4845Œ4854, 2019.
Assiri, Y. Stochastic optimization of plain convolutional
neural networks with simple methods.
arXiv preprint
arXiv:2001.08856
, 2020.
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-
freund, D., Tenenbaum, J., and Katz, B. Objectnet: A
large-scale bias-controlled dataset for pushing the lim-
its of object recognition models. In
Advances in Neural
Information Processing Systems
, pp. 9453Œ9463, 2019.
Bechmann, A. and Bowker, G. C. Unsupervised by any
other name: Hidden layers of knowledge production in
articial intelligence on social media.
Big Data & Society
,
6(1):205395171881956, January 2019. doi: 10.1177/
2053951718819569. URL
https://doi.org/10.
1177/2053951718819569
.
Blaise Aguera y Arcas, M. M. and Todorov,
A. Physiognomy's new clothes. 2017.
URL
https://medium.com/@blaisea/
physiognomys- new- clothes- f2d4b59fdd6a
.
Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and
Kalai, A. T. Man is to computer programmer as woman
is to homemaker? debiasing word embeddings.
Advances
in neural information processing systems
, 29:4349Œ4357,
2016.
Bowker, G. C. and Star, S. L.
Sorting things out: Classica-
tion and its consequences
. MIT press, 2000.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165
, 2020.
Browne, S.
Dark Matters: Surveillance of Blackness
. Duke
University Press, 2015.
Bulent Sariyildiz, M., Perez, J., and Larlus, D. Learning
visual representations with caption annotations.
arXiv
e-prints
, pp. arXivŒ2008, 2020.
Buolamwini, J. and Gebru, T. Gender shades: Intersec-
tional accuracy disparities in commercial gender classi-
cation. In
Conference on fairness, accountability and
transparency
, pp. 77Œ91, 2018.
Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A
short note on the kinetics-700 human action dataset.
arXiv
preprint arXiv:1907.06987
, 2019.
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and
Hinton, G. Big self-supervised models are strong semi-
supervised learners.
arXiv preprint arXiv:2006.10029
,
2020a.
Chen, X., Fan, H., Girshick, R., and He, K. Improved
baselines with momentum contrastive learning.
arXiv
preprint arXiv:2003.04297
, 2020b.
Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,
Cheng, Y., and Liu, J. Uniter: Learning universal image-
text representations.
arXiv preprint arXiv:1909.11740
,
2019.
Cheng, G., Han, J., and Lu, X. Remote sensing image scene
classication: Benchmark and state of the art.
Proceed-
ings of the IEEE
, 105(10):1865Œ1883, 2017.
Church, K. W. and Hanks, P. Word association norms,
mutual information, and lexicography.
Computational
Linguistics
, 16(1):22Œ29, 1990. URL
https://www.
aclweb.org/anthology/J90- 1003
.
Coates, A., Ng, A., and Lee, H. An analysis of single-
layer networks in unsupervised feature learning. In
Pro-
ceedings of the fourteenth international conference on
articial intelligence and statistics
, pp. 215Œ223, 2011.
Crawford, K. The trouble with bias.
NIPS 2017
Keynote
, 2017. URL
https://www.youtube.com/
watch?v=fMym_BKWQzk
.
Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.
In
Advances in neural information processing systems
,
pp. 3079Œ3087, 2015.
D'Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-
panahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,
J., Hoffman, M. D., et al. Underspecication presents
challenges for credibility in modern machine learning.
arXiv preprint arXiv:2011.03395
, 2020.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-
Fei, L. ImageNet: A Large-Scale Hierarchical Image
Database. In
CVPR09
, 2009.
Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,
and Fei-Fei, L. Ilsvrc 2012, 2012. URL
http://www.
image- net.org/challenges/LSVRC/2012/
.
Desai, K. and Johnson, J. Virtex: Learning visual rep-
resentations from textual annotations.
arXiv preprint
arXiv:2006.06666
, 2020.
Learning Transferable Visual Models From Natural Language Supervision
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding.
arXiv preprint arXiv:1810.04805
,
2018.
Divvala, S. K., Farhadi, A., and Guestrin, C. Learning
everything about anything: Webly-supervised visual con-
cept learning. In
Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
, pp. 3270Œ
3277, 2014.
Dodge, S. and Karam, L . A study and comparison of human
and deep learning recognition performance under visual
distortions. In
2017 26th international conference on
computer communication and networks (ICCCN)
, pp. 1Œ
7. IEEE, 2017.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale.
arXiv
preprint arXiv:2010.11929
, 2020.
Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-
er: Zero-shot learning using purely textual descriptions.
In
Proceedings of the IEEE International Conference on
Computer Vision
, pp. 2584Œ2591, 2013.
Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-
ing object categories from google's image search. In
Tenth IEEE International Conference on Computer Vision
(ICCV'05) Volume 1
, volume 2, pp. 1816Œ1823. IEEE,
2005.
Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,
Ranzato, M., and Mikolov, T. Devise: A deep visual-
semantic embedding model. In
Advances in neural infor-
mation processing systems
, pp. 2121Œ2129, 2013.
Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.
Large-scale adversarial training for vision-and-language
representation learning.
arXiv preprint arXiv:2006.06195
,
2020.
Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-
guage models better few-shot learners.
arXiv preprint
arXiv:2012.15723
, 2020.
Garvie, C., May 2019. URL
https://www.
flawedfacedata.com/
.
Geiger, A., Lenz, P., and Urtasun, R. Are we ready for
autonomous driving? the kitti vision benchmark suite. In
Conference on Computer Vision and Pattern Recognition
(CVPR)
, 2012.
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-
mann, F. A., and Brendel, W. Imagenet-trained cnns are
biased towards texture; increasing shape bias improves ac-
curacy and robustness.
arXiv preprint arXiv:1811.12231
,
2018.
Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,
Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,
D., Lee, D.-H., et al. Challenges in representation learn-
ing: A report on three machine learning contests.
Neural
Networks
, 64:59Œ63, 2015.
Google. Google cloud api: Celebrity recognition. URL
https://cloud.google.com/vision/docs/
celebrity- recognition
.
Grill, J.-B., Strub, F., Altch
´
e, F., Tallec, C., Richemond,
P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,
Z. D., Azar, M. G., et al. Bootstrap your own latent: A
new approach to self-supervised learning.
arXiv preprint
arXiv:2006.07733
, 2020.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., Fern
´
andez del
R
´
o, J., Wiebe, M., Peterson, P., G
´
erard-Marchant, P.,
Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,
Gohlke, C., and Oliphant, T. E. Array programming
with NumPy.
Nature
, 585:357Œ362, 2020. doi: 10.1038/
s41586- 020- 2649- 2.
Hays, J. and Efros, A. A. Im2gps: estimating geographic
information from a single image. In
2008 ieee confer-
ence on computer vision and pattern recognition
, pp. 1Œ8.
IEEE, 2008.
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep
into rectiers: Surpassing human-level performance on
imagenet classication. In
Proceedings of the IEEE inter-
national conference on computer vision
, pp. 1026Œ1034,
2015.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In
Proceedings of the IEEE
conference on computer vision and pattern recognition
,
pp. 770Œ778, 2016a.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In
Proceedings of the IEEE
conference on computer vision and pattern recognition
,
pp. 770Œ778, 2016b.
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-
mentum contrast for unsupervised visual representation
learning. In
Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition
, pp. 9729Œ
9738, 2020.
Learning Transferable Visual Models From Natural Language Supervision
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
Bag of tricks for image classication with convolutional
neural networks. In
Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
, pp. 558Œ
567, 2019.
Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:
A novel dataset and deep learning benchmark for land
use and land cover classication.
IEEE Journal of Se-
lected Topics in Applied Earth Observations and Remote
Sensing
, 12(7):2217Œ2226, 2019.
Henaff, O. Data-ef cient image recognition with contrastive
predictive coding. In
International Conference on Ma-
chine Learning
, pp. 4182Œ4192. PMLR, 2020.
Hendrycks, D. and Gimpel, K. Gaussian error linear units
(gelus).
arXiv preprint arXiv:1606.08415
, 2016.
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,
R., and Song, D. Pretrained transformers improve out-of-
distribution robustness.
arXiv preprint arXiv:2004.06 100
,
2020.
Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,
Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,
M., Teplyashin, D., et al. Grounded language learning in
a simulated 3d world.
arXiv preprint arXiv:1706.06551
,
2017.
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,
Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,
Y. Deep learning scaling is predictable, empirically.
arXiv
preprint arXiv:1712.00409
, 2017.
Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:
Enhancing image geolocalization by combinatorial parti-
tioning of maps. In
Proceedings of the European Confer-
ence on Computer Vision (ECCV)
, pp. 536Œ551, 2018.
Howard, J. and Ruder, S. Universal language model
ne-tuning for text classication.
arXiv preprint
arXiv:1801.06146
, 2018.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167
, 2015.
Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,
A. Deep structured output learning for unconstrained text
recognition.
arXiv preprint arXiv:1412.5903
, 2014.
Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial
transformer networks.
Advances in neural information
processing systems
, 28:2017Œ2025, 2015.
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,
Lawrence Zitnick, C., and Girshick, R. Clevr: A diag-
nostic dataset for compositional language and elemen tary
visual reasoning. In
Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
, pp.
2901Œ2910, 2017.
Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.
Learning visual features from large weakly supervised
data. In
European Conference on Computer Vision
, pp.
67Œ84. Springer, 2016.
Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal
modeling in 3d cnn architectures with bert for action
recognition.
arXiv preprint arXiv:2008.01232
, 2020.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361
, 2020.
Keyes, O. The misgendering machines: Trans/hci implica-
tions of automatic gender recognition.
Proceedings of the
ACM on Human-Computer Interaction
, 2(CSCW):1Œ22,
2018.
Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,
Ringshia, P., and Testuggine, D. The hateful memes
challenge: Detecting hate speech in multimodal memes.
arXiv preprint arXiv:2005.04790
, 2020.
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,
J., Gelly, S., and Houlsby, N. Large scale learning of
general visual representations for transfer.
arXiv preprint
arXiv:1912.11370
, 2019.
Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet
models transfer better? In
Proceedings of the IEEE
conference on computer vision and pattern recognition
,
pp. 2661Œ2671, 2019.
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,
D. A., et al. Visual genome: Connecting language and
vision using crowdsourced dense image annotations.
In-
ternational journal of computer vision
, 123(1):32Œ73,
2017.
K
¨
arkk
¨
ainen, K. and Joo, J. Fairface: Face attribute dataset
for balanced race, gender, and age, 2019.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-
man, S. J. Building machines that learn and think like
people, 2016.
Lampert, C. H., Nickisch, H., and Harmeling, S. Learning
to detect unseen object classes by between-class attribute
transfer. In
2009 IEEE Conference on Computer Vision
and Pattern Recognition
, pp. 951Œ958. IEEE, 2009.
Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning
of new tasks. 2008.
Learning Transferable Visual Models From Natural Language Supervision
LeCun, Y. The mnist database of handwritten digits.
http://yann. lecun. com/exdb/mnist/
.
Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep
zero-shot convolutional neural networks using textual
descriptions. In
Proceedings of the IEEE International
Conference on Computer Vision
, pp. 4247Œ4255, 2015.
Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning
visual n-grams from web data. In
Proceedings of the
IEEE International Conference on Computer Vision
, pp.
4183Œ4192, 2017.
Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.
Unicoder-vl: A universal encoder for vision and language
by cross-modal pre-training. 2020a.
Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,
L., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-
semantics aligned pre-training for vision-language tasks.
arXiv preprint arXiv:2004.06165
, 2020b.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-
manan, D., Doll
´
ar, P., and Zitnick, C. L. Microsoft coco:
Common objects in context. In
European conference on
computer vision
, pp. 740Œ755. Springer, 2014.
Linzen, T. How can we accelerate progress towards
human-like linguistic generalization?
arXiv preprint
arXiv:2005.00955
, 2020.
Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-
toniou, G., Shutova, E., and Yannakoudakis, H. A mul-
timodal framework for the detection of hateful memes.
arXiv preprint arXiv:2012.12871
, 2020.
Liu, Z., Luo, P., Wang, X., and Tang, X. Large-scale celeb-
faces attributes (celeba) dataset.
Retrieved August
, 15
(2018):11, 2018.
Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining
task-agnostic visiolinguistic representations for vision-
and-language tasks. In
Advances in Neural Information
Processing Systems
, pp. 13Œ23, 2019.
Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging
weakly supervised data and pose representation for action
recognition, 2020. URL
https://www.youtube.
com/watch?v=KOQFxbPPLOE&t=1390s
.
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,
M., Li, Y., Bharambe, A., and van der Maaten, L. Ex-
ploring the limits of weakly supervised pretraining. In
Proceedings of the European Conference on Computer
Vision (ECCV)
, pp. 181Œ196, 2018.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The
natural language decathlon: Multitask learning as ques-
tion answering.
arXiv preprint arXiv:1806.08730
, 2018.
Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,
I., and Sivic, J. Howto100m: Learning a text-video em-
bedding by watching hundred million narrated video clips.
In
Proceedings of the IEEE international conference on
computer vision
, pp. 2630Œ2640, 2019.
Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-
man, A. Rareact: A video dataset of unusual interactions.
arXiv preprint arXiv:2008.01018
, 2020a.
Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,
and Zisserman, A. End-to-end learning of visual represen-
tations from uncurated instructional videos. In
Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition
, pp. 9879Œ9889, 2020b.
Miller, G. A. Wordnet: a lexical database for english.
Com-
munications of the ACM
, 38(11):39Œ41, 1995.
Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect
of natural distribution shift on question answering models.
arXiv preprint arXiv:2004.14444
, 2020.
Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni-
tion using higher order language priors. 2012.
Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans-
formation based on dividing and vector quantizing images
with words. Citeseer, 1999.
Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-
cation estimation of photos using a hierarchical model
and scene classication. In
Proceedings of the European
Conference on Computer Vision (ECCV)
, pp. 563Œ579,
2018.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,
and Ng, A. Y. Reading digits in natural images with
unsupervised feature learning. 2011.
Noble, S. U. Algorithms of oppression: How search engines
reinforce racism. 2018.
Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-
ing implicit group attitudes and beliefs from a demonstra-
tion web site.
Group Dynamics: Theory, Research, and
Practice
, 6(1):101, 2002.
Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,
J. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.
A large-scale benchmark dataset for event recognition in
surveillance video. In
CVPR 2011
, pp. 3153Œ3160. IEEE,
2011.
Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-
fellow, I. Realistic evaluation of deep semi-supervised
learning algorithms.
Advances in neural information pro-
cessing systems
, 31:3235Œ3246, 2018.
Learning Transferable Visual Models From Natural Language Supervision
pandas development team, T. pandas-dev /pandas: Pan-
das, February 2020. URL
https://doi.org/10.
5281/zenodo.3509134
.
Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,
C. V. Cats and dogs. In
IEEE Conference on Computer
Vision and Pattern Recognition
, 2012.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Wallach, H.,
Larochelle, H., Beygelzimer, A., d
˜
Alch
´
e-Buc, F., Fox, E.,
and Garnett, R. (eds.),
Advances in Neural Information
Processing Systems 32
, pp. 8024Œ8035, 2019.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python.
Journal of
Machine Learning Research
, 12:2825Œ2830, 2011.
Pennington, J., Socher, R., and Manning, C. D. Glove:
Global vectors for word representation. In
Proceedings
of the 2014 conference on empirical methods in natural
language processing (EMNLP)
, pp. 1532Œ1543, 2014.
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations.
arXiv preprint arXiv:1802.05365
,
2018.
Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,
A. Imagebert: Cross-modal pre-training with large-
scale weak-supervised image-text data.
arXiv preprint
arXiv:2001.07966
, 2020.
Quattoni, A., Collins, M., and Darrell, T. Learning visual
representations using images with captions. In
2007 IEEE
Conference on Computer Vision and Pattern Recognition
,
pp. 1Œ8. IEEE, 2007.
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by generative pre-
training, 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a unied text-to-text
transformer.
arXiv preprint arXiv:1910.10683
, 2019.
Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,
J., and Denton, E. Saving face: Investigating the ethical
concerns of facial recognition auditing, 2020.
Ramanathan, V., Liang, P., and Fei-Fei, L. Video event
understanding using natural language descriptions. In
Proceedings of the IEEE International Conference on
Computer Vision
, pp. 905Œ912, 2013.
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-
agenet classiers generalize to imagenet?
arXiv preprint
arXiv:1902.10811
, 2019.
Salimans, T. and Kingma, D. P. Weight normalization: A
simple reparameterization to accelerate training of deep
neural networks. In
Advances in neural information pro-
cessing systems
, pp. 901Œ909, 2016.
Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How
computers see gender: An evaluation of gender classica-
tion in commercial facial analysis services.
Proceedings
of the ACM on Human-Computer Interaction
, 3(CSCW):
1Œ33, 2019.
Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,
S., Schoonvelde, M., and Lockhart, J. W. Diagnosing
gender bias in image recognition systems.
Socius
, 6:
2378023120967171, 2020.
Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units.
arXiv
preprint arXiv:1508.07909
, 2015.
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,
Batra, D., Parikh, D., and Rohrbach, M. Towards vqa
models that can read. In
Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
, pp.
8317Œ8326, 2019.
Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. Zero-
shot learning through cross-modal transfer. In
Advances
in neural information processing systems
, pp. 935Œ943,
2013a.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A. Y., and Potts, C. Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 conference on empirical methods
in natural language processing
, pp. 1631Œ1642, 2013b.
Sohn, K. Improved deep metric learning with multi-class
n-pair loss objective. In
Advances in neural information
processing systems
, pp. 1857Œ1865, 2016.
Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-
Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,
Kreps, S., McCain, M., Newhouse, A., Blazakis, J.,
McGufe, K., and Wang, J. Release strategies and the
social impacts of language models, 2019.
Learning Transferable Visual Models From Natural Language Supervision
Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset
of 101 human actions classes from videos in the wild.
arXiv preprint arXiv:1212.0402
, 2012.
Speer, R. ftfy. Zenodo, 2019. URL
https://doi.org/
10.5281/zenodo.2591652
. Version 5.5.
Srivastava, N. and Salakhutdinov, R. Multimodal learning
with deep boltzmann machines. In
NIPS
, 2012.
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The
German Trafc Sign Recognition Benchmark: A multi-
class classication competition. In
IEEE International
Joint Conference on Neural Networks
, pp. 1453Œ1460,
2011.
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,
A. Inception-v4, inception-resnet and the impact
of residual connections on learning.
arXiv preprint
arXiv:1602.07261
, 2016.
Tan, H. and Bansal, M. Lxmert: Learning cross-modality
encoder representations from transformers.
arXiv preprint
arXiv:1908.07490
, 2019.
Tan, M. and Le, Q. V. Efcientnet: Rethinking model
scaling for convolutional neural networks.
arXiv preprint
arXiv:1905.11946
, 2019.
Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,
and Schmidt, L. Measuring robustness to natural dis-
tribution shifts in image classication.
arXiv preprint
arXiv:2007.00644
, 2020.
Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,
K., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: Th e
new data in multimedia research.
Communications of the
ACM
, 59(2):64Œ73, 2016.
Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview
coding.
arXiv preprint arXiv:1906.05849
, 2019.
Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and
Isola, P. Rethinking few-shot image classication: a
good embedding is all you need?
arXiv preprint
arXiv:2003.11539
, 2020.
Touvron, H., Vedaldi, A., Douze, M., and J
´
egou, H. Fix-
ing the train-test resolution discrepancy. In
Advances in
neural information processing systems
, pp. 8252Œ8262,
2019.
Varadarajan, J. and Odobez, J.-M. Topic models for scene
analysis and abnormality detection. In
2009 IEEE 12th
International Conference on Computer Vision Workshops,
ICCV Workshops
, pp. 1338Œ1345. IEEE, 2009.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser,

., and Polosukhin, I. Atten-
tion is all you need. In
Advances in neural information
processing systems
, pp. 5998Œ6008, 2017.
Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and
Welling, M. Rotation equivariant CNNs for digital pathol-
ogy. June 2018.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser, W., Bright, J., van der Walt, S. J., Brett, M.,
Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat,

I.,
Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,
Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,
Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,
F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy
1.0: Fundamental Algorithms for Scientic Computing
in Python.
Nature Methods
, 17:261Œ272, 2020. doi:
10.1038/s41592- 019- 0686- 2.
Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the
deep learning era. In
Proceedings of the IEEE Interna-
tional Conference on Computer Vision
, pp. 2621Œ2630,
2017.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. Glue: A multi-task benchmark and anal-
ysis platform for natural language understanding.
arXiv
preprint arXiv:1804.07461
, 2018.
Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,
M., Wang, Y., and Liu, W. All you need is boundary: To-
ward arbitrary-shaped text spotting. In
Proceedings of the
AAAI Conference on Articial Intelligence
, volume 34,
pp. 12160Œ12167, 2020.
Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-
cation with convolutional neural networks. In
European
Conference on Computer Vision
, pp. 37Œ55. Springer,
2016.
Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-
shick, R. Detectron2.
https://github.com/
facebookresearch/detectron2
, 2019.
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training
with noisy student improves imagenet classication. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition
, pp. 10687Œ10698, 2020.
Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,
L., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware
pre-training for text-vqa and text-caption.
arXiv preprint
arXiv:2012.04638
, 2020.
Learning Transferable Visual Models From Natural Language Supervision
Yogatama, D., d'Autume, C. d. M., Connor, J., Kocisky,
T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,
Yu, L., Dyer, C., et al. Learning and evaluating general
linguistic intelligence.
arXiv preprint arXiv:1901.11373
,
2019.
Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,
and Wang, H. Ernie-vil: Knowledge enhanced vision-
language representations through scene graph.
arXiv
preprint arXiv:2006.16934
, 2020.
Zeiler, M. D. and Fergus, R. Visualizing and understand-
ing convolutional networks. In
European conference on
computer vision
, pp. 818Œ833. Springer, 2014.
Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,
Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-
mann, M., Dosovitskiy, A., et al. A large-scale study of
representation learning with the visual task adaptation
benchmark.
arXiv preprint arXiv:1910.04867
, 2019.
Zhang, R. Making convolutional networks shift-invariant
again.
arXiv preprint arXiv:1904.11486
, 2019.
Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-
glotz, C. P. Contrastive learning of medical visual repre-
sentations from paired images and text.
arXiv preprint
arXiv:2010.00747
, 2020.
Zuboff, S. Big other: surveillance capitalism and the
prospects of an information civilization.
Journ al of Infor-
mation Technology
, 30(1):75Œ89, 2015.
