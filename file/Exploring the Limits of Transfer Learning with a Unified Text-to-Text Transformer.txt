Journal of Machine Learning Research 21 (2020) 1-67 Subm itted 1/20; Revised 6/20; Published 6/20
Exploring the Limits of Transf er Learning with a Uni˝ed
Text-to-Text Transformer
Colin Ra˙el

craffel@gmail.com
Noam Shazeer

noam@google.com
Adam Rob erts

adarob@google.com
Katherine Lee

katherinelee@google.com
Sharan Narang
sharannarang@google.com
Michael Matena
mmatena@google.com
Yanqi Zhou
yanqiz@google.com
Wei Li
mweili@google.com
Peter J. Liu
peterjliu@google.com
Google, Mountain View, CA 94043, USA
Editor:
Ivan Titov
Abstract
Transfer learning, where a mo del is ˝rst pre-trained on a data-rich task b efore b eing ˝ne-
tuned on a downstream task, has emerged as a p owerful technique in natural language
pro cessing (NLP). The e˙ectiveness of transfer learning has given rise to a diversity of
approaches, metho dology, and practice. In this pap er, we explore the landscap e of transfer
learning techniques for NLP by intro ducing a uni˝ed framework that converts all text-based
language problems into a text-to-text format. Our systematic study compares pre-training
ob jectives, architectures, unlab eled data sets, transfer approaches, and other factors on
dozens of language understanding tasks. By combining the insights from our exploration
with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results
on many b enchmarks covering summarization, question answering, text classi˝cation, and
more. To facilitate future work on transfer learning for NLP, we release our data set,
pre-trained mo dels, and co de.
1
Keywords:
transfer learning, natural language pro cessing, multi-task learning, attention-
based mo dels, deep learning
1. Intro duction
Training a machine learning mo del to p erform natural language pro cessing (NLP) tasks
often requires that the mo del can pro cess text in a way that is amenable to downstream
learning. This can b e lo osely viewed as developing general-purp ose knowledge that allows
the mo del to understand text. This knowledge can range from low-level (e.g. the sp elling
*
.
Equal contribution. A description of each author's contribution is available in App endix A. Corresp ondence
to
craffel@gmail.com
.
1.
https://github.com/google- research/text- to- text- transfer- transformer
©
2020 Colin Ra˙el, Noam Shaz eer, Adam Rob erts, Katherine Lee , Sharan Narang, Michael Matena, Yanqi Zhou,
We i Li, and Peter J. Liu.
License: CC-BY 4.0, see
https://creativecommons.org/licenses/by/4.0/
. Attribution requirements are provided
at
http://jmlr.org/papers/v21/20- 074.html
.
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
or meaning of words) to high-level (e.g. that a tuba is to o large to ˝t in most backpacks).
In mo dern machine learning practice, providing this knowledge is rarely done explicitly;
instead, it is often learned as part of an auxiliary task. For example, a historically common
approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map
word identities to a continu ous representation where, ideally, similar words map to similar
vectors. These vectors are often learned through an ob jective that, for example, encourages
co-o ccurring words to b e p ositioned nearby in the continuous space (Mikolov et al., 2013b).
Recently, it has b ecome increasingly common to pre-train the entire mo del on a data-rich
task. Ideally, this pre-training causes the mo del to develop general-purp ose abilities and
knowledge that can then b e transferred to downstream tasks. In applications of tran sfer
learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski
et al., 2014), pre- train in g is typically done via sup ervised learning on a large lab eled data set
like ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, mo d ern techniques
for transfer learning in NLP often pre-train using uns up ervised learning on unlab eled data.
This approach has recently b een used to obtain state-of-the-art results in many of the most
common NLP b enchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu
et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsup ervised pre-training
for NLP is particularly attractive b ecause unlab eled text data is available en masse thanks
to the In examp le, the Common Crawl pro ject
2
pro duces ab out 20TB of text
data extracted from web pages each month. This is a natu ral ˝t for neural networks, which
have b een shown to exhibit remarkab le scalability, i.e. it is often p ossible to achieve b etter
p erformance simply by training a larger mo d el on a larger data set (Hestness et al., 2017;
Shazeer et al., 2017; Jozefowicz et al., 2016; Maha jan et al., 2018; Radford et al., 2019;
Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).
This synergy has resulted in a great deal of recent work developing transfer learning
metho dology for NLP, which has pro duced a wide landscap e of pre-training ob jectives
(Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlab eled
data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), b en chmarks (Wan g et al.,
2019b, 2018; Conneau and Kiela, 2018), ˝ne-tuning metho ds (Howard and Ruder, 2018;
Houlsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity
of techniques in this burgeoning ˝ eld can make it di˚cult to compare di˙erent algorithms,
tease apart the e˙ects of new contrib utions, and understand the space of existing metho ds for
transfer learning. Motivated by a n eed for more rigorou s understanding, we leverage a uni˝ed
approach to trans fer learning that allows us to systematically study di˙erent approaches
and pu sh the current limits of th e ˝eld.
The basic idea underlying our work is to treat every text pro cessing problem as a
text-to-text problem, i.e. taking text as inpu t and pro ducing new text as outpu t. This
approach is inspired by previous un ifying frameworks for NLP tasks , including casting all text
problems as qu es tion answering (McCann et al., 2018), language mo d eling (Radford et al.,
2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework
allows us to directly apply the same mo del, ob jective, training pro cedure, and deco ding
pro cess to every task we consider. We leverage this ˛exibility by evaluating p erformance
on a wide variety of English-b ased NLP problems, in clud ing question answering, do cument
2.
http://commoncrawl.org
2
Exploring the Limits of Transfer Learning
Figure 1:
A diagram of our text-to- text framework. Every task we 
translation, question answering, and cast as feeding our mo del
text as input and training it to generate some target text. This allows us to use the
same mo del, loss function, hyp erparameters, etc. across our diverse s et of tasks. It
also provid es a standard testb ed for the metho ds inclu ded in our empirical survey.
T5 refers to our mo del, which we dub the 
T
ext-
t
o-
T
ext
T
ransfer
T
ransformer.
summarization, and sentiment classi˝cation, to name a few. With this uni˝ed approach ,
we can compare the e˙ectiveness of di˙erent transfer learnin g ob jectives, unlab eled data
sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up
mo dels and data sets b eyond what has previou sly b een considered.
We emphasize that our goal is n ot to prop ose new metho d s but instead to provide a
comprehensive p ersp ective on where the ˝eld s tands. As such, our work primarily comprises
a survey, exploration, and empirical comparison of exis ting techniques. We also explore the
limits of current approaches by scaling u p the insights from our systematic study (training
mo dels up to
11
billion parameters) to obtain state-of-the-art results in many of the tasks
we consider. In order to p erform exp eriments at this scale, we intro duce the Colossal Clean
Crawled Corpus (C4), a data set consisting of hundreds of gigabytes of clean English text
scrap ed from the web. Recognizing that the main utility of transfer learning is the p ossibility
of leveraging pre-trained mo dels in data-scarce settings, we release our co d e, data sets, and
pre-trained mo dels.
1
The remainder of the pap er is structured as follows: In the following section, we discus s
our base mo del an d its implementation, our pro cedure f or formulating every text pro ces sing
problem as a text-to-text tas k, and the suite of tasks we cons id er. In Section 3, we present a
large set of exp eriments th at explore the ˝eld of tran sfer learning for NLP. At the end of the
section (Section 3.7), we comb in e insights from our systematic study to obtain state-of-the-art
results on a wide variety of b enchmarks. Finally, we provide a summary of our results and
wrap up with a lo ok towards the future in S ection 4.
3
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
2. Setup
Before presenting the results from our large-scale empirical study, we review the necessary
background topics required to understand our results, including the Transformer mo del
architecture and the downstream tasks we evaluate on. We also intro duce our approach
for treating every problem as a text- to-text task and describ e our Colossal Clean Crawled
Corpus (C4), the Common Crawl-based data set we created as a source of unlab eled text
data. We refer to our mo del and framework as the 
T
ext-
t
o-
T
ext
T
ransfer
T
ransformer
(T5).
2.1. Mo del
Early results on transfer learn ing for NLP leveraged recu rrent neural networks (Peters
et al., 2018; Howard and Rud er, 2018), but it has recently b ecome more common to use
mo dels b ased on the Transformer arch itecture (Vaswani et al., 2017). The Transformer
was initially shown to b e e˙ective for machine translation, but it has subsequently b een
used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCan n
et al., 2018; Yu et al., 2018). Due to its in creasing ubiquity, all of the mo dels we study are
based on the Transf ormer architecture. Apart from the details mentioned b elow and the
variants we explore in Section 3.2, we do not deviate signi˝cantly from th is architecture as
originally prop osed. Instead of providing a comprehensive de˝nition of this mo del, we refer
the interested reader to the original pap er (Vaswani et al., 2017) or follow- up tutorials
3 , 4
for
a more detailed intro duction.
The primary building blo ck of the Transformer is self-attention (Cheng et al., 2016).
Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that pro cesses
a sequence by replacing each element by a weighted average of the rest of the s equ en ce.
The original Transformer consisted of an enco der-deco der architecture an d was intended
for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has
recently also b ecome common to use mo dels con sisting of a single Transformer layer stack,
with varying forms of self-attention used to pro duce architectures approp riate for language
mo deling (Radford et al., 2018; Al-Rfou et al., 2019) or classi˝cation and span prediction
tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural
variants in Section 3.2.
Overall, our enco der-deco der Transformer implementation closely follows its originally-
prop osed form (Vaswani et al., 2017). First, an input sequence of tokens is map p ed to
a sequence of emb eddings, which is then passed into the enco der. The enco der consists
of a stack of blo cks, each of which comprises two sub comp onents: a s elf- attention layer
followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is app lied to
the input of each sub comp onent. We use a simpli˝ed version of layer normalization where
the activations are only rescaled and no add itive bias is app lied. After layer normalization,
a residual skip connection (He et al., 2016) adds each sub comp onent's input to its output.
Drop out (Srivastava et al., 2014) is applied within the feed-forward network, on the skip
connection, on the attention weights, and at the input and output of the entire stack. The
deco der is similar in structure to th e enco der except that it inclu des a standard attention
3.
http://nlp.seas.harvard.edu/2018/04/03/attention.html
4.
http://jalammar.github.io/illustrated- transformer/
4
Exploring the Limits of Transfer Learning
mechanism after each self-attention layer that attends to the output of the enco der. The
self-attention mechanism in the deco der also uses a form of autoregressive or causal self-
attention , which only allows th e mo del to attend to past outputs. The output of the ˝nal
deco der blo ck is fed into a dense layer with a softmax outp ut, whose weights are shared with
the input emb ed ding matrix. All attention mechanisms in the Transf ormer are split u p into
indep endent heads  whose outputs are concatenated b efore b eing further pro cessed.
Since self-attention is order-indep endent (i.e. it is an op eration on sets), it is common
to provide an explicit p osition signal to the Transformer. While the original Transformer
used a sinusoidal p osition signal or learned p osition emb eddings, it has recently b ecome
more common to use relative p osition emb eddings (Shaw et al., 2018; Huang et al., 2018a).
Instead of using a ˝xed emb edding for each p osition, relative p osition emb eddings pro duce
a di˙erent learn ed emb edding according to the o˙set b etween the key and query b eing
compared in the self-attention mechanism. We us e a simpli˝ed form of p os ition emb eddings
where each emb ed ding is s imply a scalar that is added to the corresp onding logit used
for computing th e attention weights. For e˚ciency, we also share the p osition emb edding
parameters across all layers in our mo del, though within a given layer each attention head
uses a di˙erent learned p osition emb edding. Typically, a ˝xed numb er of emb eddings are
learned, each corresp onding to a range of p ossible key-query o˙sets. In this work, we u se
32
emb eddings for all of our mo dels with ranges that increase in size logarithmically up to an
o˙set of
128
b eyond which we ass ign all relative p ositions to the same emb edd in g. Note
that a given layer is insensitive to relative p osition b eyond
128
tokens, but subsequent layers
can build a sens itivity to larger o˙sets by combining lo cal information from previous layers.
To summarize, our mo del is roughly equivalent to the original Transformer p rop osed by
Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing th e layer
normalization outside the residual path, an d using a di˙erent p osition emb edding sch eme.
Since these architectural changes are orthogonal to the exp erimental factors we consider in
our empirical survey of transfer learning, we leave th e ablation of their imp act for future
work.
As part of our study, we exp eriment with the scalability of these mo dels, i.e. how their
p erformance changes as they are made to have more parameters or layers. Training large
mo dels can b e non-trivial since they might not ˝t on a single machin e and require a great deal
of computation. As a result, we use a combination of mo del and data parallelism and train
mo dels on slices of Cloud TPU Po ds.
5
TPU p o ds are are multi-rack ML sup ercomputers
that contain
1
;
024
TPU v3 chips connected via a high-sp eed 2D mesh interconnect with
supp orting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,
2018) f or ease of implementation of b oth mo del parallelism an d data parallelism (Krizhevsky,
2014).
2.2. The Colossal Clean Crawled Corpus
Much of the previous work on transfer learning for NLP makes use of large unlab eled d ata
sets for un sup ervised learning. In this pap er, we are interested in measuring the e˙ect of the
quality, characteris tics , and size of this unlab eled data. To generate data sets that satisfy
our needs, we leverage Common Crawl as a source of text scrap ed from the web. Common
5.
https://cloud.google.com/tpu/
5
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Crawl has previously b een used as a source of text data for NLP, for example to train an
n-gram language mo del (Buck et al., 2014), as training data for commonsense reasoning
(Trinh and Le, 2018), for mining parallel texts for machine translation (S mith et al., 2013),
as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and
even simply as a giant text corpus for testing optimizers (Anil et al., 2019).
Common Crawl is a publicly-available web archive that provides web extracted text
by removing markup and other non-text content from th e scrap ed HTML ˝les. Th is pro cess
pro duces around 20TB of scrap ed text data each month. Unfortunately, the ma jority of the
resulting text is not n atural language. Instead, it largely comprises gibb erish or b oiler-plate
text like menus, error messages, or duplicate text. Furthermore, a go o d deal of the scrap ed
text contains content that is unlikely to b e helpfu l for any of the tasks we consider (o˙ensive
language, placeholder text, source co de, etc.). To address these issues, we used the following
heuristics for cleaning up Common Crawl's web extracted text:
‹
We only retained lines that ended in a terminal punctuation mark (i.e. a p erio d,
exclamation mark, qu es tion mark, or end quotation mark).
‹
We discarded any page with fewer th an 5 sentences and only retain ed lines that
contained at least 3 words.
‹
We removed any page that contained any word on the List of Dirty, Naughty, Obscene
or Otherwise Bad Words.
6
‹
Many of the scrap ed pages contained warnings stating th at Javascrip t should b e
enabled so we removed any lin e with the word Javascript.
‹
Some pages had placeholder lorem ipsu m text; we removed any page where the
phrase lorem ipsum app eared.
‹
Some pages inad vertently contained co de. Since the curly bracket { app ears in
many programming languages (such as Javascript, widely used on the web) but not in
natural text, we removed any pages that contained a curly bracket.
‹
To deduplicate the data set, we discarded all but one of any three-sentence span
o ccurring more than once in the data set.
Additionally, since most of our downstream tasks are fo cu sed on English-lan guage text,
we used
langdetect
7
to ˝lter out any pages that were not classi˝ed as English with a
probability of at least 0.99. Our heuristics are inspired by past work on using Common
Crawl as a source of data f or NLP: For example, Grave et al. (2018) also ˝lter text using an
automatic language d etector and discard short lines and Smith et al. (2013); Grave et al.
(2018) b oth p erform line-level deduplication. However, we opted to create a new data set
b ecause prior data sets u se a more limited set of ˝ltering heuristics, are not publicly available,
and/or are di˙erent in scop e (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,
2019c), comprise only Creative Commons content (Hab ernal et al., 2016), or are fo cused on
parallel training data for mach in e translation (Smith et al., 2013)).
6.
https://github.com/LDNOOBW/List- of- Dirty- Naughty- Obscene- and- Otherwise- Bad- Words
7.
https://pypi.org/project/langdetect/
6
Exploring the Limits of Transfer Learning
To assemble our base data set, we downloaded the web extracted text from April 2019
and applied the aforementioned ˝ltering. This pro duces a collection of text that is not on ly
orders of magnitude larger than most data sets used for pre-training (ab out 750 GB) but also
comprises reasonably clean an d natural English text. We dub this data set the 
C
olossal
C
lean
C
rawled
C
orpus (or C4 for short) and release it as part of TensorFlow Datasets.
8
We consider the impact of using various alternative versions of this data set in Section 3.4.
2.3. Downstream Tasks
Our goal in this p ap er is to measure general langu age learning abilities. As such, we study
downstream p erformance on a diverse set of b enchmarks, including mach in e translation,
question answering, abstractive summarization, and text classi˝cation. S p eci˝cally, we
measure p erf orman ce on the GLUE and Su p erGLUE text classi˝cation meta-b enchmarks;
CNN/Daily Mail abstractive summarization; SQuAD question answerin g; and WMT English
to German, French, and Romanian translation. All data was sourced from TensorFlow
Datasets.
9
GLUE (Wang et al., 2018) and Sup erGLUE (Wang et al., 2019b) each comprise a
collection of text classi˝cation tasks meant to test general language understanding abilities:
‹
Sentence acceptability judgment (CoLA (Warstadt et al., 2018))
‹
Sentiment analysis (SST-2 (So cher et al., 2013))
‹
Paraphrasing/sentence similarity (MRPC (Dolan and Bro ckett, 2005), STS-B (Cer
et al., 2017), QQP (Iyer et al., 2017))
‹
Natural language inference (MNLI (Williams et al., 2017), QNLI (Ra jpurkar et al.,
2016), RTE (Dagan et al., 2005), CB (De Marne˙ et al., 2019))
‹
Coreference resolution (WNLI and WSC (Levesque et al., 2012))
‹
Sentence completion (COPA (Ro emmele et al., 2011))
‹
Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))
‹
Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018),
Bo olQ (Clark et al., 2019))
We use the data sets as distributed by the GLUE and Sup erGLUE b enchmarks. For
simplicity, wh en ˝ne-tu ning we treat all of the tasks in the GLUE b enchmark (and similarly
for Sup erGLUE) as a single task by concatenating all of the constituent data sets. As
suggested by Ko cijan et al. (2019) we also include the De˝nite Pronoun Resolution (DPR)
data set (Rahman and Ng, 2012) in the combined S up erGLUE task.
The CNN/Daily Mail (Hermann et al., 2015) data set was intro duced as a question-
answering task but was adapted for text summarization by Nallapati et al. (2016); we
use the non-anonymized version from S ee et al. (2017) as an abstractive su mmarization
task. SQuAD (Ra jpurkar et al., 2016) is a common question-ans wering b enchmark. In our
8.
https://www.tensorflow.org/datasets/catalog/c4
9.
https://www.tensorflow.org/datasets
7
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
exp eriments, the mo del is f ed the question and its context and asked to generate the answer
token-by-token. For WMT En glis h to German, we use the same training data as (Vaswani
et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and
newstest2013
as a validation set (Bo jar et al., 2014). For English to French, we use the standard training
data from 2015 and
newstest2014
as a validation set (Bo jar et al., 2015). For English to
Romanian, which is a standard lower-resource machine translation b enchmark, we use the
train and validation sets from WMT 2016 (Bo jar et al., 2016). Note th at we only pre-train
on English data, so in order to learn to trans late a given mo del will need to learn to generate
text in a new language.
2.4. Input and Output Format
In order to train a single mo del on the diverse set of tasks describ ed ab ove, we cast all of
the tasks we consider into a text-to-text is, a task where the mo del is fed
some text for context or conditioning and is then asked to pro duce some output text. This
framework provides a cons is tent training ob jective b oth for pre-training and ˝ne-tuning.
Sp eci˝cally, the mo del is trained with a maximum likeliho o d ob jective (using teacher forcing
(Williams and Zipser, 1989)) regardless of the task. To sp ecify which task the mo del should
p erform, we add a tas k-sp eci˝c (text) pre˝x to the original input sequence b ef ore feeding it
to the mo del.
As an examp le, to ask the mo del to translate the sentence That is go o d. from En glis h
to German, the mo del would b e fed the sequence translate English to German: That is
go o d . and would b e trained to outp ut Das ist gut. For text classi˝cation tasks, the
mo del simply predicts a single word corresp onding to the target lab el. For example, on the
MNLI b enchmark (Williams et al., 2017) the goal is to predict whether a premise implies
(entailment), contradicts (contradiction), or neither (neutral) a hyp othesis. With
our prepro cessing, th e input sequence b ecomes mnli premise: I hate pigeons. hyp othesis:
My feelings towards pigeon s are ˝lled with animosity. with the corresp onding target word
entailment. Note that an issue arises if our mo del outputs text on a text class i˝ cation
task that do es not corresp ond to any of the p ossible lab els (for example if the mo del
outputs hamburger when the only p os sible lab els for a task were entailment, neutral,
or contradiction). In this case, we always count the mo del's outpu t as wrong, though we
never observed this b ehavior in any of our trained mo dels. Note that the choice of text pre˝x
used for a given task is essentially a hyp erparameter; we foun d that changing the exact
wording of the pre˝x had limited impact and so did not p erform exten sive exp eriments into
di˙erent pre˝x choices. A diagram of our text-to-text framework with a few input/output
examples is shown in Figure 1. We provide full examples of prepro cessed inputs for every
task we studied in App endix D.
Our text-to-text framework follows previous work that casts multiple NLP tasks into
a common format: McCan n et al. (2018) prop ose the Natural Lan guage Decat hlon, a
b enchmark that uses a consistent question- answering format for a s uite of ten NLP tasks.
The Natural Language Decathlon also stipulates that all mo dels must b e multi-task, i.e.
are able to simultaneously tackle all of th e tasks at once. We instead allow for separately
˝ne-tuning the mo del on each individual task and use short tas k pre˝xes instead of an explicit
question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of
8
Exploring the Limits of Transfer Learning
language mo dels by feeding some input to the mo del as a pre˝x and then autoregressively
sampling an output. For example, automatic summarization is done by feeding in a do cument
followed by th e text TL;DR: (short for to o long, d idn't read, a common abbreviation)
and then the summary is predicted via autoregressive d eco ding. We mainly consider mo dels
that explicitly pro cess an input with an enco der b efore generating an output with a separate
deco der and we fo cus on transf er learning rather than zero- shot learning. Finally, Keskar
et al. (2019b) unify many NLP tasks as span extraction, where text corresp onding to
p ossible output choices are app ended to the input and the mo del is train ed to ex tract the
input span corres p onding to the correct choice. In contrast, our framework als o allows for
generative tas ks like machine translation an d abs tractive summarization where it is not
p ossible to enumerate all p ossible ou tput choices.
We were able to straightf orwardly cast all of the tasks we considered into a text-to- text
format with the exception of STS-B, which is a regression task where the goal is to predict
a similarity score b etween
1
and
5
. We found that
most
of these scores were annotated
in increments of
0
:
2
, so we simply rounded any score to the nearest increment of
0
:
2
and
converted the resu lt to a literal string representation of the numb er (e.g. the ˛oating-p oint
value
2
:
57
would b e mapp ed to the string 2.6). At tes t time, if the mo del outp uts a
string corresp onding to a numb er b etween
1
and
5
, we convert it to a ˛oating- p oint value;
otherwise, we treat the mo del's prediction as incorrect. This e˙ectively recasts the STS-B
regression problem as a 21-class classi˝cation problem.
Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from Sup er-
GLUE, and the DPR data set we add to Sup erGLUE) into a simpler format that is more
amenable to the text-to-text framework. Examples from the Winograd tasks consist of a
text passage containing an ambiguous pronoun that could refer to more than one of the noun
phrases in the passage. For example, the passage might b e The city councilmen refused
the demonstrators a p ermit b ecause they feared violence., which contains the ambiguous
pronoun they that could refer to city cou ncilmen or demonstrators. We cast the WNLI,
WSC, an d DPR tasks as text-to-text problems by highlightin g the ambiguous pronoun in
the text passage and asking the mo del to predict the noun that it refers to. The example
mentioned ab ove would b e transformed to the input The city councilmen ref used the
demonstrators a p ermit b ecause *they* feared violen ce. and the mo del would b e trained to
predict the target text The city councilmen.
For WSC, examples contain the passage, the ambiguous pronoun, a can didate n ou n,
and a True/False lab el re˛ecting whether the candidate matches the pronoun (ignoring any
articles). We only train on examples with a True lab el since we do not know the correct
noun targets for examples with a False lab el. For evalu ation, we assign a True lab el if
the words in the mo del's output are a subset of the word s in the candidate n oun phrase
(or vice versa) and assign a False lab el oth erwise. This removes roughly half of the WSC
training set, but the DPR data set adds ab out
1
;
000
pronoun resolution examples. Examples
from DPR are annotated with the correct referent noun, making it easy to use th is data set
in the format listed ab ove.
The WNLI training and validation sets h ave a signi˝cant overlap with the WSC training
set. To avoid leaking validation examples into our training data (a particular issue in th e
multi-task exp eriments of Section 3.5.2) , we therefore never train on WNLI and never rep ort
results on the WNLI validation set. Omitting results on the WNLI validation set is standard
9
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
practice (Devlin et al., 2018) due to the fact that it is adversarial with resp ect to the
training set, i.e. validation examples are all slightly-p erturb ed versions of training examples
with the opp osite lab el. As such, we do not in clu de WNLI in the average GLUE score
whenever we rep ort on the validation set (all sections except Section 3.7 where results
are presented on the test sets). Converting examples from WNLI to the ref erent noun
prediction variant describ ed ab ove is a little more involved; we d es crib e this pro cess in
App endix B.
3. Exp eriments
Recent advances in transfer learning for NLP have come from a wide variety of developments,
such as new pre-training ob jectives, mo del architectures, unlab eled data sets , and more.
In this section, we carry out an empirical survey of these techniques in hop es of teasing
apart their contribution and signi˝cance. We then combine th e insights gained to attain
state-of-the-art in many of the tasks we cons ider. Since transfer learning for NLP is a rapidly
growing area of research, it is not feasible for us to cover every p ossible techniqu e or idea
in our empirical study. For a broader literature review, we recommend a recent survey by
Ruder et al. (2019).
We systematically study these contributions by taking a reasonable baseline (describ ed
in Section 3.1) and altering one asp ect of the setup at a time. For example, in Section 3.3
we measure the p erformance of di˙erent unsup ervised ob jectives while keeping the rest of
our exp erimental pip eline ˝xed. This co ordinate ascent approach might miss second-order
e˙ects (for example, s ome particular unsup ervised ob jective may work b est on a mo del
larger than our b aselin e setting), but p erforming a combinatorial exploration of all of the
factors in our study would b e prohibitively exp ensive. In future work, we exp ect it cou ld b e
fruitful to more thoroughly consid er combinations of the approaches we study.
Our goal is to compare a variety of di˙erent approaches on a diverse set of tas ks while
keeping as many factors ˝xed as p ossible. In order to s atis fy this aim, in s ome cases we do
not exactly replicate existing approaches. For example, enco der-only mo dels like BERT
(Devlin et al., 2018) are design ed to pro duce a single prediction p er in put token or a single
prediction for an entire input sequence. This makes them applicable for classi˝cation or span
prediction tasks but not for generative tasks like translation or ab stractive summarization.
As such, none of the mo del architectures we consider are identical to BERT or consist of an
enco der-only structure. In stead, we test approach es that are similar in example,
we consid er an analogous ob jective to BERT's masked language mo deling ob j ective in
Section 3.3 and we consider a mo del architecture that b ehaves similarly to BERT on text
classi˝cation tasks in Section 3.2.
After outlining our baseline exp erimental setup in the following subs ection, we undertake
an empirical comparison of mo del architectures (Section 3.2), unsup ervis ed ob jectives
(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and
scaling (Section 3.6). At the culmination of this section, we combine insights from our study
with scale to obtain state-of-th e- art results in many tasks we consider (Section 3.7).
10
Exploring the Limits of Transfer Learning
3.1. Basel ine
Our goal for our baseline is to re˛ect typical, mo dern practice. We pre-train a standard
Transformer (describ ed in Section 2.1) using a simple denoising ob jective and then separately
˝ne-tune on each of our downstream tasks. We describ e th e details of this exp erimental
setup in the following subsections.
3.1.1. Mod el
For our mo del, we use a stand ard enco der-deco der Transformer as prop osed by Vaswani et al.
(2017). Wh ile many mo dern approaches to transfer learning for NLP use a Transformer
architecture consisting of only a single stack (e.g. for langu age mo deling (Rad ford et al.,
2018; Dong et al., 2019) or classi˝cation and span prediction (Devlin et al., 2018; Yang et al.,
2019)), we found that using a standard enco der-deco der structure achieved go o d results
on b oth generative and classi˝cation tasks. We explore the p erformance of di˙erent mo del
architectures in Section 3.2.
Our baseline mo d el is designed so that the enco der and deco der are each similar in
size and con˝guration to a BERT
BASE
 (Devlin et al., 2018) stack. Sp eci˝cally, b oth the
enco der and deco der consist of
12
blo cks (each blo ck comprising self-attention, optional
enco der-deco der attention, and a feed-forward network). The feed-forward networks in each
blo ck consist of a dense layer with an output d imens ionality of
d

= 3072
followed by a
ReLU nonlinearity and another dens e layer. The key and value matrices of all attention
mechanisms have an inner dimensionality of
d
kv
= 64
and all attention mechanisms have
12
heads. All other sub-layers and emb ed dings have a dimensionality of
d
mo del
= 768
. In total,
this results in a mo del with ab out
220
million p arameters. This is roughly twice the numb er
of parameters of BERT
BASE
since our baseline mo del contains two layer stacks instead of
one. For regularization, we use a drop out probability of
0
:
1
everywhere drop out is applied
in the mo d el.
3.1.2. Training
As describ ed in S ection 2.4, all tasks are formulated as text-to- text tasks. This allows us to
always train using standard maximum likeliho o d, i.e. using teacher f orcing (Williams and
Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (S hazeer and
Stern, 2018). At test time, we us e greedy deco ding (i.e. cho osing the highest-p robability
logit at every timestep).
We pre-train each mo del for
2
19
= 524
;
288
steps on C4 b ef ore ˝ne-tuning. We use a
maximum sequence length of
512
and a batch size of
128
sequences. Whenever p ossible,
we pack mu ltip le sequences into each entry of the batch
10
so that our batches contain
roughly
2
16
= 65
;
536
tokens. In total, this batch size and numb er of steps corresp ond s
to pre-training on
2
35
ˇ
34B
tokens. This is considerably less than BERT (Devlin et al.,
2018), which u sed roughly
137B
tokens, or RoBERTa (Liu et al., 2019c), which us ed roughly
2
:
2T
tokens. Using only
2
35
tokens results in a reasonable computational budget while still
providin g a su˚cient amou nt of pre-training for acceptable p erformance. We consider the
10.
https://www.pydoc.io/pypi/tensor2tensor- 1.5.7/autoapi/data_generators/generator_utils/
index.html#data_generators.generator_utils.pack_examples
11
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
e˙ect of pre-training for more steps in Sections 3.6 and 3.7. Note that
2
35
tokens only covers
a fraction of the entire C4 data set, so we never rep eat any data during pre-training.
During pre-training, we use an inverse square ro ot learning rate sch ed ule:
1

p
max (
n; k
)
where
n
is the current training iteration and
k
is the numb er of warm- up steps (set to
10
4
in all of our exp eriments ). Th is sets a con stant learnin g rate of
0
:
01
for the ˝rst
10
4
steps,
then exp on entially decays the learning rate until p re-training is over. We also exp erimented
with using a triangular learning rate (Howard and Ruder, 2018), which pro duced slightly
b etter results but requires knowing the total numb er of training steps ahead of time. Since
we will b e varying the numb er of training steps in some of our exp eriments, we opt for the
more generic inverse square ro ot schedule.
Our mo dels are ˝ ne-tuned for
2
18
= 262
;
144
steps on all tasks. This value was chosen
as a trade-o˙ b etween the high-resource tasks (i.e. those with large data sets), which
b ene˝t from add ition al ˝ne-tuning, and low-resource tasks (smaller data sets), which over˝t
quickly. Du rin g ˝ne-tuning, we continue using batches with
128
length-
512
sequences (i.e.
2
16
tokens p er batch). We use a constant learning rate of
0
:
001
when ˝ne-tuning. We save
a checkp oint every
5
;
000
steps and rep ort results on the mo del checkp oint corresp onding
to the h igh es t validation p erformance. For mo dels ˝ne-tuned on multiple tasks, we cho ose
the b est checkp oint for each task indep endently. For all of the exp eriments except those in
Section 3.7, we rep ort resu lts in the validation set to avoid p erforming mo del selection on
the test set.
3.1.3. Vocabulary
We use SentencePiece (Kudo and Richardson, 2018) to enco de text as WordPiece tokens
(Sennrich et al., 2015; Kudo, 2018). For all exp eriments, we use a vo cabulary of
32
;
000
wordpieces. Since we ultimately ˝ne-tune our mo del on English to German, French, and
Romanian translation, we also require that ou r vo cabulary covers these non-English languages.
To address this, we classi˝ed pages from the Common Crawl scrap e us ed in C4 as German,
French, and Romanian. Then, we trained our SentencePiece mo del on a mixture of
10
parts
of English C4 data with
1
part each of d ata classi˝ed as German, French or Romanian.
This vo cabulary was shared across b oth the input and output of ou r mo del. Note that
our vo cabulary makes it s o th at ou r mo del can only pro cess a predetermined, ˝xed set of
languages.
3.1.4. Unsupervised Objective
Leveraging unlab eled data to pre-train our mo del necessitates an ob j ective that do es not
require lab els but (lo osely sp eakin g) teaches the mo del generalizable knowledge that will b e
useful in downstream tasks. Preliminary work that applied the transfer learning paradigm
of pre-training and ˝ ne-tuning all of the mo del's p arameters to NLP problems used a
causal language mo deling ob jective for pre-training (Dai and Le, 2015; Peters et al., 2018;
Radford et al., 2018; Howard an d Ruder, 2018). However, it has recently b een shown that
denoising ob jectives (Devlin et al., 2018; Taylor, 1953) (also called masked language
mo deling) pro du ce b etter p erformance and as a result th ey have quickly b ecome standard.
In a denoising ob jective, the mo del is trained to predict missing or otherwise corrup ted
tokens in the input. Inspired by BERT's masked language mo deling ob jective and the
12
Exploring the Limits of Transfer Learning
Figure 2:
Schematic of the ob jective we use in our baseline mo del. In this example, we
pro cess the sentence Thank you for inviting me to your party last week. The
words for, invitin g and last (marked with an

) are randomly chosen for
corruption. Each consecutive span of corrupted tokens is rep laced by a sentin el
token (shown as
<X>
and
<Y>
) that is unique over the example. Since for and
inviting o ccur consecutively, they are replaced by a s ingle s entinel
<X>
. The
output sequence then consists of the dropp ed-out spans, delimited by the sentinel
tokens used to replace them in the input plus a ˝nal sentinel token
<Z>
.
word drop out regularization technique (Bowman et al., 2015), we design an ob jective that
randomly samples and then drops out
15%
of tokens in the input sequence. All consecutive
spans of dropp ed-out tokens are replaced by a single sentinel token. Each sentinel token
is assigned a token ID that is unique to the sequence. The sentinel IDs are sp ecial tokens
which are added to our vo cabulary and do not corresp ond to any wordpiece. The target
then corresp onds to all of the dropp ed-out spans of tokens, delimited by the same sentinel
tokens used in the input sequence plus a ˝ nal sentinel token to mark the end of the target
sequence. Our choices to mask consecutive spans of tokens and only predict dropp ed-out
tokens were made to reduce the computational cost of pre-training. We p erform thorough
investigation into pre-training ob jectives in Section 3.3. An example of the transformation
resulting from applying this ob jective is shown in Figure 2. We empirically compare this
ob jective to many other variants in Section 3.3.
3.1.5. Baseli ne Performance
In this section, we present results using the baseline exp erimental pro ced ure describ ed ab ove
to get a sense of what kind of p erformance to exp ect on our s uite of downstream tasks.
Ideally, we would rep eat every exp eriment in our s tudy multiple times to get a con˝ dence
interval on our results. Un fortunately, th is would b e prohibitively exp ensive due to the large
numb er of exp eriments we run. As a cheap er alternative, we train our b aselin e mo del
10
times from scratch (i.e. with di˙erent random initializations and data set shu˜ing) and
assume that the variance over these runs of the base mo del also applies to each exp erimental
variant. We don't exp ect most of the changes we make to have a dramatic e˙ect on the
inter-run variance, so th is should provide a reasonable ind ication of the signi˝cance of
di˙erent changes. S ep arately, we also measure the p erformance of training our mo del for
2
18
steps (the same numb er we use for ˝ne-tuning) on all downstream tasks without pre-training.
This gives us an idea of h ow much pre-training b ene˝ts our mo del in the baseline setting.
13
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Baseline average
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
Baseline standard deviation
0
:
235 0
:
065 0
:
343 0
:
416 0
:
112 0
:
090 0
:
108
No pre-training
66
:
22 17
:
60 50
:
31 53
:
04 25
:
86
39
:
77
24
:
04
Table 1:
Average and standard deviation of scores achieved by ou r baseline mo del and
training pro cedure. For comp arison , we also rep ort p erformance when training on
each task from scratch (i.e. withou t any pre-training) for the same nu mb er of steps
used to ˝ne-tune the bas eline mo del. All scores in this table (and every table in
our pap er except Table 14) are rep orted on the validation sets of each data s et.
When rep orting results in the main text, we only rep ort a subset of the scores across all
the b enchmarks to conserve space and ease interpretation. For GLUE and Sup erGLUE, we
rep ort the average score across all subtasks (as stipu lated by the o˚cial b enchmarks) under
the headings GLUE and SGLUE. For all translation tasks, we rep ort the BLEU score
(Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with exp smo othing
and intl tokenization. We refer to scores f or WMT English to German, English to French,
and English to Romanian as En De, EnFr, and EnRo, resp ectively. For CNN/Daily Mail,
we ˝nd the p erformance of mo dels on the ROUGE-1- F, ROUGE-2-F, and ROUGE-L-F
metrics (Lin, 2004) to b e highly correlated so we rep ort the ROUGE-2-F score alone under
the heading CNNDM. Similarly, for SQuAD we ˝nd the p erformance of the exact match
and F1 s cores to b e highly correlated so we rep ort the exact match score alone. We
provide every score achieved on every task for all exp eriments in Table 16, App en dix E.
Our results tables are all formatted so that each row corresp onds to a particular exp eri-
mental con˝guration with columns giving the scores for each b enchmark. We will include
the mean p erformance of the baseline con˝guration in most tables. Wherever a baseline
con˝guration app ears, we will mark it with a
F
(as in the ˝rst row of Table 1). We also
will
b oldface
any score that is within two standard d eviations of the maximum (b est) in a
given exp eriment.
Our baseline results are shown in Table 1. Overall, our results are comparable to existing
mo dels of similar size. For example, BERT
BASE
achieved an exact match score of
80
:
8
on SQuAD and an accuracy of
84
:
4
on MNLI-matched, whereas we achieve
80
:
88
and
84
:
24
, resp ectively (see Table 16). Note that we cannot directly compare our baseline to
BERT
BASE
b ecause ours is an enco der-deco der mo del and was pre-trained for roughly
1
/
4
as many steps. Unsurprisingly, we ˝nd that pre-training provides signi˝cant gains across
almost all b enchmarks. The only exception is WMT English to French , which is a large
enough data set that gains from pre-training tend to b e marginal. We include this task in
our exp eriments to test the b ehavior of transfer learning in the high-resource regime. Since
we p erform early stopping by selecting the b est-p erforming checkp oint, the large disparity
b etween ou r baseline and no pre-training emphas ize how mu ch pre-training improves
p erformance on tasks with limited data. While we do not explicitly measure improvements
in data e˚ciency in this p ap er, we emphasize that this is one of the primary b ene˝ts of the
transfer learning paradigm.
14
Exploring the Limits of Transfer Learning
Figure 3:
Matrices representing di˙erent attention mask patterns. The input and output
of the self-attention mechanism are denoted
x
and
y
resp ectively. A dark cell
at row
i
and column
j
indicates that the s elf -attention mechan is m is allowed to
attend to in put element
j
at output timestep
i
. A light cell indicates that the
self-attention mechanism is
not
allowed to attend to the corresp onding
i
and
j
combination. Left: A fully-visible mask allows the self-attention mech anism to
attend to th e full input at every output times tep . Middle: A causal mask prevents
the
i
th output element from dep ending on any input elements from the future.
Right: Causal masking with a p re˝ x allows the self-attention mechanism to use
fully-visible masking on a p ortion of the input sequence.
As for inter-run variance, we ˝n d that for most tasks the standard d eviation across runs
is smaller than
1%
of the tas k's baseline score. Exceptions to this rule include CoLA, CB,
and COPA, which are all low-resource tasks from the GLUE and Sup erGLUE b enchmarks.
For example, on CB our baseline mo del had an average F1 score of
91
:
22
with a standard
deviation of
3
:
237
(see Tab le 16), which may b e partly due to the fact that CB's validation
set contains only
56
examples. Note that the GLUE and Sup erGLUE scores are computed
as the average of scores across the tasks comprising each b enchmark. As a result, we caution
that the h igh inter-run varian ce of CoLA, CB, and COPA can make it h arder to compare
mo dels using the GLUE and Sup erGLUE scores alone.
3.2. Archi tectures
While the Transformer was originally intro duced with an enco der-deco der architecture, much
mo dern work on transfer learning for NLP uses alternative architectures. In this section, we
review and compare these architectural variants.
3.2.1. Mod el Structures
A ma jor d is tinguishing factor for di˙erent architectures is the mask used by di˙erent
attention mechanisms in the mo del. Recall that the self-attention op eration in a Transformer
takes a sequence as in put and outputs a new sequence of th e same length. Each entry of
the output sequence is p ro duced by computing a weighted average of entries of the input
sequence. Sp eci˝cally, let
y
i
refer to the
i
th element of the output sequence and
x
j
refer to
15
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Figure 4:
Schematics of th e Transformer architecture variants we consider. In this diagram,
blo cks represent elements of a sequence and lines represent attention visibility.
Di˙erent colored groups of blo cks indicate di˙erent Transformer layer stacks. Dark
grey lines corresp ond to fu lly- vis ib le masking and light grey lines corresp ond
to causal masking. We use . to denote a sp ecial end-of-sequence token that
represents the end of a prediction. The input and output sequences are represented
as
x
and
y
resp ectively. Left: A s tandard en co der-deco der architecture uses fully-
visible masking in the en co der and the enco der-deco der attention, with causal
masking in the deco der. Middle: A language mo del consists of a single Transformer
layer stack and is fed the concatenation of the input and target, using a causal
mask through ou t. Right: Adding a pre˝x to a language mo del corresp on ds to
allowing fully-visible masking over the input.
the
j
th entry of the input sequence.
y
i
is computed as
P
j
w
i;j
x
j
, where
w
i;j
is the scalar
weight pro duced by the self-attention mechanism as a function of
x
i
and
x
j
. The attention
mask is then used to zero out certain weights in order to constrain which entries of the input
can b e attended to at a given output timestep. Diagrams of the masks we will consider are
shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any
w
i;j
to zero if
j > i
.
The ˝rst mo del structure we consider is an an enco der-deco d er Transformer, which
consists of two layer s tacks: The enco der, which is fed an input sequence, and the deco der,
which pro d uces a new output sequence. A schematic of this architectu ral variant is shown
in the left panel of Figure 4.
The enco der uses a fully-visible attention mask. Fully-visible maskin g allows a self-
attention mechan ism to attend to any entry of the input when pro ducing each entry of
its outp ut. We visualize this masking pattern in Figure 3, left. This form of masking is
appropriate when attending over a pre˝x, i.e. some context provided to the mo del that
is later us ed when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible
masking pattern and app en ds a sp ecial classi˝cation token to the input. BERT's output
at the timestep corresp onding to the classi˝cation token is then used to make a prediction
for classifying the input sequence.
16
Exploring the Limits of Transfer Learning
The self-attention op erations in the Transformer's deco der use a causal maskin g pattern.
When pro ducing the
i
th entry of the output s equ en ce, causal masking prevents the mo del
from attending to the
j
th entry of the input sequence for
j > i
. This is used during training
so that the mo del can't see into the future as it pro duces its output. An attention matrix
for this masking pattern is shown in Figure 3, mid dle.
The deco der in an enco der-deco der Tran sformer is used to autoregressively pro duce an
output sequence. That is, at each output timestep, a token is sampled from the mo del's
predicted dis trib ution and the sample is fed back into the mo del to pro duce a prediction for
the next output timestep, and so on. As su ch , a Tran sformer deco der (without an enco der)
can b e used as a language mo del (LM), i.e. a mo del trained solely for next-step prediction
(Liu et al., 2018; Radf ord et al., 2018; Al-Rfou et al., 2019). This constitutes the second
mo del structure we consider. A schematic of this architecture is shown in Figure 4, middle.
In fact, early work on transfer learning for NLP used this architecture with a language
mo deling ob jective as a pre-training metho d (Radford et al., 2018).
Language mo dels are typically used for comp ress ion or sequence generation (Graves,
2013). However, they can also b e u sed in the text-to-text framework simply by concatenating
the inputs and targets. As an example, con sider the case of English to German translation:
If we have a training datap oint with input sentence That is go o d. and target Das ist
gut., we would simply train the mo del on next-step prediction over the concatenated inp ut
sequence translate English to German: That is go o d. target: Das ist gut. If we wanted to
obtain the mo del's prediction for this example, the mo del would b e fed the pre˝x tran slate
English to German: That is go o d. target: and would b e asked to generate the remainder
of the sequence autoregressively. In this way, the mo d el can predict an output s equ en ce
given an input, which satis˝es the needs of text-to-text tasks. This approach was recently
used to show that language mo dels can learn to p erform some text-to-text tasks without
sup ervision (Radford et al., 2019).
A f undamental and frequently cited drawback of using a language mo del in the text-
to-text setting is that causal masking forces the mo del's representation of the
i
th entry of
the input sequence to only dep end on the entries up until
i
. To see why this is p otentially
disadvantageous, consider the text-to-text framework where the mo del is provided with a
pre˝x/context b efore b eing asked to make predictions (e.g., the pre˝x is an English sentence
and the mo del is asked to predict the German translation). With fully ca usal masking, the
mo d el's representation of a pre˝x state can only dep end on prior entries of the pre˝x. So,
when predicting an entry of the output, the mo del will attend to a representation of the
pre˝x that is unnecessarily limited. Similar arguments have b een mad e against using a
unidirectional recurrent neural network enco der in sequence-to-sequence mo dels (Bahd an au
et al., 2015).
This issue can b e avoided in a Transf ormer-bas ed language mo del simply by chan gin g
the masking pattern . Instead of using a causal mask, we use fully-visible masking during
the pre˝x p ortion of the sequence. This masking p attern and a sch ematic of the resulting
pre˝x LM (the third mo del structure we consider) are illustrated in the rightmost panels of
Figures 3 and 4, resp ectively. In the English to German translation example mentioned ab ove,
fully-visible masking would b e applied to the pre˝x translate English to German: That is
go o d. target: and causal masking would b e used during training for predicting the target
Das ist gut. Using a pre˝x LM in the text-to-text framework was originally prop osed by
17
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is e˙ective
on a wide variety of text-to-text tasks. This architecture is similar to an enco der-deco der
mo del with parameters shared across the enco der and deco der and with the enco der-deco der
attention rep laced with full attention across th e input and target sequence.
We note that when following our text-to-text framework, the p re˝x LM architecture
closely resembles BERT (Devlin et al., 2018) f or classi˝cation tasks. To see why, consider an
example from the MNLI b enchmark where the premise is I hate pigeons., the hyp othesis is
My feelin gs towards pigeons are ˝lled with animosity. and the correct lab el is entailment.
To feed this example into a language mo del, we would transf orm it into the s equ en ce mnli
premise: I hate pigeons. hyp othesis: My feelings towards pigeons are ˝lled with animosity.
target: entailment. In this case, the fully-visible pre˝x would corresp ond to the entire in put
sequence up to the word target:, which can b e s een as b ein g analogous to the classi˝cation
token used in BERT. So, our mo del would have full visibility over the entire in put, and then
would b e tasked with making a class i˝ cation by outp utting the word entailment. It is easy
for the mo del to learn to output one of the valid class lab els given the task pre˝x (mnli in
this case). As such, the main d i˙erence b etween a pre˝x LM and the BERT architecture is
that the classi˝er is simply integrated into the output layer of the Transformer deco der in
the pre˝x LM.
3.2.2. Com paring Different Model Structures
In the interest of exp erimentally comparing these architectural variants, we would like each
mo del we consider to b e equivalent in some meaningful way. We might say that two mo dels
are equivalent if they either have the same numb er of parameters or they require roughly
the same amount of computation to pro cess a given (input-sequence, target-sequence) pair.
Unfortunately, it is not p oss ib le to compare an enco der-deco der mo d el to a lan guage mo del
architecture (comprising a s in gle Transformer stack) according to b oth of these criteria
at the same time. To see why, ˝rst note an enco der-deco der mo del with
L
layers in the
enco der and
L
layers in the deco der has approximately the same numb er of parameters as a
language mo del with
2
L
layers. However, th e same
L
+
L
enco der-deco der mo del will have
approximately the same computational cost as a language mo del with
only
L
layers. Th is
is a consequence of the fact that the
L
layers in the language mo del must b e applied to
both
the input and output s equ en ce, while the enco der is only applied to the input sequence
and the deco der is only applied to th e output sequence. Note that these equivalences are
appro are some extra parameters in the deco der due to the enco der-deco der
attention and there are also some computational costs in the attention layers that are
quadratic in the sequence lengths. In practice, however, we observed nearly id entical step
times for
L
-layer language mo dels versus
L
+
L
-layer enco der-deco der mo dels, suggesting a
roughly equ ivalent computational cost. Further, for the mo del sizes we consider, the numb er
of parameters in the enco der-deco der attention layers is ab out 10% of the total parameter
count, so we make the simplifying assumption th at an
L
+
L
-layer enco der-deco der mo del
has the same numb er of parameters as an
2
L
-layer language mo del.
To provide a reasonable means of comparison, we consider mu ltip le con˝ gurations for
our enco der-deco der mo del. We will refer to th e numb er of layers and parameters in a
BERT
BASE
-sized layer stack as
L
and
P
, resp ectively. We will use
M
to refer to the numb er
18
Exploring the Limits of Transfer Learning
of FLOPs required for an
L
+
L
-layer enco der-deco der mo del or
L
-layer deco der-only mo del
to pro cess a given input-target pair. In total, we will compare:
‹
An enco der-deco der mo del with
L
layers in the enco der and
L
layers in the deco der.
This mo del h as
2
P
parameters and a computation cost of
M
FLOPs.
‹
An equ ivalent mo d el, but with p arameters shared across the enco der and deco der,
resulting in
P
parameters and an
M
-FLOP computation al cost.
‹
An enco der-deco d er mo del with
L=
2
layers each in the enco der and deco der, giving
P
parameters and an
M =
2
-FLOP cost.
‹
A deco der- on ly language mo del with
L
layers and
P
parameters and a resulting
computational cost of
M
FLOPs.
‹
A deco d er-on ly pre˝x LM with the same architecture (and thus the same numb er
of parameters and computational cost), b ut with fully-visible self-attention over the
input.
3.2.3. Objectives
As an unsup ervised ob jective, we will consider b oth a basic langu age mo deling ob jective as
well as our baseline denoising ob jective describ ed in Section 3.1.4. We include the language
mo deling ob jective due to its historic use as a pre-training ob jective (Dai and Le, 2015;
Ramachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,
2018) as well as its n atural ˝t for the language mo del architectures we consider. For mo dels
that ingest a pre˝x b efore making predictions (the enco der-deco der mo del and pre˝x LM),
we sample a span of text from our unlab eled data set and cho ose a random p oint to split
it into pre˝x and target p ortions . For the s tandard language mo del, we train the mo del
to predict the entire span from b eginning to end. Our unsup ervised denoising ob jective is
designed for text-to-text mo dels; to adapt it for use with a language mo del we concatenate
the inputs and targets as describ ed in Section 3.2.1.
3.2.4. Results
The scores achieved by each of the architectures we compare are shown in Table 2. For
all tasks, the enco der-deco der arch itecture with the denoising ob jective p erformed b est.
This variant has the highest parameter count (
2
P
) b ut the same computational cost as the
P
-parameter deco der-only mo dels. Surprisingly, we found that sharing parameters across the
enco der an d deco der p erformed nearly as well. In contrast, halving the numb er of layers in
the enco der and deco der stacks signi˝cantly hurt p erformance. Concurrent work (Lan et al.,
2019) also foun d that sharing parameters across Tran sformer blo cks can b e an e˙ective means
of lowering the total parameter count without sacri˝cing much p erformance. XLNet also
b ears some resemblance to the shared enco der-deco der approach with a denoisin g ob j ective
(Yan g et al., 2019). We also note that the shared p arameter enco der-deco der outp erforms
the deco der-only pre˝x LM, suggesting that the addition of an explicit enco d er-d eco der
attention is b ene˝cial. Finally, we con˝rm the widely-held conception that us ing a denoising
ob jective always results in b etter d ownstream task p erformance compared to a language
19
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Architecture Ob jective Params Cost GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Enco der-deco der Denoising
2
P M
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
Enc-dec, shared Denoising
P M
82
:
81 18
:
78
80
:
63 70
:
73
26
:
72 39
:
03
27
:
46
Enc-dec, 6 layers Denoising
P M =
2 80
:
88 18
:
97 77
:
59 68
:
42 26
:
38 38
:
40 26
:
95
Language mo del Denoising
P M
74
:
70 17
:
93 61
:
14 55
:
02 25
:
09 35
:
28 25
:
86
Pre˝x LM Denoising
P M
81
:
82 18
:
61 78
:
94 68
:
11 26
:
43 37
:
98 27
:
39
Enco der-deco der LM
2
P M
79
:
56 18
:
59 76
:
02 64
:
29 26
:
27 39
:
17 26
:
86
Enc-dec, shared LM
P M
79
:
60 18
:
13 76
:
35 63
:
50 26
:
62 39
:
17 27
:
05
Enc-dec, 6 layers LM
P M =
2 78
:
67 18
:
26 75
:
32 64
:
06 26
:
13 38
:
42 26
:
89
Language mo del LM
P M
73
:
78 17
:
54 53
:
81 56
:
51 25
:
23 34
:
31 25
:
38
Pre˝x LM LM
P M
79
:
68 17
:
84 76
:
87 64
:
86 26
:
28 37
:
51 26
:
76
Table 2:
Performance of the di˙erent arch itectural variants describ ed in Section 3.2.2. We
use
P
to refer to the numb er of parameters in a 12-layer base Transformer layer
stack and
M
to refer to the FLOPs required to pro cess a sequence using the enco der-
deco der mo del. We evaluate each architectural variant using a d en oising ob jective
(describ ed in Section 3.1.4) and an autoregressive ob jective (as is commonly used
to train language mo dels).
mo deling ob jective. This observation has b een previously made by Devlin et al. (2018),
Voita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more
detailed exploration of unsup ervised ob jectives in the following section.
3.3. Unsup ervised Ob jectives
The choice of unsup ervised ob jective is of central imp ortance as it provides the mechanism
through which th e mo del gains general-purp ose knowledge to apply to d ownstream tasks.
This has led to the d evelopment of a wide variety of pre-training ob jectives (Dai and Le,
2015; Ramachand ran et al., 2016; Rad ford et al., 2018; Devlin et al., 2018; Yang et al., 2019;
Liu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019).
In this s ection, we p erform a pro ced ural exploration of the space of unsup ervised ob jectives.
In many cases, we will not replicate an existing ob j ective will b e mo di˝ed to
˝t our text-to-text enco der-deco der framework and, in other cases, we will use ob jectives
that combine concepts from multiple common app roaches.
Overall, all of our ob jectives ingest a sequence of token IDs corresp onding to a tokenized
span of text from our unlab eled text data set. The token sequence is pro cessed to p ro duce a
(corrupted) input sequence and a corresp onding target. Th en , the mo del is trained as usual
with maximum likeliho o d to predict the target sequence. We provide illustrative ex amp les
of many of the ob jectives we con sider in Table 3.
3.3.1. Disparate High-Level Approaches
To b egin with, we compare three techniques that are inspired by common ly- used ob jectives
but di˙er signi˝cantly in th eir approach. First, we include a basic pre˝x lan guage mo delin g
ob jective as was used in Section 3.2.3. This technique splits a s pan of text into two
comp onents, one to use as inputs to the enco der and the other to use as a target sequence
20
Exploring the Limits of Transfer Learning
Ob j ective Inputs Targets
Pre˝x language mo deling Thank you for inviting me to your party last week .
BERT-style Devlin et al. (2018) Thank you
<M> <M>
me to your party
 apple
 week .
(original text)
Deshu˜ing party me for your to . last fun you inviting week Thank
(original text)
MASS-style Song et al. (2019) Thank you
<M> <M>
me to your party
<M>
week .
(original text)
I.i.d. noise, replace spans Thank you
<X>
me to your party
<Y>
week .
<X>
for inviting
<Y>
last
<Z>
I.i.d. noise, drop tokens Thank you me to your party week . for inviting last
Random spans Thank you
<X>
to
<Y>
week .
<X>
for inviting me
<Y>
your party last
<Z>
Table 3:
Examples of inputs and targets pro duced by s ome of the unsup ervised ob jectives
we consider applied to the input text Thank you for invitin g me to your party last
week . Note that all of our ob jectives pro cess
tokenized
text. For this particular
sentence, all words were mapp ed to a single token by our vo cab ulary. We write
(original text)
as a target to denote that the mo del is tas ked with reconstructing the
entire in put text.
<M>
denotes a shared mask token and
<X>
,
<Y>
, and
<Z>
denote
sentinel tokens that are as signed unique token IDs. The BERT-style ob jective
(second row) includes a corruption where some tokens are replaced by a random
token ID; we show this via the greyed-out word
 apple
.
Ob jective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
Pre˝x language mo deling
80
:
69 18
:
94 77
:
99 65
:
27
26
:
86
39
:
73
27
:
49
BERT-style (Devlin et al., 2018)
82
:
96 19
:
17 80
:
65 69
:
85 26
:
78 40
:
03 27
:
41
Deshu˜ing
73
:
17 18
:
59 67
:
61 58
:
47 26
:
11 39
:
30 25
:
62
Table 4:
Performance of the three disparate p re-training ob jectives describ ed in Section 3.3.1.
to b e predicted by the deco der. Second, we consider an ob jective inspired by the masked
language mo deling (MLM) ob jective us ed in BERT (Devlin et al., 2018). MLM takes a
span of text and corrupts
15%
of the tokens.
90%
of the corrupted tokens are replaced
with a sp ecial mask token and
10%
are replaced with a random token. Since BE RT is an
enco der-only mo del, its goal du rin g pre-training is to reconstruct masked tokens at the
output of the enco der. In the enco der-deco der case, we simply use the entire u ncorrup ted
sequence as the target. Note that this di˙ers from our b as eline ob jective, which uses only
the corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally,
we also consider a b asic deshu˜ing ob jective as used e.g. in (Liu et al., 2019a) where it was
applied to a d en ois ing sequential auto enco der. This approach takes a sequence of tokens,
shu˜es it, and then uses the original deshu ˜ed sequence as a target. We provide examples
of the inputs and targets for these three metho ds in the ˝rst three rows of Table 3.
The p erformance of these three ob j ectives is shown in Table 4. Overall, we ˝nd that the
BERT-style ob jective p erforms b est, though the pre˝x language mo deling ob jective attains
similar p erformance on the translation tasks. Indeed, the motivation for the BERT ob jective
was to outp erform language mo del-based p re-training. The deshu˜ing ob jective p erforms
considerably worse than b oth p re˝ x language mo deling and the BERT-style ob jective.
21
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Ob jective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
BERT-style (Devlin et al., 2018)
82
:
96 19
:
17
80
:
65
69
:
85 26
:
78
40
:
03
27
:
41
MASS-style (Song et al., 2019)
82
:
32 19
:
16 80
:
10 69
:
28 26
:
79
39
:
89
27
:
55
F
Replace corrupted spans
83
:
28
19
:
24 80
:
88 71
:
36 26
:
98
39
:
82
27
:
65
Drop corrupted tokens
84
:
44 19
:
31 80
:
52
68
:
67
27
:
07
39
:
76
27
:
82
Table 5:
Comparison of variants of the BERT-style pre-training ob jective. In the ˝rst two
variants, the mo del is trained to reconstruct the original uncorrupted text segment.
In the latter two, the mo del only p redicts the sequence of corrupted tokens.
3.3.2. Sim plifying the BERT Objective
Based on the results in the prior section, we will now fo cus on exploring mo di˝cations to
the BERT-style denoising ob jective. This ob jective was originally prop osed as a pre-training
technique for an enco der-only mo del trained for classi˝cation and span prediction. As
such, it may b e p ossible to mo dify it so that it p erforms b etter or is more e˚cient in our
enco der-deco der text-to-text setup.
First, we consider a simple variant of the BERT-style ob jective where we don't include the
random token swapping step. The resulting ob jective simply replaces
15%
of the tokens in
the input with a mask token and the mo del is trained to reconstruct the original uncorrupted
sequence. A similar masking ob jective was used by Song et al. (2019) where it was referred to
as MASS, so we call this variant the MASS-style ob jective. Second, we were interested
to see if it was p ossible to avoid predicting the entire uncorrupted text sp an since this
requires self-attention over long sequences in the deco der. We consider two strategies to
achieve this: First, instead of replacing each corrupted token with a mask token, we replace
the entirety of each consecutive span of corrupted tokens with a unique mask token. Then,
the target sequence b ecomes the concatenation of the corrupted spans, each p re˝ xed by
the mask token used to replace it in the input. This is the pre-training ob jective we use in
our baseline, describ ed in S ection 3.1.4. Secon d, we also consider a variant where we simply
drop the corru pted tokens from the inp ut sequence completely and task the mo del with
reconstructing the dropp ed tokens in order. Examples of these approaches are shown in the
˝fth an d sixth rows of Table 3.
An empirical comparison of the original BERT- style ob jective to these three alternatives
is sh own in Table 5. We ˝nd th at in our setting, all of these variants p erform similarly. The
only exception was that dropping corrupted token s completely pro d uced a small improvement
in the GLUE score than ks to a signi˝cantly higher score on CoLA (
60
:
04
, compared to our
baseline average of
53
:
84
, see Table 16). This may b e due to the fact that CoLA involves
classifying whether a given sentence is grammatically and syntactically acceptable, and
b eing able to determine when tokens are missing is closely related to detecting acceptability.
However, dropping tokens completely p erformed worse than replacing them with sentinel
tokens on Sup erGLUE. The two variants that do not require predicting the full original
sequence (replace corrupted spans and drop corrupted spans) are b oth p otentially
attractive since they make the target sequences shorter and consequently make training
22
Exploring the Limits of Transfer Learning
Corruption rate GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
10%
82
:
82
19
:
00
80
:
38
69
:
55
26
:
87
39
:
28
27
:
44
F
15%
83
:
28
19
:
24
80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
25%
83
:
00 19
:
54 80
:
96
70
:
48
27
:
04 39
:
83 27
:
47
50% 81
:
27 19
:
32 79
:
80 70
:
33
27
:
01 39
:
90 27
:
49
Table 6: Performance of the i.i.d. corruption ob j ective with di˙erent corruption rates.
faster. Going forward, we will explore variants where we replace corrupted spans with
sentinel tokens and only predict the corrup ted tokens (as in our baseline ob jective).
3.3.3. Varying the Corruption Rate
So far, we have b een corrupting 15% of the tokens, the value used in BERT (Devlin et al.,
2018). Again, since our text-to-text framework di˙ers from BERT's, we are interested to
see if a di˙erent corruption rate works b etter for us. We compare corruption rates of
10%
,
15%
,
25%
, an d
50%
in Table 6. Overall, we ˝n d that the corruption rate had a limited
e˙ect on the mo del's p erformance. The only exception is that the largest corrup tion rate we
consider (
50%
) results in a signi˝cant degradation of p erformance on GLUE and SQuAD.
Using a larger corruption rate also results in longer targets, which can p otentially slow down
training. Based on these results and the historical precedent set by BERT, we will use a
corruption rate of
15%
going forward.
3.3.4. Corrupting Spans
We now turn towards the goal of sp eeding up training by predicting shorter targets. The
approach we have used so far makes an i.i.d. decision for each input token as to whether
to corrupt it or not. When multiple consecutive tokens have b een corrupted, th ey are
treated as a span and a single unique mask token is used to replace the entire s pan.
Replacing entire spans with a single token res ults in unlab eled text data b eing pro cessed into
shorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case
that a signi˝cant numb er of corrupted tokens app ear con secu tively. As a result, we might
obtain additional sp eedup by sp eci˝cally corrupting spans of tokens rather than corrup ting
individual tokens in an i.i.d. manner. Corrupting sp ans was also previously considered as a
pre-training ob jective for BERT, where it was found to improve p erformance (Joshi et al.,
2019).
To test this idea, we consider an ob jective that sp eci˝cally corrupts contiguous, randomly-
spaced span s of token s. This ob jective can b e parametrized by the prop ortion of tokens to
b e corrupted and the total numb er of corrupted sp ans. The span lengths are then chosen
randomly to satisfy these sp eci˝ed parameters. For example, if we are pro cessing a sequence
of
500
tokens and we have sp eci˝ed that
15%
of tokens should b e corru pted and that there
should b e
25
total spans, then the total numb er of corrupted tokens would b e
500

0
:
15 = 75
and the average span length would b e
75
=
25 = 3
. Note that given the original sequence
length and corruption rate, we can equivalently parametrize this ob jective either by the
average span length or the total numb er of span s.
23
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Span length GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Baseline (i.i.d.)
83
:
28
19
:
24 80
:
88 71
:
36
26
:
98 39
:
82 27
:
65
2
83
:
54
19
:
39
82
:
09 72
:
20 26
:
76 39
:
99 27
:
63
3
83
:
49 19
:
62 81
:
84 72
:
53 26
:
86
39
:
65
27
:
62
5
83
:
40
19
:
24
82
:
05 72
:
23 26
:
88
39
:
40
27
:
53
10 82
:
85 19
:
33
81
:
84
70
:
44
26
:
79
39
:
49
27
:
69
Table 7:
Performance of the span-corruption ob jective (inspired by Josh i et al. (2019)) f or
di˙erent average span lengths. In all cases, we corrupt 15% of the original text
sequence.
Figure 5:
A ˛ow chart of our exploration of unsup ervised ob jectives. We ˝rst consider a
few disparate approaches in Section 3.3.1 and ˝nd that a BERT-style denoising
ob jective p erforms b est. Then, we consider various metho ds for simplifying the
BERT ob jective so that it pro duces shorter target sequences in Section 3.3.2.
Given that replacing dropp ed-out spans with sentinel t okens p erforms well and
results in short target sequences, in Section 3.3.3 we exp eriment with di˙erent
corruption rates. Finally, we evaluate an ob jective that intentionally corrupts
contiguous spans of tokens in Section 3.3.4.
We compare the span-corruption ob jective to the i.i.d-corruption ob jective in Table 7.
We use a corruption rate of
15%
in all cases and compare using average span lengths of
2
,
3
,
5
and
10
. Again, we ˝nd a limited di˙erence b etween these ob jectives, th ough the version
with an average span length of
10
slightly underp erforms the other values in some cases.
We also ˝nd in particular that using an average span length of
3
slightly (but signi˝cantly)
outp erforms the i.i.d. ob jective on most non-translation b enchmarks. Fortunately, the
span-corruption ob j ective also provides some sp eedup du ring training compared to the i.i.d.
noise approach b ecause span corrup tion pro duces shorter sequences on average.
3.3.5. Discussio n
Figure 5 shows a ˛ow chart of th e choices made during our exploration of unsu p ervised
ob jectives. Overall, the most s igni˝cant di˙erence in p erformance we ob served was that
24
Exploring the Limits of Transfer Learning
denoising ob j ectives outp erformed language mo deling and deshu˜ing for pre-training. We
did not ob serve a remarkable di˙erence across the many variants of the denoising ob jectives
we explored. However, di˙erent ob jectives (or parameterizations of ob jectives) can lead to
di˙erent sequence lengths and thus di˙erent training sp eeds. This implies that cho osing
among the denoising ob jectives we considered here should mainly b e done according to
their comp utational cost. Our results also s uggest that additional exp loration of ob jectives
similar to the ones we consider here may not lead to signi˝cant gain s for the tasks and mo del
we consider. Ins tead , it may b e fortuitous to explore entirely di˙erent ways of leveraging
unlab eled data.
3.4. Pre-trai ning Data set
Like the unsup ervised ob jective, the pre-training data set itself is a crucial comp onent of
the transfer learning pip eline. However, unlike ob jectives and b enchmarks, new pre-training
data sets are usually not treated as s ign i˝cant contributions on their own and are often not
released alongside pre-trained mo dels and co de. Instead, they are typ ically intro duced in
the course of presentin g a new metho d or mo del. As a result, there has b een relatively little
comparison of di˙erent pre-training data sets as well as a lack of a standard data set used
for pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c;
Yang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced)
data set to using a smaller preexisting data set (often Wikip ed ia). To prob e more deeply
into the impact of the pre-training data set on p erformance, in this section we compare
variants of our C4 data set and oth er p otential sources of pre-training data. We release all
of the C4 data set variants we consider as part of TensorFlow Datasets.
11
3.4.1. Unlabel ed Data Sets
In creating C4, we develop ed various heuristics to ˝lter the web-extracted text from Common
Crawl (see Section 2.2 for a description). We are interested in measuring whether this
˝ltering results in improved p erformance on downstream tasks, in addition to comparing
it to other ˝ltering ap proaches and common pre-training data sets. Towards this end, we
compare the p erformance of our baseline mo del after pre-training on the following data sets:
C4
As a baseline, we ˝rst consider pre-training on our prop osed unlab eled data set as
describ ed in Section 2.2.
Un˝ltered C4
To measure the e˙ect of the heu ris tic ˝ltering we used in creating C4
(deduplication, removing bad words, only retaining sentences, etc.), we also generate
an alternate version of C4 that forgo es this ˝ltering. Note that we still use
langdetect
to extract English text. As a result, our un˝ltered variant still includes s ome ˝ltering
b ecause
langdetect
sometimes assigns a low probab ility to non-natural English text.
RealNews-like
Recent work has used text data extracted from news websites (Zellers
et al., 2019; Baevski et al., 2019). To compare to this approach, we generate another
unlab eled data set by additionally ˝ltering C4 to only include content from one of the
domains used in the RealNews data set (Zellers et al., 2019). Note that for ease of
11.
https://www.tensorflow.org/datasets/catalog/c4
25
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Data set Size GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
C4 745GB
83
:
28
19
:
24
80
:
88 71
:
36
26
:
98 39
:
82 27
:
65
C4, un˝ltered 6.1TB
81
:
46 19
:
14 78
:
78 68
:
04 26
:
55 39
:
34 27
:
21
RealNews-like 35GB
83
:
83 19
:
23
80
:
39 72
:
38
26
:
75 39
:
90 27
:
48
WebText-like 17GB
84
:
03 19
:
31 81
:
42
71
:
40
26
:
80 39
:
74 27
:
59
Wikip edia 16GB
81
:
85
19
:
31
81
:
29 68
:
01
26
:
94
39
:
69
27
:
67
Wikip edia + TBC 20GB
83
:
65
19
:
28 82
:
08 73
:
24 26
:
77
39
:
63
27
:
57
Table 8:
Performance resu ltin g from pre-training on di˙erent data sets. The ˝rst four
variants are based on our n ew C4 data set.
comparison, we retain the heuristic ˝ltering metho ds used in C4; the only di˙erence is
that we have ostensibly omitted any non-news content.
WebText-like
Similarly, the WebText data set (Radford et al., 2019) only uses content
from webpages that were submitted to the content aggregation website Red dit and
received a score of at least 3. The score for a webpage submitted to Reddit is
computed based on the prop ortion of users who endorse (upvote) or opp ose (downvote)
the webpage. Th e idea b ehind us ing the Reddit s core as a quality signal is that users
of th e site would on ly upvote high-quality text content. To generate a comparable data
set, we ˝rst tried removing all content from C4 that did not originate from a URL that
app eared in the list prepared by the Op enWebText e˙ort.
12
However, this resulted in
comparatively little conten ab out 2 ecause most pages never app ear on
Reddit. Recall that C4 was created based on a single month of Common Crawl data.
To avoid using a prohibitively small data set, we therefore downloaded 12 months
of data from Common Crawl from August 2018 to July 2019, applied our heuristic
˝ltering for C4, then applied the Reddit ˝lter. This pro duced a 17 GB WebText-like
data set, wh ich is of comparable size to the original 40GB WebText data set (Radford
et al., 2019).
Wikip edia
The website Wikip edia consists of millions of encyclop edia articles written
collab oratively. The content on the site is sub ject to strict quality guid elines and
therefore has b een used as a reliable source of clean and natural text. We use the
English Wikip edia text data from TensorFlow Datasets,
13
which omits any markup or
reference sections f rom the articles.
Wikip edia + Toronto Bo oks Corpus
A drawback of using pre-training data from Wikip edia
is that it represents only one p ossible domain of natural text (encyclop edia articles).
To mitigate this, BERT (Devlin et al., 2018) combined data from Wikip edia with the
Toronto Bo oks Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from
eBo oks, which represents a di˙erent domain of natural language. BERT's p opularity
has led to the Wikip edia + TBC combination b ein g used in many subsequent works.
12.
https://github.com/jcpeterson/openwebtext
13.
https://www.tensorflow.org/datasets/catalog/wikipedia
26
Exploring the Limits of Transfer Learning
The results achieved after pre-training on each of th es e data sets is shown in Table 8. A
˝rst obvious takeaway is that removing the heuristic ˝ltering from C4 uniformly degrades
p erformance and makes the un˝ltered variant p erform the worst in every task. Beyond
this, we found that in some cases a pre-training data set with a more constrained domain
outp erformed the diverse C4 data set. For example, using the Wikip edia + TBC corpus
pro duced a Sup erGLUE score of
73
:
24
, b eating our baseline's score (using C4) of
71
:
36
.
This is almost entirely attributable to a b o ost in p erformance from
25
:
78
(baseline, C4) to
50
:
93
(Wikip edia + TBC) on th e Exact Match score for MultiRC (see Table 16). MultiRC
is a reading comprehension data set whose largest s ource of data comes from ˝ction b o oks,
which is exactly the domain covered by TBC. Similarly, using the RealNews-like data set
for pre-training conferred an increase from
68
:
16
to
73
:
72
on the Exact Match score for
ReCoRD, a data set that measures reading comp rehens ion on news articles. As a ˝nal
example, using data from Wikip edia pro duced signi˝cant (but less dramatic) gains on
SQuAD, which is a question-answering data set with passages sourced from Wikip edia.
Similar observations have b een made in prior work, e.g. Beltagy et al. (2019) found that
pre-training BERT on text from research pap ers improved its p erf orman ce on s cienti˝c tasks.
The main lesson b ehind th es e ˝ndings is that
pre-training on in-domain unlabeled data can
improve performance on downstream tasks
. This is unsurprising but also unsatisfyin g if
our goal is to pre-train a mo del that can rapid ly adapt to language tasks from arbitrary
domains. Liu et al. (2019c) als o observed that pre-training on a more diverse d at a set yielded
improvements on downstream tasks. This observation also motivates the parallel line of
research on domain adaptation for natural language pro cess in g; for surveys of this ˝eld see
e.g. Ru der (2019); Li (2012).
A drawback to only pre-training on a single d omain is that the resulting data sets are
often substantially smaller. Similarly, while the WebText-like variant p erformed as well or
b etter than the C4 data set in our baseline setting, the Reddit-based ˝ltering pro duced a
data set that was ab out
40

smaller than C4 despite b ein g based on
12

more data from
Common Crawl. Note, however, that in our baseline setup we only pre-train on
2
35
ˇ
34B
tokens, which is only ab out
8
times larger than the smallest pre-training data set we cons id er.
We investigate at what p oint using a s maller pre-training data sets p oses an issue in the
following section.
3.4.2. Pre-training Data set Size
The pip eline we us e to create C4 was designed to b e able to create extremely large pre-
training data sets. The access to so much data allows us to pre-train our mo dels without
rep eating examples. It is not clear whether rep eating examples during pre-training would
b e helpful or harmful to downstream p erformance b ecaus e our pre-train ing ob j ective is itself
sto chastic and can help prevent the mo del from seeing the same exact data multiple times.
To tes t the e˙ect of limited unlab eled data set sizes, we pre-trained our baseline mo del
on arti˝cially truncated versions of C4. Recall that we pre-train our baseline mo del on
2
35
ˇ
34B
tokens (a small fraction of the total s ize of C4). We consider training on truncated
variants of C4 consisting of
2
29
,
2
27
,
2
25
and
2
23
tokens. These sizes corresp ond to rep eating
the data set
64
,
256
,
1
;
024
, and
4
;
096
times resp ectively over the course of pre-training.
27
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Numb er of tokens Rep eats GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Full data set
0
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
2
29
64
82
:
87 19
:
19 80
:
97 72
:
03 26
:
83 39
:
74 27
:
63
2
27
256 82
:
62
19
:
20
79
:
78 69
:
97
27
:
02 39
:
71
27
:
33
2
25
1
;
024 79
:
55 18
:
57 76
:
27 64
:
76 26
:
38 39
:
56 26
:
80
2
23
4
;
096 76
:
34 18
:
33 70
:
92 59
:
29 26
:
37 38
:
84 25
:
81
Table 9:
Measuring the e˙ect of rep eating data during pre-training. In these exp eriments,
we only use the ˝rst
N
tokens from C4 (with varying values of
N
shown in th e
˝rst column) but still pre- train over
2
35
tokens. This res ults in the data set b eing
rep eated over the course of pre-training (with the numb er of rep eats for each
exp eriment shown in the second column), which may resu lt in memorization (see
Figure 6).
The resulting d ownstream p erformance is shown in Table 9. As exp ected, p erformance
degrades as the data set size shrinks. We susp ect this may b e due to the fact that the mo del
b egins to memorize the pre-training data set. To measure if this is true, we plot the training
loss for each of these data s et sizes in Figure 6. Indeed, the mo del attains signi˝cantly
smaller training losses as the size of the pre-training data set shrinks, suggesting p ossible
memorization. Baevski et al. (2019) similarly observed that truncating the pre-training data
set size can degrade down stream task p erformance.
We n ote that these e˙ects are limited wh en the pre-training data set is rep eated only
64
times. This suggests that some amount of rep etition of p re-training data might not b e
harmful. However, given that additional pre-training can b e b ene˝cial (as we will show in
Section 3.6) an d that obtaining additional unlab eled data is cheap and easy, we suggest
using large pre-training data sets whenever p ossible. We also note that this e˙ect may b e
more pronoun ced for larger mo del sizes , i.e. a bigger mo del may b e more prone to over˝tting
to a smaller pre-training data set.
3.5. Training Strategy
So far we have considered the setting wh ere all parameters of a mo del are pre-trained on
an unsup ervised task b efore b eing ˝ne-tuned on individual sup ervised tasks. While this
approach is straightforward, various alternative metho ds for training the mo del on down-
stream/sup ervised tasks have b een prop osed. In this section, we compare di˙erent s chemes
for ˝ne-tuning the mo del in addition to the ap proach of training the mo del simultan eou sly
on multiple tasks.
3.5.1. Fine-tuning Methods
It has b een argued that ˝ne-tuning all of the mo del's parameters can lead to sub optimal
results, p articularly on low-resource tasks (Peters et al., 2019). Early results on transf er
learning for text classi˝cation tasks advo cated ˝ne-tuning only the parameters of a small
classi˝er that was fed sentence emb eddings pro d uced by a ˝xed pre-trained mo del (Subra-
manian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau
28
Exploring the Limits of Transfer Learning
Figure 6:
Pre-training loss for our original C4 data set as well as
4
arti˝cially truncated
versions. The sizes listed refer to the numb er of tokens in each data set. The four
sizes considered corresp ond to rep eating the data set b etween
64
and
4
;
096
times
over the course of pre-training. Using a smaller data s et size results in smaller
training loss values, which may suggest some memorization of the unlab eled data
set.
et al., 2017). This approach is less applicable to our enco der-deco der mo del b ecause the
entire deco der must b e trained to output the target sequences for a given task. Instead, we
fo cus on two alternative ˝ne-tuning app roaches that up date only a subset of the parameters
of our enco d er-deco der mo del.
The ˝rst, ad apter layers (Houlsby et al., 2019; Bapna et al., 2019), is motivated by
the goal of keeping most of the original mo del ˝xed while ˝ne-tuning. Ad apter layers are
additional dense-ReLU-dense blo cks that are added after each of the preexisting feed-forward
networks in each blo ck of the Transformer. These n ew feed-forward networks are designed
so that their output dimensionality matches their in put. This allows them to b e inserted
into the network with no ad ditional changes to the structure or parameters. When ˝ne-
tuning, only the adapter layer and layer normalization parameters are up dated. The main
hyp erparameter of this approach is the inner dimens ion ality
d
of the feed-forward network,
which changes the numb er of new parameters added to the mo del. We exp eriment with
various values for
d
.
The second alternative ˝ne-tuning metho d we consider is gradual unfreezing (Howard
and Ruder, 2018). In gradual unfreezing, more and more of the mo del's parameters are ˝ne-
tuned over time. Gradual unfreezing was originally applied to a language mo del architecture
consisting of a single stack of layers . In this setting, at the start of ˝ne-tuning only the
parameters of the ˝nal layer are up dated, then after training for a certain numb er of up dates
the parameters of the s econd- to-last layer are also included, and so on until the entire
network's parameters are b eing ˝ne-tuned. To adapt this approach to our enco der-deco der
mo del, we gradu ally unf reeze layers in the enco der and deco der in parallel, starting from
the top in b oth cases. Since the parameters of our input emb edding matrix and output
classi˝cation matrix are shared, we up date them throughout ˝ne-tuning. Recall that our
baseline mo del consists of
12
layers each in the enco der and deco der and is ˝ne-tuned for
29
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Fine-tuning m etho d GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
All param eters
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
Adapter layers,
d
= 32 80
:
52 15
:
08 79
:
32 60
:
40 13
:
84 17
:
88 15
:
54
Adapter layers,
d
= 128 81
:
51 16
:
62 79
:
47 63
:
03 19
:
83 27
:
50 22
:
63
Adapter layers,
d
= 512 81
:
54 17
:
78 79
:
18 64
:
30 23
:
45 33
:
98 25
:
81
Adapter layers,
d
= 2048 81
:
51 16
:
62 79
:
47 63
:
03 19
:
83 27
:
50 22
:
63
Gradual unfreezing
82
:
50 18
:
95 79
:
17
70
:
79
26
:
71 39
:
02 26
:
93
Table 10:
Comparison of di˙erent alternative ˝ne-tuning metho ds that only up date a subset
of the mo del's parameters. For adapter layers,
d
refers to the inner dimensionality
of the adapters.
2
18
steps. As such, we sub divide the ˝ne-tuning pro cess into
12
episo des of
2
18
=
12
steps each
and train from layers
12

n
to
12
in the
n
th episo de. We note that Howard and Ruder
(2018) suggested ˝ne-tunin g an additional layer after each ep o ch of training. However, since
our sup ervised data sets vary so much in size and since some of our d ownstream tasks are
actually mixtures of many tasks (GLUE and Sup erGLUE), we instead adopt the simpler
strategy of ˝ne-tuning an additional layer after every
2
18
=
12
steps.
A comparison of the p erformance of these ˝ne-tuning approaches is sh own in Table 10.
For adapter layers, we rep ort the p erformance using an inner dimensionality
d
of
32
,
128
,
512
,
2048
. Pu rs uant with past results (Houlsby et al., 2019; Bapn a et al., 2019) we ˝ nd that
lower-resource tasks like SQuAD work well with a small value of
d
whereas higher resource
tasks require a large dimen sionality to achieve reasonable p erformance. This suggests that
adapter layers could b e a promising technique for ˝n e- tuning on fewer parameters as long as
the dimensionality is scaled appropriately to the task size. Note that in our cas e we treat
GLUE and Sup erGLUE each as a single task by concatenating their constituent data
sets, so althou gh they comprise some low-resource d ata sets the comb in ed data set is large
enough that it necessitates a large value of
d
. We found that gradual unf reezing caused
a minor degradation in p erformance across all tasks, though it did provide some sp eedup
during ˝ ne- tuning. Better results may b e attainable by more carefully tuning the unf reezing
schedule.
3.5.2. Multi-task Learning
So far, we have b een pre-training our mo del on a single unsup ervised learnin g task b efore
˝ne-tuning it individu ally on each d ownstream task. An alternative approach, called multi-
task learning (Ruder, 2017; Caruan a, 1997), is to train th e mo del on multiple tasks at a
time. This approach typically has the goal of training a single mo del that can s imultaneously
p erform many tas ks at once, i.e. the mo del and most of its parameters are shared across all
tasks. We relax this goal somewhat and instead investigate metho ds for training on multiple
tasks at once in order to eventually pro duce separate parameter settings that p erform well
on each individual task. For example, we might train a single mo del on many tasks, but
when rep orting p erformance we are allowed to select a di˙erent checkp oint for each task.
This lo osens the multi-task learning framework and puts it on more even fo oting compared
to the pre-train-then-˝ne-tune approach we have consid ered so far. We als o note that in our
30
Exploring the Limits of Transfer Learning
uni˝ed text-to-text framework, multi-task learning simply corresp onds to mixing data sets
together. It follows that we can still train on unlab eled data when using multi-task learning
by treating the unsu p ervised task as one of th e tasks b eing mixed together. In contrast,
most applications of multi-task learning to NLP add task-sp eci˝c class i˝cation networks or
use di˙erent loss functions for each task (Liu et al., 2019b).
As p ointed out by Arivazhagan et al. (2019), an extremely imp ortant factor in multi-task
learning is how much data from each task the mo del should b e trained on. Our goal is to n ot
under- or over-train the mo is, we want the mo del to see enough data from a given
task that it can p erform the tas k well, but not to see so much data that it memorizes the
training set. How exactly to s et the prop ortion of data coming from each task can dep end on
various factors including data set sizes, the di˚culty of learning the task (i.e. how much
data the mo del must see b efore b eing able to p erform the task e˙ectively), regularization,
etc. An additional issue is the p otential f or task interference or negative transfer, where
ach ieving go o d p erformance on one task can hinder p erformance on another. Given these
concerns, we b egin by exploring various strategies for setting the prop ortion of data coming
from each task. A similar exploration was p erformed by Wang et al. (2019a).
Examples-prop ortional mixing
A ma j or factor in how quickly a mo del will over˝t to
a given task is the task's data set size. As such, a natural way to s et the mixing
prop ortions is to sample in prop ortion to the size of each task's data set. This is
equivalent to concatenating the data sets for all tasks and rand omly sampling examples
from the combined data set. Note, h owever, that we are including our unsu p ervised
denoising tas k, which uses a data set that is orders of magnitude larger than every
other task's. It follows that if we simply sample in prop ortion to each data set's size,
the vas t ma jority of the data the mo del sees will b e unlab eled, and it will un dertrain
on all of the sup ervised tasks. Even without the unsup ervised task, some tasks (e.g.
WMT English to French ) are so large that they would similarly crowd out most of
the batches. To get around this issue, we set an arti˝cial limit on the data set sizes
b efore computing the prop ortions. Sp eci˝ cally, if the numb er of examples in each of
our
N
task's data sets is
e
n
; n
2 f
1
; : : : ; N
g
then we set probability of sampling an
example from the
m
th task du rin g training to
r
m
=
min
(
e
m
; K
)
=
P
min
(
e
n
; K
)
where
K
is the arti˝cial data set size limit.
Temp erature-scaled mixing
An alternative way of mitigating the huge disparity b etween
data set sizes is to adjust the temp erature of the mixing rates. This approach was
used by multilingual BERT to ensure that the mo del was su˚ciently trained on low-
resource languages.
14
To implement temp erature scaling with temp erature
T
, we raise
each task's mixing rate
r
m
to the p ower of
1
/
T
and renormalize the rates so that they
sum to 1. When
T
= 1
, this approach is equivalent to examples-prop ortional mixin g
and as
T
increases the prop ortions b ecome clos er to equal mixing. We retain the data
set size limit
K
(applied to obtain
r
m
b efore temp erature scaling) but set it to a large
value of
K
= 2
21
. We use a large value of
K
b ecause increasing the temp erature will
decrease the mixin g rate of th e largest data s ets .
14.
https://github.com/google- research/bert/blob/master/multilingual.md
31
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Mixing strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Baseline (pre-train/˝ne-tune)
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
Equal
76
:
13 19
:
02 76
:
51 63
:
37 23
:
89 34
:
31 26
:
78
Examples-prop ortional,
K
= 2
16
80
:
45 19
:
04 77
:
25 69
:
95 24
:
35 34
:
99 27
:
10
Examples-prop ortional,
K
= 2
17
81
:
56 19
:
12 77
:
00 67
:
91 24
:
36 35
:
00 27
:
25
Examples-prop ortional,
K
= 2
18
81
:
67 19
:
07 78
:
17 67
:
94 24
:
57 35
:
19 27
:
39
Examples-prop ortional,
K
= 2
19
81
:
42
19
:
24
79
:
78 67
:
30 25
:
21 36
:
30
27
:
76
Examples-prop ortional,
K
= 2
20
80
:
80
19
:
24 80
:
36
67
:
38 25
:
66 36
:
93
27
:
68
Examples-prop ortional,
K
= 2
21
79
:
83 18
:
79 79
:
50 65
:
10 25
:
82 37
:
22 27
:
13
Temp erature-scaled,
T
= 2 81
:
90
19
:
28
79
:
42 69
:
92 25
:
42 36
:
72 27
:
20
Temp erature-scaled,
T
= 4 80
:
56
19
:
22
77
:
99 69
:
54 25
:
04 35
:
82 27
:
45
Temp erature-scaled,
T
= 8 77
:
21 19
:
10 77
:
14 66
:
07 24
:
55 35
:
35 27
:
17
Table 11:
Comparison of multi-task training using di˙erent mixing strategies. Examples-
prop ortional mixing refers to sampling examples from each data set according to
the total size of each data set, with an arti˝cial limit (
K
) on the maximum data set
size. Temp erature-scaled mixing re-scales the sampling rates by a temp erature
T
.
For temp erature-scaled mixing, we u se an arti˝cial data set size limit of
K
= 2
21
.
Equal mixing
In this case, we sample examples from each task with equal probability.
Sp eci˝cally, each example in each batch is sampled uniformly at random from one of
the data sets we train on. This is most likely a sub optimal strategy, as the mo del will
over˝t quickly on low-resource tasks and under˝t on high-resource tasks. We mainly
include it as a p oint of reference of what might go wrong when the prop ortions are set
sub optimally.
To compare these mixing strategies on equal fo oting with our baseline pre-train-then-
˝ne-tune results, we train mu lti- task mo dels for the same total numb er of steps:
2
19
+ 2
18
=
786
;
432
. The results are shown in Table 11.
In general, we ˝nd that multi-task training underp erforms pre-training followed by
˝ne-tuning on most tasks. The equal mixing strategy in particular results in dramatically
degraded p erformance, which may b e b ecaus e the low-resource tasks have over˝t, the high-
resource tasks have not seen enough data, or the mo del has not seen enough unlab eled data to
learn general-purp ose language capabilities. For examples-prop ortional mixing, we ˝n d that
for most tasks there is a sweet sp ot for
K
where the mo del obtains the b est p erformance,
and larger or smaller values of
K
tend to result in worse p erformance. The exception (for the
range of
K
values we considered) was WMT English to French translation, which is such a
high-resource task that it always b ene˝ts from a high er mixing prop ortion. Finally, we note
that temp erature-scaled mixing also provides a means of obtaining reasonable p erf ormance
from most tasks , with
T
= 2
p erforming th e b est in most cases. Th e ˝nding that a multi-task
mo del is outp erformed by separate mo dels trained on each individual task has previously
b een observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has
b een shown that the multi-task setup can confer b ene˝ts across very similar tasks Liu et al.
(2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap
b etween multi-task training an d the pre-train-then-˝ne-tun e approach.
32
Exploring the Limits of Transfer Learning
Training strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Unsup ervised pre-training + ˝ne-tuning
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98
39
:
82 27
:
65
Multi-task training
81
:
42
19
:
24
79
:
78 67
:
30 25
:
21 36
:
30 27
:
76
Multi-task pre-training + ˝ne-tuning
83
:
11 19
:
12 80
:
26 71
:
03 27
:
08
39
:
80
28
:
07
Leave-one-out multi-task training
81
:
98 19
:
05 79
:
97
71
:
68 26
:
93
39
:
79
27
:
87
Sup ervised multi-task pre-training
79
:
93 18
:
96 77
:
38 65
:
36 26
:
81
40
:
13 28
:
04
Table 12:
Comparison of unsup ervised pre-training, mu lti- task learnin g, and various forms
of multi-task pre-training.
3.5.3. Com bining Multi-Task Learning with Fine-Tuning
Recall that we are studying a relaxed version of multi-task learning where we train a single
mo del on a mixture of tasks but are allowed to evaluate p erformance using di˙erent parameter
settings (checkp oints) for the mo del. We can extend this approach by considering the case
where the mo del is pre-trained on all tasks at once but is then ˝ne-tuned on the individual
sup ervised tasks. This is th e metho d used by th e MT-DNN (Liu et al., 2015, 2019b),
which achieved state-of-the-art p erformance on GLUE and other b enchmarks wh en it was
intro duced. We consider three variants of this approach : In the ˝rst, we simply pre-train the
mo del on an examples-prop ortional mixture with an arti˝cial data set size limit of
K
= 2
19
b efore ˝ne-tuning it on each individual downstream task. Th is helps us measure whether
including the sup ervised tasks alongside the unsup ervised ob jective d uring pre-training
gives the mo del some b en e˝ cial early exp osure to the downstream tasks. We might also
hop e that mixing in many sources of sup ervision could help the pre-trained mo del obtain a
more general set of skills (lo osely sp eaking) b efore it is adapted to an individual task. To
measure this d irectly, we consider a second variant where we pre-train the mo del on the same
examples-prop ortional mixture (with
K
= 2
19
) excep t that we omit one of the down strea m
tasks from this pre-training mixtu re. Then, we ˝ne-tune the mo del on th e task that was
left out during pre-training. We rep eat this for each of the downs tream tasks we consider.
We call this approach leave-one-out multi-task training. This simulates the real-world
setting where a pre-trained mo del is ˝ne-tuned on a task it had not seen during pre-training.
Note that multi-task pre-training provides a diverse mixture of sup ervised tasks. Since other
˝elds (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski
et al., 2014)) use a sup ervised data set for pre-training, we were interested to see whether
omitting the unsup ervised task from the multi-task pre-training mixture still pro duced go o d
results. For our third variant we therefore pre-train on an examples-prop ortional mixture of
all of the sup ervised tasks we consider with
K
= 2
19
. In all of these variants, we follow our
standard p ro cedure of pre-training for
2
19
steps b efore ˝ ne-tuning for
2
18
steps.
We compare the results of these approaches in Table 12. For comparison, we also include
results for our baseline (pre-train then ˝ne-tune) and for stan dard multi-task learning
(without ˝ ne-tuning) on an examples-prop ortional mixture with
K
= 2
19
. We ˝nd that
˝ne-tuning after multi-task pre-training results in comparable p erformance to our baseline.
This suggests that using ˝ne-tuning after multi-task learning can help mitigate some of
the trade-o˙s b etween di˙erent mixing rates describ ed in Section 3.5.2. Interestingly, the
33
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
p erformance of leave-one-out training was only slightly worse, suggesting that a mo del
that was trained on a variety of tas ks can still adapt to new t as ks (i.e. multi-task pre-
training might not result in a dramatic task interference). Finally, sup ervised multi-task
pre-training p erformed signi˝cantly worse in every case except for the translation tasks. This
could suggest that the translation tasks b ene˝t less from (English) pre-training, whereas
unsup ervised pre-training is an imp ortant factor in the other tasks.
3.6. Scali ng
The bitter lesson of machine learning research argues that general metho ds that can
leverage add ition al computation ultimately win out against metho ds that rely on human
exp ertise (Sutton, 2019; Hestness et al., 2017; Sh azeer et al., 2017; Jozef owicz et al., 2016;
Maha jan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a).
Recent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c;
Radford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has rep eatedly b een shown
that scaling up pro duces improved p erformance compared to more carefully-engineered
metho ds. However, there are a variety of p ossible ways to scale, including using a bigger
mo del, training the mo del f or more steps, an d ensembling. In this section, we compare these
di˙erent approach es by addressin g the following premise: You were ju st given
4

more
compute. How should you use it?
We start with our baseline mo del, which has
220M
parameters and is pre-trained and
˝ne-tuned for
2
19
and
2
18
steps resp ectively. The enco der and deco der are b oth sized
similarly to BERT
BASE
. To exp eriment with increased mo del size, we follow the guidelines
of BERT
LARGE
 Devlin et al. (2018) and use
d

= 4096
,
d
mo de l
= 1024
,
d
kv
= 64
and
16
-head attention mechanisms. We then generate two variants with
16
and
32
layers each in
the enco der and deco der, pro ducing mo dels with
2

and
4

as many parameters as our
original mo del. These two variants als o have a roughly
2

and
4

the computational cost.
Using our baseline and these two larger mo dels, we consider three ways of using
4

as much
computation: Training for
4

as many steps, training for
2

as many steps with the
2

bigger mo del, and training the
4

bigger mo del for the baseline numb er of training steps.
When we increase the training steps, we scale b oth the pre-train and ˝ne-tune steps for
simplicity. Note that when increasing the numb er of pre-training steps, we are e˙ectively
including more pre- train in g data as C4 is so large that we do not complete one pass over
the data even when training for
2
23
steps.
An alternative way for the mo del to s ee
4

as much d ata is to increase the batch size by a
factor of
4
. This can p otentially result in faster training du e to more e˚cient parallelization.
However, training with a
4

larger batch size can yield a d i˙erent outcome than training
for
4

as many s tep s (Shallue et al., 2018). We include an additional exp eriment where we
train our baseline mo del with a
4

larger batch size to compare these two cases.
It is common practice on many of the b enchmarks we consider to eke out additional
p erformance by training and evaluating using an ens emble of mo dels . This provides an
orthogonal way of using additional computation. To compare other scaling meth o ds to
ensembling, we also measure th e p erf orman ce of an ensemble of
4
separately pre-trained and
˝ne-tuned mo dels. We average the logits across the ensemble b efore feeding them into the
output
softmax
nonlinearity to obtain an aggregate prediction. Instead of pre-training
4
34
Exploring the Limits of Transfer Learning
Scaling strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Baseline
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
1

size,
4

training steps
85
:
33 19
:
33 82
:
45 74
:
72 27
:
08 40
:
66 27
:
93
1

size,
4

batch size
84
:
60 19
:
42 82
:
52 74
:
64 27
:
07 40
:
60 27
:
84
2

size,
2

training steps
86
:
18
19
:
66
84
:
18
77
:
18 27
:
52
41
:
03
28
:
19
4

size,
1

training steps
85
:
91
19
:
73
83
:
86 78
:
04
27
:
47 40
:
71 28
:
10
4

ensembled
84
:
77
20
:
10
83
:
09 71
:
74
28
:
05
40
:
53
28
:
57
4

ensembled, ˝ne-tune only
84
:
05 19
:
57 82
:
36 71
:
55 27
:
55 40
:
22 28
:
09
Table 13:
Comparison of di˙erent metho ds of scaling up our baseline mo del. All metho ds
except ensembling ˝ ne-tuned mo dels use
4

the computation as the baseline.
Size refers to th e numb er of parameters in the mo del and trainin g time refers
to the numb er of steps used f or b oth pre-training and ˝ne-tuning.
separate mo dels , a cheap er alternative is to take a single pre-trained mo del and pro duce
4
separate ˝ne-tuned versions. While this do es not use our entire
4

computational budget,
we also include this metho d to see if it pro duces comp etitive p erformance to the other scaling
metho ds.
The p erformance achieved after applying these various scaling metho ds is shown in
Table 13. Unsurprisingly, increasing the training time and/or mo del size consistently
improves the baseline. There was n o clear winner b etween training for
4

as many steps
or using a
4

larger batch size, th ough b oth were b ene˝cial. In gen eral, increas ing the
mo del size resulted in an additional bump in p erformance compared to solely increasing
the training time or batch size. We did not observe a large di˙erence b etween training a
2

bigger mo del for
2

as long and training a
4

bigger mo del on any of the tasks we
studied. This suggests that increasing the training time and increasing the mo del size can b e
complementary means of improving p erformance. Our results also suggest that ensembling
provides an orthogonal and e˙ective means of improving p erformance through scale. In some
tasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling
4
completely separately trained mo dels signi˝ cantly outp erformed every other scaling approach.
Ensembling mo dels that were pre-train ed together but ˝ne-tuned separately also gave a
substantial p erformance increase over the baseline, which suggests a cheap er means of
improving p erformance. The only exception was Sup erGLUE, where neither ensembling
approach signi˝cantly improved over the baseline.
We note that di˙erent scaling metho ds have di˙erent trade-o˙s that are separate from
their p erformance. For example, using a larger mo del can make downstream ˝ne-tuning and
inference more exp ensive. In contrast, the cost of pre-training a small mo del for longer is
e˙ectively amortized if it is applied to many downstream tasks. Separately, we n ote that
ensembling
N
separate mo dels has a similar cost to using a mo del that has an
N

higher
computational cost. As a result, some consideration for the eventual use of th e mo del is
imp ortant when cho osing b etween scaling metho ds.
35
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
3.7. Putting It All Together
We now leverage the ins ights from our systematic study to determine how far we can push
p erformance on p opular NLP b enchmarks. We are also interested in exploring the current
limits of transfer learning for NLP by training larger mo dels on large amounts of data. We
start with our baseline training approach and make the following changes:
Ob jective
We swap out the i.i.d. denoising ob jective in our baseline for the span-corruption
ob jective describ ed in Section 3.3.4, which was lo os ely inspired by SpanBERT (Joshi
et al., 2019). Sp eci˝cally, we use a mean span length of
3
and corrupt
15%
of the
original sequence. We found that this ob jective pro duced marginally b etter p erformance
(Table 7) while b eing slightly more computationally e˚cient due to shorter target
sequence lengths.
Longer training
Our baseline mo del uses a relatively small amount of pre-training (
1
/
4
as
much as BERT (Devlin et al., 2018),
1
/
16
as much as XLNet (Yang et al., 2019),
1
/
64
as
much as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we
can train for substantially longer without rep eating data (which can b e detrimental,
as shown in Section 3.4.2). We found in Section 3.6 th at additional pre-training can
indeed b e helpful, and that b oth increasing the batch size and increasing the nu mb er of
training steps can confer this b ene˝t. We therefore pre-train our mo dels for
1
million
steps on a batch size of
2
11
sequences of length
512
, corresp onding to a total of ab out
1
trillion pre-training token s (ab ou t
32

as many as our baseline). In Section 3.4.1, we
showed that pre-training on the RealNews-like, WebText-like, and Wikip edia + TBC
data s ets outp erformed pre-training on C4 on a few downstream tasks. However, these
data set variants are su˚ciently small that they would b e rep eated hundreds of times
over the course of pre-training on
1
trillion tokens. Since we showed in Section 3.4.2
that this rep etition could b e harmful, we opted instead to continue using th e C4 data
set.
Mo del sizes
In Section 3.6 we also showed how scaling up the baseline mo del size improved
p erformance. However, using smaller mo dels can b e helpful in settings where limited
computational resources are available for ˝n e-tu ning or inference. Based on these
factors, we train mo dels with a wide range of sizes:
‹
Base.
This is our baseline mo del, whose hyp erparameters are describ ed in
Section 3.1.1. It has roughly
220
million parameters.
‹
Small.
We consider a smaller mo del, which scales the baseline down by u sing
d
mo de l
= 512
,
d

= 2
;
048
,
8
-headed attention, and only
6
layers each in the
enco der and deco der. This variant has ab out
60
million parameters.
‹
Large.
Since our baseline uses a BERT
BASE
-sized enco der and deco der, we
also consider a variant where the enco der and deco der are b oth similar in size
and structure to BERT
LARGE
. Sp eci˝cally, this variant uses
d
mo del
= 1
;
024
,
d

= 4
;
096
,
d
kv
= 64
,
16
-headed attention, and
24
layers each in the enco der and
deco der, resulting in around
770
million parameters.
‹
3B and 11B.
To further explore what kind of p erformance is p ossible when
using larger mo dels, we consider two additional variants. In b oth cases, we use
36
Exploring the Limits of Transfer Learning
d
mo de l
= 1024
, a
24
layer enco der an d deco der, and
d
kv
= 128
. For the 3B
variant, we use
d

= 16
;
384
with
32
-headed attention, which resu lts in around
2
:
8
billion parameters; for 11B we use
d

= 65
;
536
with
128
-headed attention
pro ducing a mo del with ab out
11
billion parameters. We chose to scale up
d

sp eci˝cally b ecause mo dern accelerators (such as the TPUs we train our mo dels
on) are most e˚cient for large dense matrix multiplications like th ose in th e
Transformer's feed-forward networks.
Multi-task pre-traini ng
In Section 3.5.3, we showed that pre-training on a multi-task
mixture of unsup ervised and sup ervised tasks b efore ˝ne-tuning worked as well as
pre-training on the unsup ervised task alone. This is the approach advo cated by the
MT-DNN (Liu et al., 2015, 2019b). It also has the practical b ene˝t of b eing able to
monitor downstream p erf orman ce for the entire duration of training, rather than
just during ˝ne-tuning. We therefore used multi-task pre-training in our ˝nal set of
exp eriments. We hyp othesize that larger mo dels trained for longer might b ene˝t from
a larger prop ortion of unlab eled data b ecause they are more likely to over˝t to smaller
training data sets. However, we also note that the results of Section 3.5.3 suggest that
˝ne-tuning after multi-task pre-training can mitigate some of the issues that might
arise from cho osing a sub optimal prop ortion of unlab eled data. Based on these ideas,
we substitute the following arti˝cial data set sizes for our unlab eled data b efore us in g
standard example-prop ortional mixing (describ ed in Section 3.5.2):
710
;
000
for Small,
2
;
620
;
000
for Base,
8
;
660
;
000
for Large,
33
;
500
;
000
for 3B, and
133
;
000
;
000
for 11B.
For all mo del variants, we also capp ed the e˙ective data set size of the WMT English
to Fren ch and WMT English to German data sets to
1M
examples during pre-training.
Fine-tuning on individual GLUE and Sup erGLUE tasks
So far, when ˝ne-tuning
on GLUE and Sup erGLUE, we have con catenated all of the data sets in each b enchmark
so that we on ly ˝ne-tu ne mo dels once for GLUE and once for Sup erGLUE. This
approach makes our stu dy logistically simpler, but we found that this sacri˝ces a small
amount of p erformance on some tasks compared to ˝ne-tuning on the task separately. A
p otential issue with ˝ne-tuning on individual tasks, which would otherwise b e mitigated
by training on all tasks at once, is that we might over˝t quickly to low-resource tasks .
For example, our large batch size of
2
11
length-
512
sequences would result in the entire
data set app earing multiple times in each batch for many of the low-resource GLUE
and Sup erGLUE tasks. We therefore use a smaller batch size of
8
length-
512
sequences
during ˝n e-tu ning for each GLUE and Sup erGLUE task. We also save checkp oints
every
1
;
000
steps rather than every
5
;
000
steps to ensure we have access to the mo del's
parameters b efore it over˝ts.
Beam search
All of our previous results were rep orted using greedy deco ding. For tasks
with long output sequences, we found improved p erformance from using b eam search
(Sutskever et al., 2014). S p eci˝cally, we use a b eam width of
4
and a length p enalty
of

= 0
:
6
(Wu et al., 2016) for the WMT translation and CNN/DM summarization
tasks.
Test set
Since th is is ou r ˝nal s et of exp eriments, we rep ort results on the test set rather
than the validation set. For CNN/Daily Mail, we use the standard test set distribu ted
37
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
with the data set. For the WMT tasks, this corresp onds to using
newstest2014
for
English-German,
newstest2015
for English-French, and
newstest2016
for English-
Romanian. For GLUE and S up erGLUE, we used the b enchmark evaluation servers to
compute o˚cial test set scores.
15 , 16
For SQuAD, evaluating on the test set requires
running inference on a b enchmark server. Unfortunately, th e computational resources
on this server are insu˚cient for obtaining predictions from our largest mo dels. As
a result, we instead continue to rep ort p erformance on the SQuAD validation set.
Fortunately, the mo del with the highest p erformance on the SQuAD test set also
rep orted res ults on the valid ation set, so we can still compare to what is os ten sibly
the state-of-the-art.
Apart from those changes mentioned ab ove, we use the same train ing pro cedu re and
hyp erparameters as our baseline (AdaFactor optimizer, inverse square ro ot learning rate
schedule for pre-training, constant learning rate for ˝ne-tuning, d rop out regularization,
vo cabulary, etc.). For reference, these details are describ ed in Section 2.
The results of this ˝nal set of exp eriments are sh own in Table 14. Overall, we achieved
state-of-the-art p erformance on
18
out of the
24
tasks we consider. As exp ected , our largest
(
11
billion parameter) mo del p erformed b est among our mo del size variants across all tasks.
Our T5-3B mo del variant did b eat the previous state of the art in a few tasks, but scaling
the mo del size to
11
billion parameters was the most imp ortant ingredient for achieving our
b est p erformance. We now analyze the results for each individual b enchmark.
We achieved a state-of-the-art average GLUE score of
90
:
3
. Notably, our p erformance was
substantially b etter than the previous state-of-the-art for th e natural language inference tasks
MNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine p erformance
has historically lagged b ehind human p erformance, which is
93
:
6
and
95
:
9
resp ectively (Wang
et al., 2018). In terms of parameter count, ou r 11B mo del variant is the largest mo del that
has b een submitted to the GLUE b enchmark. However, most of the b est-scoring s ubmissions
use a large amount of ensembling and computation to pro duce predictions. For example,
the b est-p erforming variant of ALBERT (Lan et al., 2019) uses a mo del similar in size an d
architecture to our 3B variant (though it h as d ramatically fewer parameters due to clever
parameter sharing). To pro duce its impressive p erformance on GLUE, the ALBERT authors
ensembled from 6 to 17 mo dels dep ending on the task. This likely results in it b eing more
computationally exp ensive to p ro duce pred ictions with the ALBERT ensemble than it is
with T5-11B.
For S QuAD, we outp erf ormed the previous state-of-the-art (ALBERT (Lan et al., 2019))
by over one p oint on the Exact Match score. SQuAD is a long-standing b enchmark that
was created over th ree years ago, and most recent improvements have only increased the
state-of-the-art by a fraction of a p ercentage p oint. We note that when results are rep orted
on the test s et, they are typically based on an ensemble of mo dels and/or leverage external
data sets (e.g. TriviaQA (Joshi et al., 2017) or NewsQA (Trischler et al., 2016)) to augment
the small SQuAD training set. Human p erformance on SQuAD is estimated at
82
:
30
and
91
:
22
for the Exact Match and F1 metric resp ectively (Ra jpu rkar et al., 2016), so it is not
clear if further improvements on this b enchmark are meaningful.
15.
http://gluebenchmark.com
16.
http://super.gluebenchmark.com
38
Exploring the Limits of Transfer Learning
GLUE CoLA SST-2 MRPC MRPC STS-B STS-B
Mo del Average Matthew's Accuracy F1 Accuracy Pearson Sp earman
Previous b est
89
:
4
a
69
:
2
b
97
:
1
a
93
:
6
b
91
:
5
b
92
:
7
b
92
:
3
b
T5-Small
77
:
4 41
:
0 91
:
8 89
:
7 86
:
6 85
:
6 85
:
0
T5-Base
82
:
7 51
:
1 95
:
2 90
:
7 87
:
5 89
:
4 88
:
6
T5-Large
86
:
4 61
:
2 96
:
3 92
:
4 89
:
9 89
:
9 89
:
2
T5-3B
88
:
5 67
:
1 97
:
4 92
:
5 90
:
0 90
:
6 89
:
8
T5-11B
90
:
3 71
:
6 97
:
5
92
:
8 90
:
4
93
:
1 92
:
8
QQP QQP MNLI-m MNLI-mm QNLI RTE WNLI
Mo del F1 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy
Previous b est
74
:
8
c
90
:
7
b
91
:
3
a
91
:
0
a
99
:
2
a
89
:
2
a
91
:
8
a
T5-Small
70
:
0 88
:
0 82
:
4 82
:
3 90
:
3 69
:
9 69
:
2
T5-Base
72
:
6 89
:
4 87
:
1 86
:
2 93
:
7 80
:
1 78
:
8
T5-Large
73
:
9 89
:
9 89
:
9 89
:
6 94
:
8 87
:
2 85
:
6
T5-3B
74
:
4 89
:
7 91
:
4 91
:
2 96
:
3 91
:
1 89
:
7
T5-11B
75
:
1
90
:
6
92
:
2 91
:
9
96
:
9
92
:
8 94
:
5
SQuAD SQuAD Sup erGLUE Bo olQ CB CB COPA
Mo del EM F1 Average Accuracy F1 Accuracy Accuracy
Previous b est
90
:
1
a
95
:
5
a
84
:
6
d
87
:
1
d
90
:
5
d
95
:
2
d
90
:
6
d
T5-Small
79
:
10 87
:
24 63
:
3 76
:
4 56
:
9 81
:
6 46
:
0
T5-Base
85
:
44 92
:
08 76
:
2 81
:
4 86
:
2 94
:
0 71
:
2
T5-Large
86
:
66 93
:
79 82
:
3 85
:
4 91
:
6 94
:
8 83
:
4
T5-3B
88
:
53 94
:
95 86
:
4 89
:
9 90
:
3 94
:
4 92
:
0
T5-11B
91
:
26 96
:
22 88
:
9 91
:
2 93
:
9 96
:
8 94
:
8
MultiRC MultiRC ReCoR D ReCoRD RTE WiC WSC
Mo del F 1a EM F1 Accuracy Accuracy Accuracy Accuracy
Previous b est
84
:
4
d
52
:
5
d
90
:
6
d
90
:
0
d
88
:
2
d
69
:
9
d
89
:
0
d
T5-Small
69
:
3 26
:
3 56
:
3 55
:
4 73
:
3 66
:
9 70
:
5
T5-Base
79
:
7 43
:
1 75
:
0 74
:
2 81
:
5 68
:
3 80
:
8
T5-Large
83
:
3 50
:
7 86
:
8 85
:
9 87
:
8 69
:
3 86
:
3
T5-3B
86
:
8 58
:
3 91
:
2 90
:
4 90
:
7 72
:
1 90
:
4
T5-11B
88
:
1 63
:
3 94
:
1 93
:
4 92
:
5 76
:
9 93
:
8
WMT EnDe WMT EnFr WMT EnRo CNN/DM CNN/DM CNN/DM
Mo del BLEU BLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L
Previous b est
33
:
8
e
43
:
8
e
38
:
5
f
43
:
47
g
20
:
30
g
40
:
63
g
T5-Small
26
:
7 36
:
0 26
:
8 41
:
12 19
:
56 38
:
35
T5-Base
30
:
9 41
:
2 28
:
0 42
:
05 20
:
34 39
:
40
T5-Large
32
:
0 41
:
5 28
:
1 42
:
50 20
:
68 39
:
75
T5-3B
31
:
8 42
:
6 28
:
2 42
:
72 21
:
02 39
:
94
T5-11B
32
:
1 43
:
4 28
:
1
43
:
52 21
:
55 40
:
69
Table 14:
Performance of our T5 variants on every task we study. Small, Base, Large, 3B,
and 11B refer to mo del con˝gurations with
60
million,
220
million,
770
million,
3
billion, and
11
billion parameters, resp ectively. In the ˝rst row of each table,
we rep ort the s tate- of-the-art for the task (as of Octob er 24th, 2019), with the
sup erscript denoting its source with references listed at the end of th is caption. All
results are rep orted on the test set except for SQuAD where we use the validation
set.
a
(Lan et al., 2019)
b
(Wang et al., 2019c)
c
(Zhu et al., 2019)
d
(Liu et al.,
2019c)
e
(Edunov et al., 2018)
f
(Lample and Conneau, 2019)
g
(Dong et al., 2019)
39
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
For Sup erGLUE, we improved up on the state-of-the-art by a large margin (from an
average score of
84
:
6
(Liu et al., 2019c) to
88
:
9
). Sup erGLUE was designed to include
tasks that were b eyond the scop e of cu rrent state-of-the-art systems, bu t solvable by most
college-educated English sp eakers (Wang et al., 2019b). We nearly match the human
p erformance of
89
:
8
(Wang et al., 2019b). Interestingly, on the reading comprehension tas ks
(MultiRC and ReCoRD) we exceed human p erformance by a large margin , suggesting the
evaluation metrics used for these tasks may b e biased towards machine-made predictions.
On the other hand, human s achieve
100%
accuracy on b oth COPA and WSC, which is
signi˝cantly b etter than our mo del's p erformance. This suggests that there remain linguistic
tasks that are hard f or our mo del to p erfect, particularly in the low-resource setting.
We did not achieve state-of-the-art p erformance on any of the WMT translation tasks.
This may b e in part due to our use of an English-only unlab eled data set. We also note that
most of the b est results on these tasks use backtranslation (Edunov et al., 2018; Lample and
Conneau, 2019), which is a sop histicat ed data augmentation scheme. The state of the art on
the low-resource English to Romanian b enchmark also uses additional forms of cross-lingual
unsup ervised training (Lample and Conneau, 2019). Our results suggest that scale and
English-language pre-training may b e ins u˚cient to match the p erformance of these more
sophisticated metho ds. On a more sp eci˝c note, the b est results on English to German
newstest2014
set use the much larger training set from WMT 2018 (Edunov et al., 2018),
making direct comparison to our results di˚cult.
Finally, on CNN/Daily Mail we attain state-of-the-art p erformance, though only by
a signi˝cant amount on the ROUGE-2-F score. It has b een shown that improvements
to the ROUGE score do not necessarily corresp ond to more coherent summaries (Paulus
et al., 2017). Furthermore, while CNN/Daily Mail is p osed as an abstractive s ummarization
b enchmark, purely extractive approaches have b een shown to work well (Liu, 2019). It has
also b een argued th at generative mo dels trained with maximum likeliho o d are prone to
pro ducing rep etitive summaries (See et al., 2017). Despite these p otential issues, we ˝ nd
that our mo dels do generate coherent and largely correct summaries. We provide some
non-cherry-picked validation set examples in App endix C.
To ach ieve its strong results, T5 combines insights from our exp erimental study with
unprecedented scale. Note that in S ection 3.6 we found that scaling up the pre-training
amount or size of our baseline mo del pro duced substantial gains. Given this, we were
interested to measure how much the non-scaling changes we intro duced into T5 contributed
to its strong p erformance. We therefore carried out a ˝nal exp eriment where we compared
the following three con˝gurations: First, the standard baseline mo del, which was pre-trained
on
2
35
ˇ
34B
tokens; second, the baseline trained instead for ab out 1 trillion tokens (i.e.
the same amount of pre-training used for T5), which we refer to as baseline-1T; and
third, T5-Base. Note that the di˙erences b etween baseline-1T and T5-Base comprise the
non-scaling changes we made when designing T5. As such, comparing the p erformance of
these two mo dels gives us a concrete measurement of the impact of the insights f rom our
systematic study.
The p erformance of these three mo del con˝gurations is shown in Table 15. Consistent
with the ˝ndings in Section 3.6, we ˝nd that additional pre-training improves p erformance
over the baseline. Nevertheless, T5-Base substantially outp erforms b aselin e-1T on all
down stream tasks . This suggests that scale is not the only factor that contributes to T5's
40
Exploring the Limits of Transfer Learning
Mo del GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo
F
Baseline
83
:
28 19
:
24 80
:
88 71
:
36 26
:
98 39
:
82 27
:
65
Baseline-1T
84
:
80 19
:
62 83
:
01 73
:
90 27
:
46 40
:
30 28
:
34
T5-Base
85
:
97 20
:
90 85
:
44 75
:
64 28
:
37 41
:
37 28
:
98
Table 15:
Perf orman ce comparis on of T5-Base to our baseline exp erimental setup used in
the rest of the pap er. Results are rep orted on the validation set. Baseline-1T
refers to the p erformance achieved by pre-training the baseline mo del on 1 trillion
tokens (the same numb er used for the T5 mo del variants ) instead of
2
35
ˇ
34B
tokens (as was used for the baseline).
success. We hyp oth es ize that the larger mo dels b ene˝t not only from their increased size
but also from these n on-scaling factors.
4. Re˛e ction
Having comp leted our systematic study, we wrap up by ˝ rst recapping some of our most
signi˝cant ˝ndings. Our results provide some high-level p ersp ective on which avenues of
research might b e more or less p romis in g. To conclude, we outline some top ics we think
might provide e˙ective app roaches for fu rth er progressing the ˝ eld .
4.1. Takeaways
Text-to-text
Our text-to-text framework provides a simple way to train a single mo d el
on a wide variety of text tasks using the same loss function and deco ding pro cedure.
We showed how this approach can b e su ccessf ully applied to generative tasks like
abstractive summarization, classi˝cation tasks like natural langu age inference, and
even regression tasks like STS-B. In s pite of its simplicity, we found the text- to-
text framework obtained comp arab le p erformance to task-sp eci˝c architectures and
ultimately pro duced state-of- the-art results when combined with scale.
Architectures
While some work on transfer learning for NLP has considered architectural
variants of the Transformer, we found th e original enco der- deco der form worked
b est in our text-to- text framework. Though an enco der-deco der mo del uses twice as
many parameters as enco der-only (e.g. BERT) or deco der-only (language mo del)
architectures, it has a similar computational cost. We also showed that sharing the
parameters in the enco der and deco d er did not result in a substantial p erformance
drop while halving the total parameter count.
Unsup ervised ob jectives
Overall, we found that mos t denoising ob jectives, which train
the mo del to reconstruct randomly corrupted text, p erformed similarly in the text-to-
text setup. As a result, we suggest using ob jectives that pro duce short target sequences
so that unsup ervised pre-training is more computationally e˚cient.
Data sets
We intro duced the Colossal Clean Crawled Corpus (C4), which comprises
heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to
41
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
data sets that use additional ˝ltering, we found that training on in-domain unlab eled
data could b o ost p erformance in a few downstream tasks. However, cons train in g to
a single domain typically res ults in a s maller data set. We separately showed that
p erformance can degrade when an unlab eled data set is small enough that it is rep eated
many times over the course of pre-training. This motivates the use of a large and
diverse d ata set like C4 for generic lan guage understanding tasks .
Training strategies
We found that the basic app roach of up dating all of a pre-trained
mo del's parameters during ˝ne-tuning outp erformed metho ds that are designed to
up date fewer parameters, although up dating all parameters is most exp ensive. We also
exp erimented with various approaches for training the mo del on multiple tasks at once,
which in our text-to-text setting simply corresp onds to mixing examples from di˙erent
data sets when constructing batches. The primary concern in multi-tas k learning is
setting the prop ortion of each task to train on. We ultimately did not ˝nd a strategy
for setting mixing prop ortions that matched the p erformance of the basic approach of
unsup ervised pre-training followed by sup ervised ˝ne-tuning. However, we f ound that
˝ne-tuning after pre- train in g on a mixture of tasks pro duced comparable p erformance
to unsu p ervised pre-training.
Scaling
We compared various s trategies f or taking advantage of additional compute, includ-
ing training the mo del on more data, training a larger mo del, an d using an ensemble
of mo dels. We found each app roach conferred a signi˝cant b o ost in p erformance,
though training a smaller mo del on more data was often ou tp erformed by training
a larger mo del for fewer steps. We also showed an ensemb le of mo dels can provide
substantially b etter results than a single mo del, which provides an orth ogonal means
of leveraging ad ditional computation. Ensembling mo dels th at were ˝ne-tuned from
the same base pre-trained mo del p erformed worse than pre-training and ˝ne-tuning
all mo dels completely separately, though ˝ne-tune-only ensembling still substantially
outp erformed a sin gle mo del.
Pushing the limits
We combined our ab ove insights and trained substantially larger
mo dels (up to
11
billion parameters) to achieve state-of-the-art results across many of
the b enchmarks we considered. For unsu p ervised training, we extracted text from our
C4 data set and applied a denoising ob jective that corrupts contiguous spans of tokens.
We pre-trained on a multi-task mixture b efore ˝ne-tuning on individual tasks. Overall,
our mo dels were trained on over
1
trillion tokens. In the interest of facilitating the
replication, extension, and application of our results, we release our co de, the C4 data
set, and pre-trained mo del weights for each T5 variant.
1
4.2. Outlo ok
The inconvenience of large mo del s
An unsurprising but imp ortant result from our
study is that larger mo dels tend to p erform b etter. The fact that the hardware used for
running these mo dels is continually getting cheap er and more p owerful suggests that
scaling up may continue to b e a promising way to achieve b etter p erf orman ce (Su tton,
2019). However, it will always b e the case that there are applications and scenarios
where using a smaller or less exp ensive mo del is helpful, for example when p erforming
42
Exploring the Limits of Transfer Learning
client-side inference or federated learning (Kone£ny et al., 2015, 2016). Relatedly, one
b ene˝cial u se of transfer learning is the p os sibility of attaining go o d p erformance on
low-resource tasks. Low-resource tasks often o ccur (by de˝nition) in s ettin gs where
one lacks the as sets to lab el more data. It follows that low-resource ap plications often
also have limited access to computational resources which can incur additional costs.
As a resu lt, we ad vo cate for research on metho ds that achieve stronger p erformance
with cheap er mo dels so that transfer learning can b e applied where it will have the
most impact. Some current work along these lines include distillation (Hinton et al.,
2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and
conditional computation (Shazeer et al., 2017).
More e˚cient knowledge extraction
Recall that one of the goals of pre- trainin g is
(lo osely sp eaking) to provid e the mo del with general-purp ose knowledge that improves
its p erformance on downstream tasks. The metho d we use in this work, which is
currently common practice, is to train the mo del to denoise corrupted spans of text.
We susp ect that this simplistic technique may not b e a very e˚cient way to teach the
mo del general-pu rp ose knowledge. More concretely, it would b e useful to b e able to
attain go o d ˝ne-tuning p erformance without needing to train our mo dels on
1
trillion
tokens of text ˝rst. Some concurrent work along these lines improves e˚ciency by
pre-training a mo del to distinguish b etween real and machine-generated text (Clark
et al., 2020).
Formalizing the similarity b etween tasks
We observed that pre-training on unlab eled
in-domain data can improve p erf ormance on downstream tasks (Section 3.4). This
˝nding mostly relies on basic observations like th e fact that SQu AD was created using
data from Wikip edia. It would b e us ef ul to formulate a more rigorous notion of the
similarity b etween th e pre-training and downstream tasks, so that we could make
more principled choices ab out what source of unlab eled data to use. There is some
early empirical work along these lines in th e ˝eld of computer vision (Huh et al., 2016;
Kornblith et al., 2018; He et al., 2018). A b etter notion of the relatedness of tasks could
also help cho ose
supervised
pre-training tasks, which h as b een shown to b e helpful for
the GLUE b en chmark (Phang et al., 2018).
Language-agnostic mo dels
We were disapp ointed to ˝nd that English-only pre-training
did not achieve state-of-the-art results on the trans lation tasks we studied. We also
are interested in avoidin g the logistical di˚culty of n eedin g to sp ecify wh ich languages
a vo cabulary can enco de ahead of time. To address these issues, we are interested in
further investigating language-agnostic mo dels, i.e. mo dels that can p erform a given
NLP task with go o d p erformance regard less of the text's lan guage. This is an esp ecially
p ertinent issue given that English is not the native language for the ma jority of the
world's p opulation.
The motivation for this pap er was the ˛urry of recent work on trans fer learning for
NLP. Before we b egan this work, these advances had already enabled breakthrough s in
settings where learning-based metho ds had not yet b een shown to b e e˙ective. We are
happy to b e able to continue this trend , for example by nearly matching human-level
p erformance on the Sup erGLUE b enchmark, a task sp eci˝cally designed to b e di˚cult
43
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
for mo dern tran sfer-learnin g pip elines. Our results stem from the combination of a
straightforward and un i˝ ed text-to-text framework, our new C4 d ata set, and insights
from our systematic study. Additionally, we provided an empirical overview of the
˝eld and a p ersp ective on where it stands. We are excited to see continued work using
transfer learning towards th e goal of general language understanding.
Ack nowledgments
We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augu stus Odena, Daphne Ipp olito,
Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on
this manuscript; Zak Stone and the TFRC team for their supp ort; Au stin Tarango for
his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Je˙
Klingner, and Naveen Arivazhagan for in sight into multi-task machine translation; Neil
Houlsby f or comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Ban˝eld,
Yi Lin, and Frank Ch en for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and
Pierre Ruys sen for collab oration on TensorFlow Datasets; Rohan Anil for help with our
download pip eline for Common Crawl; Robby Neale and Taku Kudo for their work on
SentencePiece; and many other memb ers of the Go ogle Brain team for their discussion and
insight.
44
Exploring the Limits of Transfer Learning
App endix A. Contributions
Colin designed the scop e of this pro ject and wrote this pap er, ran all the exp eriments in
Sections 3.1 to 3.6, and contributed a large p ortion of our co d ebas e. Noam contributed
many of the ideas, including the text-to-text framework, u nsup ervised ob jectives, and
data set mixing s trategies; implemented our base Transformer mo del and its architectural
variants; and ran the exp eriments in Section 3.7. Adam oversaw all engineering asp ects
for this pro j ect, created the C4 data set, implemented our data set pip eline, and added
various b enchmark data sets. Katherine co ordinated exp eriments, wrote and up dated
do cumentation, ran exp eriments to help design our bas eline, and contributed to many parts
of ou r co debase. Sharan contributed s ome of the required data sets and prepro cessors, and
ran assorted preliminary exp eriments, in addition to co-leading the op en-sourcing of our
co debase. Michael owned all asp ects of the Winograd data sets, ingested many of the d ata
sets we used, contributed various imp rovements and ˝xes to our infrastructure, and ran some
preliminary exp eriments. Yanqi ran exp eriments and imp lemented metho ds to help settle on
a reasonable baseline and help ed with the ˝nal ˝ne-tuning of the mo dels in Section 3.7. Wei
also help ed with ˝nal ˝ne-tuning and improved s ome of our prepro cessors. Peter prototyp ed
an early version of the pre-training data set and resolved is sues p ertaining to th e SQuAD
and CNN/DM tasks. All authors help ed set the scop e and research direction we followed in
this work.
App endix B. Converting WNLI to Our Text-to-Text Format
Note th at as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead,
when evaluating on the WNLI test set (for the res ults in Section 3.7), we convert the WNLI
test set to the referent noun prediction text-to-text format so that we can evaluate using a
mo del trained on WSC and DPR. Our WNLI prep ro cessor is inspired by the one prop osed
by He et al. (2019). Recall that examples f rom WNLI consist of a premise, a hyp othesis,
and a lab el th at indicates whether the hyp othesis is True or False. Using the example from
Section 2.4, the hyp oth es is would b e The city councilmen refused the demonstrators a
p ermit b ecause they feared violence. with the premise The demonstrators feared violence.
and the lab el False. We ˝rst ˝nd the lo cation of all pronouns in the premise (they in
our example). Then, we ˝nd th e maximum numb er of words that precede or follow each
pronoun that are a substring in the hyp othesis (f eared violence in our example), ignoring
case and punctuation. When the premise contains multiple candidate pronouns, we cho ose
the pronoun that is preceded or followed by the largest substring of the hyp othesis. We then
highlight the pronoun in the premise by surrounding it with asterisks. For the candidate
noun (which is compared to our mo del's prediction to obtain a Tru e or False lab el), we
remove the match in g substring from th e hyp othesis and op tionally make it non-p ossess ive
(resulting in the demonstrators).
App endix C. Example Predictions on CNN/Daily Mail
To show that our mo del is generating ˛uent summaries, we include a few example deco des
from our b est mo del (T5-11B) on the validation set along with the ground-truth summaries.
These examples selected at random and were not cherry-picked.
45
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
1.
Ground-truth:
leopard gave up after spiky creature refused to back down
in fight in kruger national park, south africa . wildlife enthusiast lisl
moolman, 41, caught the bizarre battle while out on the road and armed with
her camera . the leopard and porcupine tumbled out of the bushes and began
to fight by roadside - watched by ms moolman .
Prediction:
leopard tried to make lunch out of a plucky porcupine in kruger
national park, south africa . but the predator was put firmly in its place
after the spiky creature refused to back down during a fight . wildlife
enthusiast lisl moolman, 41, caught the bizarre battle while out on the
road and armed with her camera .
2.
Ground-truth:
researchers say homes are a hotbed of undiscovered species
. study of 40 north carolina homes found 8,000 bacterial and archaeal taxa
. 11 houses in california revealed a great variety of fungus among us .
Prediction:
researchers estimate that the indoor biome makes up about 0.5
percent of ice-free land, or about 247,000 square miles, almost the size
of texas . a study of 40 houses in north carolina used genomic technologies
to document more than 8,000 bacterial and archaeal taxa . another study
of 50 houses in the same state described more than 750 types of arthropods,
including insects and spiders . an examination of 11 houses in california
found a great variety of fungus among us .
3.
Ground-truth:
executions have been temporarily halted in the state after
the drug used for lethal injections appeared faulty on monday night . kelly
gissendaner had her death delayed for a second time when it was found that
the lethal pentobarbital dose appeared cloudy . state officials dithered
over the decision , changing their mind three times before deciding not
to take the risk that it wouldn't work . it is 'essential that executions
are carried out in a constitutional manner,' said georgia attorney general
sam olens .
Prediction:
the execution of kelly gissendaner, the only woman on georgia's
death row, was delayed for a second time on monday . the execution team
found that the lethal pentobarbital dose appeared cloudy . the cloudy drug
bolstered death penalty opponents, who have been vocal in their opposition
after three botched executions in other parts of the country .
4.
Ground-truth:
dani alves was not selected for the brazil squad to face
france and chile . barcelona defender was filmed serving up hot dogs to
people on saturday . this week alves released a charity single with former
team-mat jose pinto . alves looks set to leave barcelona on a free transfer
this summer . reports in spanish press claim he will sign for paris saint-germain
.
Prediction:
dani alves was not selected for brazil's friendlies against
france and chile . the barcelona right back has released a charity single
with jose pinto . alves was filmed serving up snacks from behind the counter
46
Exploring the Limits of Transfer Learning
of a hot dog bar on saturday . alves is yet to be offered a new deal at
the nou camp . click here for all the latest barcelona news .
App endix D. Prepro cessed Examples
In this s ection, we provid e examples of our prepro ces sing for each of the data sets we cons id er.
D.1. CoLA
Original i nput:
Sentence:
John made Bill master of himself.
Pro cessed input:
cola sentence: John made Bill master of himself.
Original target:
1
Pro cessed target:
acceptable
D.2. RTE
Original i nput:
Sentence 1:
A smaller proportion of Yugoslavia's Italians were settled in Slovenia
(at the 1991 national census, some 3000 inhabitants of Slovenia declared
themselves as ethnic Italians).
Sentence 2:
Slovenia has 3,000 inhabitants.
Pro cessed input:
rte sentence1: A smaller proportion of Yugoslavia's Italians
were settled in Slovenia (at the 1991 national census, some 3000 inhabitants
of Slovenia declared themselves as ethnic Italians). sentence2: Slovenia
has 3,000 inhabitants.
Original target:
1
Pro cessed target:
not_entailment
D.3. MNLI
Original i nput:
Hyp othesis:
The St. Louis Cardinals have always won.
Premise:
yeah well losing is i mean i'm i'm originally from Saint Louis and
Saint Louis Cardinals when they were there were uh a mostly a losing team
but
Pro cessed input:
mnli hypothesis: The St. Louis Cardinals have always won. premise:
yeah well losing is i mean i'm i'm originally from Saint Louis and Saint Louis
Cardinals when they were there were uh a mostly a losing team but
47
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Original target:
2
Pro cessed target:
contradiction
D.4. MRPC
Original i nput:
Sentence 1:
We acted because we saw the existing evidence in a new light ,
through the prism of our experience on 11 September , " Rumsfeld said .
Sentence 2:
Rather , the US acted because the administration saw " existing
evidence in a new light , through the prism of our experience on September
11 " .
Pro cessed input:
mrpc sentence1: We acted because we saw the existing evidence
in a new light , through the prism of our experience on 11 September , " Rumsfeld
said . sentence2: Rather , the US acted because the administration saw "
existing evidence in a new light , through the prism of our experience on
September 11 " .
Original target:
1
Pro cessed target:
equivalent
D.5. QNLI
Original i nput:
Question:
Where did Jebe die?
Sentence:
Genghis Khan recalled Subutai back to Mongolia soon afterwards, and
Jebe died on the road back to Samarkand.
Pro cessed input:
qnli question: Where did Jebe die? sentence: Genghis Khan recalled
Subutai back to Mongolia soon afterwards, and Jebe died on the road back to
Samarkand.
Original target:
0
Pro cessed target:
entailment
D.6. QQP
Original i nput:
Question 1:
What attributes would have made you highly desirable in ancient
Rome?
Question 2:
How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?
Pro cessed input:
qqp question1: What attributes would have made you highly desirable
in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A
FRESHER?
48
Exploring the Limits of Transfer Learning
Original target:
0
Pro cessed target:
not_duplicate
D.7. SST2
Original i nput:
Sentence:
it confirms fincher 's status as a film maker who artfully bends
technical know-how to the service of psychological insight .
Pro cessed input:
sst2 sentence: it confirms fincher 's status as a film maker
who artfully bends technical know-how to the service of psychological insight
.
Original target:
1
Pro cessed target:
positive
D.8. STSB
Original i nput:
Sentence 1:
Representatives for Puretunes could not immediately be reached
for comment Wednesday.
Sentence 2:
Puretunes representatives could not be located Thursday to comment
on the suit.
Pro cessed input:
stsb sentence1: Representatives for Puretunes could not immediately
be reached for comment Wednesday. sentence2: Puretunes representatives could
not be located Thursday to comment on the suit.
Original target:
3.25
Pro cessed target:
3.2
D.9. CB
Original i nput:
Hyp othesis:
Valence was helping
Premise:
Valence the void-brain, Valence the virtuous valet. Why couldn't
the figger choose his own portion of titanic anatomy to shaft? Did he think
he was helping?
Pro cessed input:
cb hypothesis: Valence was helping premise: Valence the void-brain,
Valence the virtuous valet. Why couldn't the figger choose his own portion
of titanic anatomy to shaft? Did he think he was helping?
Original target:
1
Pro cessed target:
contradiction
49
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
D.10. COPA
Original i nput:
Question:
effect
Premise:
Political violence broke out in the nation.
Choice 1:
Many citizens relocated to the capitol.
Choice 2:
Many citizens took refuge in other territories.
Pro cessed input:
copa choice1: Many citizens relocated to the capitol. choice2:
Many citizens took refuge in other territories. premise: Political violence
broke out in the nation. question: effect
Original target:
1
Pro cessed target:
True
D.11. Multi RC
Original i nput:
Answer:
There was only pie to eat, rather than traditional breakfast foods
Paragraph:
<b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent
2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent
3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent
4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie's
pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food
before they left.<br><b>Sent 6: </b>He couldn't find anything to eat except
for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),
or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went
to the pond.<br><b>Sent 9: </b>On their way there they saw their friend
Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several
hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent
12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent
13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When
they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent
15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:
</b>The two squirrels ate some food that Joey's mom, Jasmine, made and went
off to bed.<br>
Question:
Why was Joey surprised the morning he woke up for breakfast?
Pro cessed input:
multirc question: Why was Joey surprised the morning he woke
up for breakfast? answer: There was only pie to eat, rather than traditional
breakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel
named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin
Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were
always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together
50
Exploring the Limits of Transfer Learning
at their Aunt Julie's pond.<br><b>Sent 5: </b>Joey woke up early in the morning
to eat some food before they left.<br><b>Sent 6: </b>He couldn't find anything
to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit
(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and
Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their
friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for
several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent
12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent
13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When
they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent
15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The
two squirrels ate some food that Joey's mom, Jasmine, made and went off to
bed.<br>
Original target:
1
Pro cessed target:
True
D.12. WiC
Original i nput:
POS:
N
Sentence 1:
It was the deliberation of his act that was insulting .
Sentence 2:
The deliberations of the jury .
Word:
deliberation
Pro cessed input:
wic pos: N sentence1: It was the deliberation of his act that
was insulting . sentence2: The deliberations of the jury . word: deliberation
Original target:
0
Pro cessed target:
False
D.13. WSC and DPR
Original i nput:
Span 2 text:
it
Span 1 text:
stable
Span 2 index:
20
Span 1 index:
1
Text:
The stable was very roomy, with four good stalls; a large swinging window
opened into the yard , which made it pleasant and airy.
Pro cessed input:
wsc: The stable was very roomy, with four good stalls; a large
swinging window opened into the yard , which made *it* pleasant and airy.
51
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Original target:
1
Pro cessed target:
stable
D.14. CNN/Dail y Mail
Original i nput:
marouane fellaini and adnan januzaj continue to show the world
they are not just teammates but also best mates. the manchester united and
belgium duo both posted pictures of themselves out at a restaurant on monday
night ahead of their game against newcastle on wednesday . januzaj poses
in the middle of fellaini and a friend looking like somebody who failed to
receive the memo about it being a jackson 5 themed night. premier league
duo adnan januzaj and marouane fellaini pose with a friend on the dance floor
. manchester united and belgium duo fellaini and januzaj are good friends
both on and off the pitch . manchester united ace fellaini runs over to the
bench to celebrate his goal against qpr with friend januzaj . the disco effect
in the background adds to the theory, but januzaj doesn't seem to mind as
they later pose on the dance floor with other friends. united haven't had
too many reasons to have a song and dance this season so it seems they may
be hitting the discotheques as another form of release. however, victory against
newcastle on wednesday would leave manager louis van gaal at least tapping
his toes as they continue to fight for a champions league spot this season.
januzaj and robin van persie join fellaini in celebrating in front of the
manchester united fans at west brom . januzaj receives some words of wisdom
from manchester united's dutch manager louis van gaal . januzaj and fellaini
are joined by some friends as they take to the dance floor ahead of the newcastle
game .
Pro cessed input:
summarize: marouane fellaini and adnan januzaj continue to show
the world they are not just teammates but also best mates. the manchester
united and belgium duo both posted pictures of themselves out at a restaurant
on monday night ahead of their game against newcastle on wednesday . januzaj
poses in the middle of fellaini and a friend looking like somebody who failed
to receive the memo about it being a jackson 5 themed night. premier league
duo adnan januzaj and marouane fellaini pose with a friend on the dance floor
. manchester united and belgium duo fellaini and januzaj are good friends
both on and off the pitch . manchester united ace fellaini runs over to the
bench to celebrate his goal against qpr with friend januzaj . the disco effect
in the background adds to the theory, but januzaj doesn't seem to mind as
they later pose on the dance floor with other friends. united haven't had
too many reasons to have a song and dance this season so it seems they may
be hitting the discotheques as another form of release. however, victory against
newcastle on wednesday would leave manager louis van gaal at least tapping
his toes as they continue to fight for a champions league spot this season.
januzaj and robin van persie join fellaini in celebrating in front of the
manchester united fans at west brom . januzaj receives some words of wisdom
52
Exploring the Limits of Transfer Learning
from manchester united's dutch manager louis van gaal . januzaj and fellaini
are joined by some friends as they take to the dance floor ahead of the newcastle
game .
Original target:
the belgian duo took to the dance floor on monday night with
some friends . manchester united face newcastle in the premier league on
wednesday . red devils will be looking for just their second league away win
in seven . louis van gaal's side currently sit two points clear of liverpool
in fourth .
Pro cessed target:
the belgian duo took to the dance floor on monday night with
some friends . manchester united face newcastle in the premier league on
wednesday . red devils will be looking for just their second league away win
in seven . louis van gaal's side currently sit two points clear of liverpool
in fourth .
D.15. SQuAD
Original i nput:
Question:
What does increased oxygen concentrations in the patient's lungs
displace?
Context:
Hyperbaric (high-pressure) medicine uses special oxygen chambers
to increase the partial pressure of O 2 around the patient and, when needed,
the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression
sickness (the 'bends') are sometimes treated using these devices. Increased
O 2 concentration in the lungs helps to displace carbon monoxide from the
heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria
that cause gas gangrene, so increasing its partial pressure helps kill them.
Decompression sickness occurs in divers who decompress too quickly after
a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming
in their blood. Increasing the pressure of O 2 as soon as possible is part
of the treatment.
Pro cessed input:
question: What does increased oxygen concentrations in the patient's
lungs displace? context: Hyperbaric (high-pressure) medicine uses special
oxygen chambers to increase the partial pressure of O 2 around the patient
and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene,
and decompression sickness (the 'bends') are sometimes treated using these
devices. Increased O 2 concentration in the lungs helps to displace carbon
monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the
anaerobic bacteria that cause gas gangrene, so increasing its partial pressure
helps kill them. Decompression sickness occurs in divers who decompress too
quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and
helium, forming in their blood. Increasing the pressure of O 2 as soon as
possible is part of the treatment.
53
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Original target:
carbon monoxide
Pro cessed target:
carbon monoxide
D.16. WMT English to German
Original i nput:
"Luigi often said to me that he never wanted the brothers to end
up in court," she wrote.
Pro cessed input:
translate English to German: "Luigi often said to me that he
never wanted the brothers to end up in court," she wrote.
Original target:
"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder
vor Gericht landen", schrieb sie.
Pro cessed target:
"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder
vor Gericht landen", schrieb sie.
D.17. WMT English to French
Original i nput:
This image section from an infrared recording by the Spitzer telescope
shows a "family portrait" of countless generations of stars: the oldest stars
are seen as blue dots, while more difficult to identify are the pink-coloured
"new-borns" in the star delivery room.
Pro cessed input:
translate English to French: This image section from an infrared
recording by the Spitzer telescope shows a "family portrait" of countless
generations of stars: the oldest stars are seen as blue dots, while more difficult
to identify are the pink-coloured "new-borns" in the star delivery room.
Original target:
Ce détail d'une photographie infrarouge prise par le télescope
Spitzer montre un "portrait de famille" des innombrables générations d'étoiles:
les plus vieilles étoiles sont en bleu et les points roses, plus difficiles
à identifier, sont les "nouveau-nés" dans la salle d'accouchement de l'univers.
Pro cessed target:
Ce détail d'une photographie infrarouge prise par le télescope
Spitzer montre un "portrait de famille" des innombrables générations d'étoiles:
les plus vieilles étoiles sont en bleu et les points roses, plus difficiles
à identifier, sont les "nouveau-nés" dans la salle d'accouchement de l'univers.
D.18. WMT English to Romanian
Original i nput:
Taco Bell said it plans to add 2,000 locations in the US by 2022.
Pro cessed input:
translate English to Romanian: Taco Bell said it plans to add
2,000 locations in the US by 2022.
Original target:
Taco Bell a afirmat c€, pân€ în 2022, intent
,
ioneaz€ s€ deschid€
2000 de restaurante în SUA.
54
Exploring the Limits of Transfer Learning
Pro cessed target:
Taco Bell a afirmat c€, pân€ în 2022, intent
,
ioneaz€ s€ deschid€
2000 de restaurante în SUA.
55
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
App endix E. Scores on Every Task for All Exp e riments
The f ollowing table lists the scores achieved on every task in the exp eriments describ ed in
Sections 3.2 to 3.6.
56
GLUE
Sup erGLUE WMT
Score CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLI
m
MNLI
mm
QNLI RTE
CNN/DM SQuAD
Score Bo olQ CB CB COPA MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC EnDe EnFr EnRo
Table Exp eriment Average MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc R-1-F R-2-F R-L-F EM F 1 Average Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc BLEU BLEU BLEU
1
F
Baseline average
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
1 Baseline standard deviation
0
:
235 1
:
111 0
:
569 0
:
729 1
:
019 0
:
374 0
:
418 0
:
108 0
:
070 0
:
291 0
:
231 0
:
361 1
:
393 0
:
065 0
:
065 0
:
058 0
:
343 0
:
226 0
:
416 0
:
365 3
:
237 2
:
560 2
:
741 0
:
716 1
:
011 0
:
370 0
:
379 1
:
228 0
:
850 2
:
029 0
:
112 0
:
090 0
:
108
1 No pre-training
66
:
22 12
:
29 80
:
62 81
:
42 73
:
04 72
:
58 72
:
97 81
:
94 86
:
62 68
:
02 67
:
98 75
:
69 58
:
84 39
:
19 17
:
60 36
:
69 50
:
31 61
:
97 53
:
04 65
:
38 71
:
61 76
:
79 62
:
00 59
:
10 0
:
84 20
:
33 17
:
95 54
:
15 54
:
08 65
:
38 25
:
86 39
:
77 24
:
04
2
F
Enc/dec, denoising
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
2 Enc/dec, shared, denoising
82
:
81 55
:
24 91
:
86 91
:
58 88
:
24 87
:
43 87
:
58 88
:
69 91
:
60 83
:
88 84
:
01 90
:
23 73
:
65 41
:
11 18
:
78 38
:
48 80
:
63 88
:
49 70
:
73 77
:
13 95
:
04 96
:
43 65
:
00 66
:
16 22
:
98 68
:
95 68
:
09 70
:
76 68
:
18 75
:
96 26
:
72 39
:
03 27
:
46
2 Enc/dec, 6 layers, denoising
80
:
88 46
:
26 92
:
09 91
:
51 87
:
99 87
:
01 86
:
76 87
:
93 90
:
97 82
:
20 82
:
41 88
:
83 71
:
48 40
:
83 18
:
97 38
:
31 77
:
59 86
:
07 68
:
42 73
:
79 91
:
70 92
:
86 67
:
00 61
:
02 19
:
62 61
:
26 60
:
33 72
:
20 65
:
99 75
:
00 26
:
38 38
:
40 26
:
95
2 Language mo del, denoising
74
:
70 24
:
50 90
:
60 86
:
08 78
:
92 85
:
22 85
:
42 85
:
40 88
:
99 76
:
72 77
:
05 86
:
02 64
:
62 39
:
49 17
:
93 36
:
91 61
:
14 71
:
37 55
:
02 65
:
47 60
:
08 71
:
43 58
:
00 43
:
03 2
:
94 53
:
35 52
:
31 53
:
07 58
:
62 63
:
46 25
:
09 35
:
28 25
:
86
2 Pre˝x LM, denoising
81
:
82 49
:
99 92
:
43 91
:
43 88
:
24 87
:
20 86
:
98 88
:
41 91
:
39 82
:
32 82
:
93 88
:
71 74
:
01 40
:
46 18
:
61 37
:
90 78
:
94 87
:
31 68
:
11 75
:
50 93
:
37 91
:
07 60
:
00 63
:
43 21
:
20 65
:
03 64
:
11 71
:
48 65
:
67 73
:
08 26
:
43 37
:
98 27
:
39
2 Enc/dec, LM
79
:
56 42
:
03 91
:
86 91
:
64 88
:
24 87
:
13 87
:
00 88
:
21 91
:
15 81
:
68 81
:
66 88
:
54 65
:
70 40
:
67 18
:
59 38
:
13 76
:
02 84
:
85 64
:
29 72
:
23 85
:
74 89
:
29 57
:
00 60
:
53 16
:
26 59
:
28 58
:
30 65
:
34 64
:
89 70
:
19 26
:
27 39
:
17 26
:
86
2 Enc/dec, shared, LM
79
:
60 44
:
83 92
:
09 90
:
20 85
:
78 86
:
03 85
:
87 87
:
77 91
:
02 81
:
74 82
:
29 89
:
16 65
:
34 40
:
16 18
:
13 37
:
59 76
:
35 84
:
86 63
:
50 70
:
49 91
:
41 87
:
50 55
:
00 60
:
21 16
:
89 57
:
83 56
:
73 63
:
54 63
:
48 70
:
19 26
:
62 39
:
17 27
:
05
2 Enc/dec, 6 layers, LM
78
:
67 38
:
72 91
:
40 90
:
40 86
:
52 86
:
82 86
:
49 87
:
87 91
:
03 80
:
99 80
:
92 88
:
05 65
:
70 40
:
29 18
:
26 37
:
70 75
:
32 84
:
06 64
:
06 71
:
38 85
:
25 89
:
29 60
:
00 57
:
56 16
:
79 55
:
22 54
:
30 66
:
79 63
:
95 71
:
15 26
:
13 38
:
42 26
:
89
2 Language mo del, LM
73
:
78 28
:
53 89
:
79 85
:
23 78
:
68 84
:
22 84
:
00 84
:
88 88
:
70 74
:
94 75
:
77 84
:
84 58
:
84 38
:
97 17
:
54 36
:
37 53
:
81 64
:
55 56
:
51 64
:
22 59
:
92 71
:
43 64
:
00 53
:
04 1
:
05 46
:
81 45
:
78 58
:
84 56
:
74 69
:
23 25
:
23 34
:
31 25
:
38
2 Pre˝x LM, LM
79
:
68 41
:
26 92
:
09 90
:
11 86
:
27 86
:
82 86
:
32 88
:
35 91
:
35 81
:
71 82
:
02 89
:
04 68
:
59 39
:
66 17
:
84 37
:
13 76
:
87 85
:
39 64
:
86 71
:
47 93
:
37 91
:
07 57
:
00 58
:
67 16
:
89 59
:
25 58
:
16 64
:
26 66
:
30 71
:
15 26
:
28 37
:
51 26
:
76
4 Language mo deling with pre˝x
80
:
69 44
:
22 93
:
00 91
:
68 88
:
48 87
:
20 87
:
18 88
:
39 91
:
41 82
:
66 83
:
09 89
:
29 68
:
95 40
:
71 18
:
94 38
:
15 77
:
99 86
:
43 65
:
27 73
:
55 83
:
95 87
:
50 55
:
00 59
:
65 18
:
89 61
:
76 60
:
76 68
:
59 65
:
67 73
:
08 26
:
86 39
:
73 27
:
49
4 BERT-style (Devlin et al., 2018)
82
:
96 52
:
49 92
:
55 92
:
79 89
:
95 87
:
68 87
:
66 88
:
47 91
:
44 83
:
60 84
:
05 90
:
33 75
:
45 41
:
27 19
:
17 38
:
72 80
:
65 88
:
24 69
:
85 76
:
48 94
:
37 94
:
64 61
:
00 63
:
29 25
:
08 66
:
76 65
:
85 72
:
20 69
:
12 75
:
00 26
:
78 40
:
03 27
:
41
4 Deshu˜ing
73
:
17 22
:
82 87
:
16 86
:
88 81
:
13 84
:
03 83
:
82 86
:
38 89
:
90 76
:
30 76
:
34 84
:
18 58
:
84 40
:
75 18
:
59 38
:
10 67
:
61 76
:
76 58
:
47 69
:
17 63
:
70 78
:
57 56
:
00 59
:
85 12
:
70 45
:
52 44
:
36 57
:
04 64
:
89 68
:
27 26
:
11 39
:
30 25
:
62
5 BERT-style (Devlin et al., 2018)
82
:
96 52
:
49 92
:
55 92
:
79 89
:
95 87
:
68 87
:
66 88
:
47 91
:
44 83
:
60 84
:
05 90
:
33 75
:
45 41
:
27 19
:
17 38
:
72 80
:
65 88
:
24 69
:
85 76
:
48 94
:
37 94
:
64 61
:
00 63
:
29 25
:
08 66
:
76 65
:
85 72
:
20 69
:
12 75
:
00 26
:
78 40
:
03 27
:
41
5 MASS-style (Song et al., 2019)
82
:
32 47
:
01 91
:
63 92
:
53 89
:
71 88
:
21 88
:
18 88
:
58 91
:
44 82
:
96 83
:
67 90
:
02 77
:
26 41
:
16 19
:
16 38
:
55 80
:
10 88
:
07 69
:
28 75
:
08 84
:
98 89
:
29 63
:
00 64
:
46 23
:
50 66
:
71 65
:
91 72
:
20 67
:
71 78
:
85 26
:
79 39
:
89 27
:
55
5
F
Replace corrupted spans
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
5 Drop corrupted tokens
84
:
44 60
:
04 92
:
89 92
:
79 89
:
95 87
:
28 86
:
85 88
:
56 91
:
54 83
:
94 83
:
92 90
:
74 79
:
42 41
:
27 19
:
31 38
:
70 80
:
52 88
:
28 68
:
67 75
:
90 96
:
02 94
:
64 56
:
00 65
:
06 23
:
92 65
:
54 64
:
60 71
:
12 67
:
40 74
:
04 27
:
07 39
:
76 27
:
82
6 Corruption rate =
10% 82
:
82 52
:
71 92
:
09 91
:
55 88
:
24 88
:
19 88
:
15 88
:
47 91
:
40 83
:
50 84
:
51 90
:
33 75
:
45 41
:
05 19
:
00 38
:
53 80
:
38 88
:
36 69
:
55 74
:
98 92
:
37 92
:
86 62
:
00 66
:
04 24
:
66 67
:
93 67
:
09 70
:
76 67
:
24 75
:
96 26
:
87 39
:
28 27
:
44
6
F
Corruption rate =
15% 83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
6 Corruption rate =
25% 83
:
00 53
:
47 93
:
00 92
:
44 89
:
46 87
:
36 87
:
36 88
:
68 91
:
53 84
:
44 84
:
15 90
:
77 74
:
01 41
:
69 19
:
54 39
:
14 80
:
96 88
:
61 70
:
48 76
:
39 93
:
02 92
:
86 68
:
00 65
:
46 24
:
66 68
:
20 67
:
39 73
:
65 67
:
87 72
:
12 27
:
04 39
:
83 27
:
47
6 Corruption rate =
50% 81
:
27 46
:
26 91
:
63 91
:
11 87
:
99 87
:
87 87
:
64 88
:
70 91
:
57 83
:
64 84
:
10 90
:
24 70
:
76 41
:
51 19
:
32 38
:
89 79
:
80 87
:
76 70
:
33 75
:
02 93
:
05 92
:
86 68
:
00 62
:
97 24
:
13 64
:
94 64
:
13 72
:
20 68
:
50 77
:
88 27
:
01 39
:
90 27
:
49
7
F
Baseline (i.i.d.)
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
7 Average span length =
2 83
:
54 53
:
82 92
:
20 93
:
05 90
:
44 87
:
85 87
:
71 88
:
42 91
:
40 84
:
28 84
:
46 90
:
88 77
:
62 41
:
23 19
:
39 38
:
69 82
:
09 89
:
69 72
:
20 77
:
06 90
:
43 91
:
07 70
:
00 66
:
28 26
:
13 71
:
34 70
:
61 75
:
45 68
:
34 78
:
85 26
:
76 39
:
99 27
:
63
7 Average span length =
3 83
:
49 53
:
90 92
:
43 92
:
25 89
:
46 87
:
49 87
:
53 88
:
72 91
:
51 84
:
85 84
:
84 90
:
99 77
:
26 41
:
50 19
:
62 38
:
94 81
:
84 89
:
66 72
:
53 76
:
85 94
:
37 94
:
64 70
:
00 67
:
64 28
:
75 70
:
84 69
:
90 74
:
73 67
:
71 77
:
88 26
:
86 39
:
65 27
:
62
7 Average span length =
5 83
:
40 52
:
12 93
:
12 92
:
63 89
:
71 88
:
70 88
:
47 88
:
84 91
:
64 84
:
32 84
:
29 90
:
79 76
:
90 41
:
39 19
:
24 38
:
82 82
:
05 89
:
79 72
:
23 77
:
06 83
:
06 89
:
29 69
:
00 68
:
16 30
:
12 71
:
36 70
:
53 75
:
81 69
:
91 79
:
81 26
:
88 39
:
40 27
:
53
7 Average span length =
10 82
:
85 50
:
11 92
:
09 91
:
95 88
:
97 88
:
45 88
:
22 88
:
86 91
:
63 84
:
34 84
:
28 91
:
07 76
:
17 41
:
38 19
:
33 38
:
80 81
:
84 89
:
39 70
:
44 76
:
45 87
:
40 89
:
29 65
:
00 66
:
87 29
:
59 69
:
82 68
:
94 72
:
56 67
:
55 75
:
96 26
:
79 39
:
49 27
:
69
8
F
C4
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
8 C4, un˝ltered
81
:
46 48
:
01 91
:
63 92
:
72 89
:
95 87
:
79 87
:
60 88
:
31 91
:
27 82
:
30 82
:
34 88
:
71 72
:
20 41
:
09 19
:
14 38
:
54 78
:
78 87
:
04 68
:
04 75
:
75 89
:
17 91
:
07 62
:
00 65
:
52 25
:
60 62
:
42 61
:
58 69
:
68 67
:
08 72
:
12 26
:
55 39
:
34 27
:
21
8 RealNews-like
83
:
83 56
:
55 92
:
66 92
:
06 88
:
97 87
:
71 87
:
37 88
:
51 91
:
49 84
:
35 84
:
46 90
:
61 78
:
34 41
:
38 19
:
23 38
:
84 80
:
39 88
:
50 72
:
38 77
:
00 93
:
09 94
:
64 66
:
00 65
:
92 23
:
82 74
:
56 73
:
72 75
:
81 66
:
61 80
:
77 26
:
75 39
:
90 27
:
48
8 WebText-like
84
:
03 56
:
38 93
:
12 92
:
31 89
:
22 88
:
69 88
:
68 88
:
65 91
:
56 84
:
70 84
:
84 90
:
83 77
:
62 41
:
23 19
:
31 38
:
70 81
:
42 89
:
15 71
:
40 76
:
88 83
:
08 89
:
29 66
:
00 64
:
10 24
:
24 72
:
24 71
:
36 75
:
45 68
:
03 82
:
69 26
:
80 39
:
74 27
:
59
8 Wikip edia
81
:
85 45
:
53 92
:
32 91
:
67 88
:
24 85
:
62 86
:
40 88
:
37 91
:
34 82
:
61 83
:
25 90
:
96 77
:
26 41
:
39 19
:
31 38
:
81 81
:
29 89
:
18 68
:
01 76
:
12 56
:
03 80
:
36 67
:
00 65
:
01 25
:
92 69
:
03 68
:
06 74
:
73 67
:
08 76
:
92 26
:
94 39
:
69 27
:
67
8 Wikip edia + TBC
83
:
65 55
:
53 92
:
78 92
:
41 89
:
22 86
:
67 86
:
27 89
:
47 92
:
29 84
:
38 83
:
45 91
:
94 76
:
90 41
:
22 19
:
28 38
:
67 82
:
08 89
:
70 73
:
24 76
:
22 95
:
40 92
:
86 69
:
00 51
:
59 50
:
93 69
:
53 68
:
51 77
:
62 66
:
93 81
:
73 26
:
77 39
:
63 27
:
57
9
F
Full data set
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
9
2
29
(
64
rep eats)
82
:
87 53
:
82 92
:
78 91
:
79 88
:
73 87
:
56 87
:
58 88
:
73 91
:
54 84
:
07 84
:
21 90
:
59 73
:
65 41
:
18 19
:
19 38
:
67 80
:
97 88
:
90 72
:
03 76
:
76 92
:
96 92
:
86 66
:
00 65
:
11 26
:
76 69
:
35 68
:
49 75
:
81 67
:
24 82
:
69 26
:
83 39
:
74 27
:
63
9
2
27
(
256
rep eats)
82
:
62 50
:
60 92
:
32 92
:
07 88
:
73 87
:
83 87
:
60 88
:
65 91
:
54 83
:
43 84
:
37 90
:
12 75
:
81 41
:
24 19
:
20 38
:
70 79
:
78 87
:
63 69
:
97 75
:
29 93
:
42 91
:
07 63
:
00 61
:
82 23
:
61 66
:
27 65
:
39 73
:
65 66
:
30 80
:
77 27
:
02 39
:
71 27
:
33
9
2
25
(
1
;
024
rep eats)
79
:
55 43
:
84 91
:
28 89
:
32 85
:
05 85
:
92 85
:
74 88
:
05 91
:
09 81
:
29 81
:
72 87
:
90 69
:
31 40
:
66 18
:
57 38
:
13 76
:
27 84
:
58 64
:
76 72
:
63 83
:
97 82
:
14 64
:
00 59
:
39 17
:
94 56
:
94 56
:
04 64
:
98 65
:
20 73
:
08 26
:
38 39
:
56 26
:
80
9
2
23
(
4
;
096
rep eats)
76
:
34 32
:
68 89
:
45 89
:
84 86
:
03 83
:
49 83
:
42 87
:
18 90
:
61 77
:
80 78
:
69 85
:
47 64
:
62 40
:
16 18
:
33 37
:
66 70
:
92 80
:
20 59
:
29 69
:
85 73
:
48 73
:
21 56
:
00 57
:
66 14
:
38 46
:
69 45
:
79 59
:
57 65
:
05 68
:
27 26
:
37 38
:
84 25
:
81
10
F
All parameters
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
10 Adapter layers,
d
= 32 80
:
52 45
:
33 91
:
63 90
:
59 86
:
76 88
:
38 88
:
06 86
:
99 90
:
26 83
:
63 83
:
94 90
:
72 67
:
15 34
:
50 15
:
08 32
:
15 79
:
32 87
:
70 60
:
40 65
:
32 50
:
87 73
:
21 52
:
00 58
:
61 19
:
41 65
:
50 64
:
58 62
:
09 64
:
58 73
:
08 13
:
84 17
:
88 15
:
54
10 Adapter layers,
d
= 128 81
:
51 45
:
35 92
:
89 91
:
49 88
:
24 87
:
73 87
:
65 87
:
73 90
:
93 83
:
64 84
:
09 90
:
52 72
:
56 36
:
71 16
:
62 34
:
37 79
:
47 87
:
61 63
:
03 69
:
20 52
:
21 75
:
00 56
:
00 61
:
08 18
:
05 67
:
94 66
:
97 68
:
59 66
:
77 73
:
08 19
:
83 27
:
50 22
:
63
10 Adapter layers,
d
= 512 81
:
54 44
:
25 93
:
35 91
:
00 87
:
25 88
:
74 88
:
44 88
:
02 91
:
15 83
:
08 83
:
80 89
:
62 74
:
37 38
:
63 17
:
78 36
:
25 79
:
18 87
:
32 64
:
30 73
:
18 59
:
86 71
:
43 56
:
00 62
:
94 18
:
57 66
:
56 65
:
74 70
:
76 67
:
87 74
:
04 23
:
45 33
:
98 25
:
81
10 Adapter layers,
d
= 2048 82
:
62 49
:
86 92
:
55 91
:
30 87
:
99 88
:
46 88
:
35 88
:
36 91
:
40 83
:
63 83
:
18 90
:
66 76
:
53 39
:
44 18
:
30 37
:
06 79
:
40 87
:
36 68
:
61 74
:
53 88
:
00 91
:
07 58
:
00 61
:
10 18
:
89 66
:
73 66
:
06 73
:
29 71
:
16 75
:
96 25
:
64 36
:
92 26
:
93
10 Gradual Unfreezing
82
:
50 51
:
74 91
:
97 92
:
61 89
:
71 87
:
27 86
:
90 88
:
26 91
:
35 83
:
42 83
:
49 89
:
71 75
:
09 40
:
88 18
:
95 38
:
40 79
:
17 87
:
30 70
:
79 75
:
51 93
:
09 94
:
64 70
:
00 62
:
03 21
:
51 65
:
69 64
:
79 72
:
92 69
:
12 77
:
89 26
:
71 39
:
02 26
:
93
11
F
Baseline (pre-train/˝ne-tune)
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
11 Equal
76
:
13 39
:
47 90
:
94 82
:
90 75
:
74 78
:
83 78
:
44 86
:
45 89
:
71 82
:
08 82
:
92 90
:
13 59
:
93 40
:
95 19
:
02 38
:
39 76
:
51 85
:
61 63
:
37 73
:
06 82
:
37 83
:
93 65
:
00 60
:
89 17
:
52 60
:
51 59
:
70 61
:
01 60
:
03 65
:
38 23
:
89 34
:
31 26
:
78
11 Examples-prop ortional,
K
= 2
16
80
:
45 42
:
07 91
:
97 90
:
97 87
:
50 85
:
41 85
:
04 86
:
89 90
:
10 83
:
01 83
:
66 90
:
74 72
:
56 41
:
16 19
:
04 38
:
59 77
:
25 85
:
72 69
:
95 76
:
67 86
:
38 89
:
29 70
:
00 65
:
93 27
:
91 62
:
78 61
:
95 76
:
90 65
:
83 73
:
08 24
:
35 34
:
99 27
:
10
11 Examples-prop ortional,
K
= 2
17
81
:
56 47
:
35 91
:
40 91
:
55 88
:
24 86
:
15 85
:
93 86
:
94 90
:
06 82
:
76 84
:
12 90
:
79 75
:
09 41
:
06 19
:
12 38
:
47 77
:
00 85
:
87 67
:
91 77
:
89 77
:
54 85
:
71 57
:
00 67
:
78 27
:
07 61
:
51 60
:
54 79
:
06 65
:
20 74
:
04 24
:
36 35
:
00 27
:
25
11 Examples-prop ortional,
K
= 2
18
81
:
67 46
:
85 91
:
63 91
:
99 88
:
73 87
:
68 87
:
20 86
:
93 90
:
35 83
:
30 84
:
01 91
:
47 73
:
29 40
:
96 19
:
07 38
:
43 78
:
17 86
:
74 67
:
94 76
:
57 78
:
88 87
:
50 62
:
00 67
:
70 30
:
85 63
:
43 62
:
54 76
:
53 65
:
67 67
:
31 24
:
57 35
:
19 27
:
39
11 Examples-prop ortional,
K
= 2
19
81
:
42 45
:
94 91
:
63 92
:
20 89
:
22 88
:
44 88
:
32 86
:
84 90
:
10 83
:
73 84
:
29 91
:
84 70
:
40 41
:
26 19
:
24 38
:
71 79
:
78 88
:
15 67
:
30 75
:
66 75
:
59 87
:
50 59
:
00 68
:
22 30
:
64 65
:
32 64
:
29 73
:
65 65
:
05 69
:
23 25
:
21 36
:
30 27
:
76
11 Examples-prop ortional,
K
= 2
20
80
:
80 42
:
55 92
:
78 91
:
27 87
:
99 88
:
36 88
:
10 86
:
10 89
:
62 84
:
15 84
:
26 92
:
20 68
:
95 41
:
05 19
:
24 38
:
46 80
:
36 88
:
27 67
:
38 73
:
21 76
:
18 83
:
93 62
:
00 67
:
57 26
:
86 66
:
12 65
:
22 76
:
90 64
:
73 69
:
23 25
:
66 36
:
93 27
:
68
11 Examples-prop ortional,
K
= 2
21
79
:
83 44
:
45 91
:
28 89
:
00 84
:
31 87
:
54 87
:
40 84
:
93 88
:
53 82
:
54 84
:
16 90
:
85 67
:
87 40
:
51 18
:
79 37
:
92 79
:
50 87
:
48 65
:
10 71
:
16 68
:
88 85
:
71 57
:
00 62
:
75 23
:
40 64
:
50 63
:
65 72
:
92 64
:
11 71
:
15 25
:
82 37
:
22 27
:
13
11 Temp erature-scaled,
T
= 2 81
:
90 54
:
00 91
:
74 90
:
56 86
:
76 85
:
11 84
:
60 86
:
40 89
:
74 83
:
47 84
:
15 91
:
51 72
:
56 41
:
09 19
:
28 38
:
54 79
:
42 87
:
77 69
:
92 76
:
73 92
:
37 92
:
86 57
:
00 69
:
80 31
:
90 66
:
65 65
:
74 72
:
92 67
:
08 75
:
96 25
:
42 36
:
72 27
:
20
11 Temp erature-scaled,
T
= 4 80
:
56 45
:
38 91
:
97 89
:
68 85
:
78 83
:
13 82
:
76 86
:
39 90
:
00 82
:
78 84
:
19 91
:
16 73
:
65 41
:
09 19
:
22 38
:
51 77
:
99 86
:
81 69
:
54 76
:
76 97
:
36 96
:
43 59
:
00 68
:
10 31
:
48 64
:
26 63
:
27 74
:
73 64
:
26 71
:
15 25
:
04 35
:
82 27
:
45
11 Temp erature-scaled,
T
= 8 77
:
21 40
:
07 91
:
06 88
:
11 83
:
33 79
:
20 79
:
06 86
:
60 89
:
90 83
:
05 83
:
56 90
:
21 59
:
93 41
:
01 19
:
10 38
:
40 77
:
14 85
:
99 66
:
07 73
:
94 93
:
70 94
:
64 60
:
00 66
:
36 26
:
86 63
:
46 62
:
60 62
:
09 63
:
32 65
:
38 24
:
55 35
:
35 27
:
17
12
F
Unsup erv ised pre-training + ˝ne-tuning
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
12 Multi-task training
81
:
42 45
:
94 91
:
63 92
:
20 89
:
22 88
:
44 88
:
32 86
:
84 90
:
10 83
:
73 84
:
29 91
:
84 70
:
40 41
:
26 19
:
24 38
:
71 79
:
78 88
:
15 67
:
30 75
:
66 75
:
59 87
:
50 59
:
00 68
:
22 30
:
64 65
:
32 64
:
29 73
:
65 65
:
05 69
:
23 25
:
21 36
:
30 27
:
76
12 Multi-task pre-training + ˝ne-tuning
83
:
11 51
:
42 92
:
66 91
:
73 88
:
73 88
:
06 87
:
70 88
:
61 91
:
61 84
:
09 84
:
31 91
:
85 76
:
53 41
:
15 19
:
12 38
:
59 80
:
26 88
:
50 71
:
03 79
:
54 81
:
69 87
:
50 65
:
00 70
:
72 31
:
48 65
:
94 65
:
03 81
:
23 68
:
18 73
:
08 27
:
08 39
:
80 28
:
07
12 Leave-one-out multi-task training
81
:
98 48
:
00 93
:
23 91
:
72 88
:
24 87
:
76 87
:
32 88
:
61 91
:
44 84
:
00 84
:
11 90
:
79 72
:
20 41
:
34 19
:
05 38
:
77 79
:
97 88
:
10 71
:
68 78
:
35 86
:
76 89
:
29 66
:
00 68
:
09 29
:
49 66
:
23 65
:
27 79
:
06 68
:
65 78
:
85 26
:
93 39
:
79 27
:
87
12 Sup ervised multi-task pre-training
79
:
93 36
:
60 92
:
43 91
:
58 88
:
24 87
:
03 86
:
78 88
:
15 91
:
20 82
:
87 83
:
16 90
:
13 70
:
76 41
:
12 18
:
96 38
:
49 77
:
38 85
:
65 65
:
36 75
:
66 68
:
87 83
:
93 58
:
00 64
:
81 21
:
93 55
:
37 54
:
61 71
:
12 67
:
40 75
:
96 26
:
81 40
:
13 28
:
04
13
F
Baseline
83
:
28 53
:
84 92
:
68 92
:
07 88
:
92 88
:
02 87
:
94 88
:
67 91
:
56 84
:
24 84
:
57 90
:
48 76
:
28 41
:
33 19
:
24 38
:
77 80
:
88 88
:
81 71
:
36 76
:
62 91
:
22 91
:
96 66
:
20 66
:
13 25
:
78 69
:
05 68
:
16 75
:
34 68
:
04 78
:
56 26
:
98 39
:
82 27
:
65
13
1

size,
4

training steps
85
:
33 60
:
29 93
:
81 94
:
06 91
:
67 89
:
42 89
:
25 89
:
15 91
:
87 86
:
01 85
:
70 91
:
63 78
:
34 41
:
52 19
:
33 38
:
96 82
:
45 90
:
19 74
:
72 79
:
17 94
:
75 92
:
86 71
:
00 67
:
34 29
:
70 72
:
63 71
:
59 78
:
34 72
:
10 82
:
69 27
:
08 40
:
66 27
:
93
13
1

size,
4

batch size
84
:
60 56
:
08 93
:
12 92
:
31 89
:
22 88
:
85 88
:
84 89
:
35 92
:
07 85
:
98 86
:
13 91
:
07 80
:
14 41
:
70 19
:
42 39
:
08 82
:
52 90
:
21 74
:
64 78
:
78 93
:
69 94
:
64 72
:
00 68
:
09 30
:
95 74
:
73 73
:
90 76
:
53 70
:
06 81
:
73 27
:
07 40
:
60 27
:
84
13
2

size,
2

training steps
86
:
18 62
:
04 93
:
69 93
:
36 90
:
69 89
:
18 89
:
23 89
:
35 92
:
05 87
:
23 87
:
05 92
:
68 81
:
95 41
:
74 19
:
66 39
:
14 84
:
18 91
:
29 77
:
18 80
:
98 97
:
36 96
:
43 74
:
00 71
:
34 35
:
68 77
:
11 76
:
34 80
:
51 69
:
28 85
:
58 27
:
52 41
:
03 28
:
19
13
4

size,
1

training steps
85
:
91 57
:
58 94
:
38 92
:
67 89
:
95 89
:
60 89
:
60 89
:
44 92
:
14 87
:
05 87
:
12 93
:
12 83
:
39 41
:
60 19
:
73 39
:
08 83
:
86 91
:
32 78
:
04 81
:
38 89
:
09 94
:
64 73
:
00 73
:
74 40
:
40 78
:
25 77
:
40 81
:
59 70
:
22 91
:
35 27
:
47 40
:
71 28
:
10
13
4

ensembled
84
:
77 56
:
14 93
:
46 93
:
31 90
:
67 89
:
71 89
:
60 89
:
62 92
:
24 86
:
22 86
:
53 91
:
60 77
:
98 42
:
10 20
:
10 39
:
56 83
:
09 90
:
40 71
:
74 77
:
58 89
:
85 91
:
07 66
:
00 69
:
32 29
:
49 72
:
67 71
:
94 76
:
90 69
:
12 72
:
12 28
:
05 40
:
53 28
:
09
13
4

ensembled, ˝ne-tune only
84
:
05 54
:
78 92
:
78 93
:
15 90
:
44 88
:
34 88
:
12 89
:
27 91
:
97 85
:
33 85
:
88 90
:
98 77
:
62 41
:
66 19
:
57 39
:
12 82
:
36 89
:
86 71
:
56 77
:
43 90
:
07 92
:
86 69
:
00 67
:
31 26
:
34 70
:
47 69
:
64 75
:
45 68
:
18 74
:
04 27
:
55 40
:
22 28
:
09
Table 16:
Score achieved on every task we consider for all of the exp eriments in this pap er. In the ˝rst column, we list the table where the condensed results were presented for a given exp eriment. As in the main text, a row marked with
F
denotes our baseline mo del (d es crib ed in Section 3.1).
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
References
Rami Al-Rfou, Doko ok Cho e, Noah Constant, Mandy Gu o, and Llion Jones. Character-level
language mo deling with deep er self-attention. In
Proceedings of the AAAI Conference on
Arti˝cial Intel l igence
, 2019.
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-e˚cient adaptive
optimization for large-scale learning.
arXiv preprint arXiv:1901.11150
, 2019.
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin John son, Maxim
Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multi-
lingual neural machine translation in the wild: Findings and challenges.
arXiv preprint
arXiv:1907.05019
, 2019.
Jimmy Lei Ba, Jamie Ryan Kiros , and Geo˙rey E. Hinton. Layer normalization.
arXiv
preprint arXiv:1607.06450
, 2016.
Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-
driven pretrain in g of self-attention networks.
arXiv preprint arXiv:1903.07785
, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yos hua Bengio. Neural machine translation by
jointly learning to align and translate. In
Third International Conference on Learning
Representations
, 2015.
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural
machine translation.
arXiv preprint arXiv:1909.08478
, 2019.
Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language mo del f or scienti˝c
text. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)
, 2019.
Ond°ej Bo jar, Christian Buck, Christian Federmann, Barry Haddow, Ph ilipp Ko ehn, Jo-
hannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al.
Findings of the 2014 workshop on statistical machine translation. In
Proceedings of the
Ninth Workshop on Statistical Machine Translation
, 2014.
Ond°ej Bo jar, Ra jen Chatterjee, Christian Federmann, Barry Haddow, Matthias Hu ck,
Chris Hokamp, Philipp Ko ehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al.
Findings of the 2015 workshop on statistical machine translation. In
Proceedings of the
Tenth Workshop on Statistical Machine Translation
, 2015.
Ond°ej Bo jar, Ra jen Chatterjee, Chris tian Federmann, Yvette Graham, Barry Haddow,
Matthias Huck, Antonio Jimen o Yep es, Philipp Ko ehn, Varvara Logacheva, Christof Monz,
et al. Findings of th e 2016 con ference on machine translation. In
Proceedings of the First
Conference on Machine Translation
, 2016.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy
Bengio. Generating sentences from a continuous space.
arXiv preprint arXiv:1511.06349
,
2015.
58
Exploring the Limits of Transfer Learning
Christian Buck, Kenneth Hea˝eld, and Bas Van Ooyen. N-gram counts and language mo dels
from the common crawl. In
LREC
, 2014.
Rich Caru ana. Multitask learning.
Machine learning
, 28(1), 1997.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lop ez-Gazpio, and Lucia Sp ecia. Semeval-2017
task 1: Semantic textual similarity-multilingual and cross-lingual fo cu sed evalu ation.
arXiv preprint arXiv:1708.00055
, 2017.
Jianp eng Ch en g, Li Dong, and Mirella Lapata. Long short-term memory-networks for
machine read in g.
arXiv preprint arXiv:1601.06733
, 2016.
Christopher Clark, Kenton Lee, Min g-Wei Chang, Tom Kwiatkows ki, Michael Collins, and
Kristina Toutanova. Bo olQ: Exp loring the surprising di˚culty of natural yes/no questions.
arXiv preprint arXiv:1905.10044
, 2019.
Kevin Clark, Minh-Thang Luong, Quo c V Le, and Christopher D Manning. Electra:
Pre-training text en co ders as discriminators rather than generators.
arXiv preprint
arXiv:2003.10555
, 2020.
Alexis Conneau and Douwe Kiela. S entEval: An evaluation to olkit f or u niversal sentence
representations.
arXiv preprint arXiv:1803.05449
, 2018.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Sup er-
vised learning of universal sentence representations from natural lan guage inference data.
arXiv preprint arXiv:1705.02364
, 2017.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual
entailment challenge. In
Machine Learning Chal lenges Workshop
, 2005.
Andrew M. Dai and Quo c V. Le. Semi-sup ervised sequence learning. In
Advances in neural
information processing systems
, 2015.
Marie-Catherine De Marne˙, Mandy Simons, and Judith Tonhauser. The CommitmentBank:
Investigating pro jection in naturally o ccurring discourse. In
Sinn und Bedeutung 23
, 2019.
Jia Deng, Wei Dong, Richard So cher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A
large-scale hierarchical image database. In
2009 IEEE conference on computer vision and
pattern recognition
, 2009.
Jacob Devlin, Ming-Wei Ch ang, Kenton Lee, and Kristina Toutanova. BERT: Pre-
training of deep bidirectional transformers for langu age understanding.
arXiv preprint
arXiv:1810.04805
, 2018.
William B. Dolan and Chris Bro ckett. Automatically constructing a corpus of s entential para-
phrases. In
Proceedings of the Third International Workshop on Paraphrasing (IWP2005)
,
2005.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiao dong Liu, Yu Wang, Jianfeng Gao, Ming
Zhou, and Hsiao-Wuen Hon. Un i˝ ed language mo del pre-training for natural language
understanding and generation.
arXiv preprint arXiv:1905.03197
, 2019.
59
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understand ing back-translation
at scale.
arXiv preprint arXiv:1808.09381
, 2018.
Edouard Grave, Piotr Bo janowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.
Learning word vectors for 157 languages.
arXiv preprint arXiv:1802.06893
, 2018.
Alex Graves. Generating sequences with recurrent neural networks.
arXiv preprint
arXiv:1308.0850
, 2013.
Ivan Hab ernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus
with free license. In
Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC'16)
, pages 914922, 2016.
Kaiming He, Xiangyu Zhang, Shao qing Ren, and Jian Sun. Deep residual learning for
image recognition. In
Proceedings of the IEEE conference on computer vision and pattern
recognition
, 2016.
Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training.
arXiv
preprint arXiv:1811.08883
, 2018.
Pengch en g He, Xiao dong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network
mo del for commonsens e reasoning.
arXiv preprint arXiv:1907.11983
, 2019.
Karl Moritz Hermann, Tomas Ko cisky, Edward Grefenstette, Las se Esp eholt, Will Kay,
Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In
Advances in neural information processing systems
, 2015.
Jo el Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewo o Jun, Hassan
Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling
is predictable, empirically.
arXiv preprint arXiv:1712.00409
, 2017.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of
sentences from unlab elled data.
arXiv preprint arXiv:1602.03483
, 2016.
Geo˙rey Hinton, Oriol Vinyals, and Je˙ Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531
, 2015.
Neil Hou ls by, Andrei Giurgiu, Stan islaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and S ylvain Gelly. Parameter-e˚cient transfer
learning for NLP.
arXiv preprint arXiv:1902.00751
, 2019.
Jeremy Howard an d Sebastian Ruder. Universal language mo del ˝ne-tuning for text classi˝ -
cation.
arXiv preprint arXiv:1801.06146
, 2018.
Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorn e,
Noam Shazeer, Andrew M. Dai, Matthew D. Ho˙man, Monica Din cu lescu, and Dou-
glas Eck. Mu sic transformer: Generating music with long-term structure. In
Seventh
International Conference on Learning Representations
, 2018a.
60
Exploring the Limits of Transfer Learning
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJo ong Lee, Jiquan Ngiam, Quo c V
Le, and Zhifeng Chen. GPip e: E˚cient training of giant neural networks u sing pip eline
parallelism.
arXiv preprint arXiv:1811.06965
, 2018b.
Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet go o d for
transfer learning?
arXiv preprint arXiv:1608.08614
, 2016.
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First Quora dataset release: Question
pairs.
https://data.quora.com/First- Quora- Dataset- Release- Question- Pairs
,
2017.
Yangqing Jia, Evan Shelhamer, Je˙ Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Gu adarrama, an d Trevor Darrell. Ca˙e: Convolutional architecture for fast feature
emb edding. In
Proceedings of the 22nd ACM int ernational conference on Multimedia
,
2014.
Xiao qi Jiao, Yichun Yin, Lifeng S hang, Xin Jiang, Xiao Chen, Linlin Li, Fan g Wang, and
Qun Liu. TinyBERT: Distilling BERT f or natural langu age understanding.
arXiv preprint
arXiv:1909.10351
, 2019.
Mandar Joshi, Eunsol Choi, Daniel S . Weld, and Luke Zettlemoyer. TriviaQA: A large
scale distantly sup ervised challenge dataset for reading comprehension.
arXiv preprint
arXiv:1705.03551
, 2017.
Mandar Joshi, Danqi Chen, Yin han Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.
SpanBERT: Improving pre-training by representing an d predicting spans.
arXiv preprint
arXiv:1907.10529
, 2019.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language mo deling.
arXiv preprint arXiv:1602.02410
, 2016.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network
for mo delling senten ces. In
Proceedings of the 52nd Annual Meeting of the Association for
Computational Linguistics
, 2014.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard
So cher. CTRL: A con ditional transformer language mo del for controllable generation.
arXiv preprint arXiv:1909.05858
, 2019a.
Nitish Shirish Keskar, Bryan McCan n, Caiming Xiong, and Richard So cher. Unifying question
answering and text classi˝cation via span extraction.
arXiv preprint arXiv:1904.09286
,
2019b.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, an d Dan Roth.
Lo oking b eyond the surface: A challenge set for readin g comprehension over multiple
sentences. In
Proceedings of North American Chapter of the Association for Computational
Linguistics (NAACL)
, 2018.
61
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun , Antonio
Torralba, and Sanja Fidler. Skip-thought vectors. In
Advances in neural information
processing systems
, 2015.
Vid Ko cijan, Ana-Maria Cretu , Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick f or Winograd schema challenge.
arXiv preprint
arXiv:1905.06290
, 2019.
Jakub Kone£ny, Brend an McMahan, and Daniel Ramage. Federated op timization: Dis-
tributed optimization b eyond the datacenter.
arXiv preprint arXiv:1511.03575
, 2015.
Jakub Kone£ny, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
e˚ciency.
arXiv preprint arXiv:1610.05492
, 2016.
Simon Kornblith, Jonathon Shlens, and Quo c V. Le. Do b etter ImageNet mo dels transfer
b etter?
arXiv preprint arXiv:1805.08974
, 2018.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks.
arXiv
preprint arXiv:1404.5997
, 2014.
Taku Kudo. Subword regularization: Improvin g neural network translation mo dels with
multiple subword cand id ates .
arXiv preprint arXiv:1804.10959
, 2018.
Taku Kudo and John Richardson. SentencePiece: A simple and language indep endent sub-
word tokenizer and detokenizer for neural text pro cessing.
arXiv preprint arXiv:1808.06226
,
2018.
Guillaume Lample and Alexis Conneau. Cross-lingual language mo del pretraining.
arXiv
preprint arXiv:1901.07291
, 2019.
Zhenzhong Lan, Mingda Chen, Sebastian Go o dman, Kevin Gimp el, Piyush Sharma, and
Radu Soricut. ALBERT: A lite BERT for self-sup ervised learning of language representa-
tions.
arXiv preprint arXiv:1909.11942
, 2019.
Hector Levesque, Ern es t Davis, and Leora Morgenstern. Th e Win ograd schema ch allenge.
In
Thirteenth International Conference on the Principles of Know ledge Representation
and Reasoning
, 2012.
Qi Li. Literature survey: domain adaptation algorithms for natural langu age pro cessing.
2012.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In
Text
summarization branches out
, 2004.
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Go o drich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating Wikip edia by summarizing long sequences.
arXiv preprint
arXiv:1801.10198
, 2018.
Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization
using length- agnostic auto-enco ders.
arXiv preprint arXiv:1910.00998
, 2019a.
62
Exploring the Limits of Transfer Learning
Xiao dong Liu, Jianfeng Gao, Xiao dong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Rep-
resentation learning using multi-task deep neural networks for semantic class i˝cation
and information retrieval. In
Proceedings of the 2015 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies
,
2015.
Xiao dong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural
networks for natural lan guage understanding.
arXiv preprint arXiv:1901.11504
, 2019b.
Yang Liu. Fine-tune BERT for extractive summarization.
arXiv preprint arXiv:1903.10318
,
2019.
Yinhan Liu, Myle Ott, Naman Goyal, Jin gfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly op timized
BERT pretraining approach.
arXiv preprint arXiv:1907.11692
, 2019c.
La janugen Logeswaran and Honglak Lee. An e˚cient framework for learning sentence
representations.
arXiv preprint arXiv:1803.02893
, 2018.
Dhruv Maha jan, Ross Girshick, Vignesh Ramanathan, Kaimin g He, Manohar Paluri, Yixuan
Li, Ashwin Bharamb e, an d Laurens van der Maaten. Exploring the limits of weakly
sup ervised pretraining. In
Proceedings of the European Conference on Computer Vision
(ECCV)
, 2018.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard So cher. The nat-
ural language decathlon: Multitask learning as question ans wering.
arXiv preprint
arXiv:1806.08730
, 2018.
Tomas Mikolov, Kai Chen, Greg Corrad o, and Je˙rey Dean. E˚cient estimation of word
representations in vector space.
arXiv preprint arXiv:1301.3781
, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Je˙ Dean. Distributed
representation s of words and phrases and their comp ositionality. In
Advances in neural
information processing systems
, 2013b.
Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing
Xiang. Abstractive text s ummarization using sequence-to-sequence RNNs and b eyon d.
arXiv preprint arXiv:1602.06023
, 2016.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring
mid-level image representations using convolutional neural networks. In
Proceedings of
the IEEE conference on computer vision and pattern recognition
, 2014.
Kishore Papineni, Salim Rou kos, To dd Ward , an d Wei-Jing Zhu. BLEU: a metho d for
automatic evaluation of machine translation. In
Proceedings of the 40th annual meeting on
association for computational linguistics
. Asso ciation for Computational Lin guistics, 2002.
Romain Paulus, Caiming Xiong, and Richard So cher. A deep reinforced mo del for ab stractive
summarization.
arXiv preprint arXiv:1705.04304
, 2017.
63
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Je˙rey Pennington, Richard So cher, and Christopher Manning. GloVe: Global vectors
for word representation. In
Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)
, 2014.
Matthew Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting
pretrained representations to diverse tasks.
arXiv preprint arXiv:1903.05987
, 2019.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton
Lee, and Luke Zettlemoyer. Deep contextualized word representations.
arXiv preprint
arXiv:1802.05365
, 2018.
Jason P hang, Thibault Févry, an d Samuel R. Bowman. Sentence enco ders on STILTs: Sup-
plementary training on intermediate lab eled-data tasks.
arXiv preprint arXiv:1811.01088
,
2018.
Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for
evaluating context-sensitive rep resentations.
arXiv preprint arXiv:1808.09121
, 2018.
Matt Post. A call for clarity in rep orting BLEU scores.
arXiv preprint arXiv:1804.08771
,
2018.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training, 2018.
Alec Radford, Je˙rey Wu, Rewon Child, David Luan, Dario Amo dei, and Ilya Su tskever.
Language mo dels are unsup ervised multitask learners, 2019.
Altaf Rahman and Vincent Ng. Resolving complex cases of de˝nite pronouns: the Winograd
schema challenge. In
Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning
. Asso ciation
for Computational Linguistics, 2012.
Pranav Ra jpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+
questions f or machine comprehension of text.
arXiv preprint arXiv:1606.05250
, 2016.
Pra jit Ramachandran, Peter J. Liu, an d Quo c V. Le. Unsup ervised pretraining for sequence
to sequence learning.
arXiv preprint arXiv:1611.02683
, 2016.
Alex Ratner, Braden Hanco ck, Jared Dunnmon, Roger Goldman, and Christopher Ré.
Snorkel MeTaL: Weak sup ervision for multi-task learning. In
Proceedings of the Second
Workshop on Data Management for End-To-End Machine Learning
, 2018.
Melissa Ro emmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible
alternatives: An evaluation of commonsense caus al reasoning. In
2011 AAAI Spring
Symposium Series
, 2011.
Sebastian Ruder. An overview of multi-task learning in deep neural networks.
arXiv preprint
arXiv:1706.05098
, 2017.
Sebastian Ruder.
Neural transfer l earning for natural language processing
. PhD thesis, NUI
Galway, 2019.
64
Exploring the Limits of Transfer Learning
Sebastian Ruder, Matthew E. Peters , Swabha Swayamdipta, and Thomas Wolf. Transfer
learning in natural language pro cessing. In
Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Tutorials
,
pages 1518, 2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale
visual recognition challenge.
International journal of computer vision
, 2015.
Victor San h, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled
vers ion of BERT: s maller, faster, cheap er and lighter.
arXiv preprint arXiv:1910.01108
,
2019.
Abigail See, Peter J. Liu, and Christopher D. Man ning. Get to the p oint: Summarization
with p ointer-generator networks.
arXiv preprint arXiv:1704.04368
, 2017.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare
words with subword units.
arXiv preprint arXiv:1508.07909
, 2015.
Christopher J Shallue, Jaeho on Lee, Jo e Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the e˙ects of data parallelism on neural network training.
arXiv preprint arXiv:1811.03600
, 2018.
Peter Shaw, Jakob Uszkoreit, and As hish Vaswani. Self-attention with relative p osition
representations.
arXiv preprint arXiv:1803.02155
, 2018.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost.
arXiv preprint arXiv:1804.04235
, 2018.
Noam Shazeer, Azalia Mirh osein i, Krzysztof Maziarz, Andy Davis, Quo c Le, Geo˙rey Hinton,
and Je˙ Dean. Outrageou sly large neural networks: The s parsely- gated mixture-of-exp erts
layer.
arXiv preprint arXiv:1701.06538
, 2017.
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Pen p orn
Koanantako ol, Peter Hawkin s, HyoukJo ong Lee, Mingsheng Hong, Cli˙ Young, Ryan
Sepassi, and Blake Hechtman. Mesh -tensor˛ow: Deep learning for sup ercomputers. In
Advances in Neural Information Processing Systems
, 2018.
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Ko ehn, Chris Callison-
Burch, and Adam Lop ez. Dirt cheap web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
,
2013.
Richard So cher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. Recursive d eep mo dels for semantic comp ositionality over a
sentiment treebank. In
Proceedings of the 2013 conference on empirical methods in natural
language processing
, 2013.
Kaitao S ong, Xu Tan, Tao Qin, Jian feng Lu, and Tie-Yan Liu. MASS: Masked sequence to
sequence pre-training for language generation.
arXiv preprint arXiv:1905.02450
, 2019.
65
Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu
Nitish Srivastava, Geo˙rey Hinton, Alex Krizhevsky, Ilya Suts kever, and Ruslan Salakhutdi-
nov. Drop out: a simple way to prevent neural networks from over˝tting.
The Journal of
Machine Learning Research
, 2014.
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning
general p urp ose distributed sentence representations via large scale multi-task learning.
arXiv preprint arXiv:1804.00079
, 2018.
Ilya Sutskever, Oriol Vinyals, and Quo c V. Le. Sequence to sequence learning with neural
networks. In
Advances in neural information processing systems
, 2014.
Richard S. Su tton. The bitter lesson.
http://www.incompleteideas.net/IncIdeas/
BitterLesson.html
, 2019.
Wilson L. Taylor. Cloze pro cedure: A new to ol for measuring readability.
Journalism
Bul letin
, 1953.
Trieu H. Trinh and Quo c V. Le. A simple metho d for commonsense reasoning.
arXiv preprint
arXiv:1806.02847
, 2018.
Adam Trisch ler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip
Bachman, and Kaheer Suleman. NewsQA: A machine comp rehens ion datas et.
arXiv
preprint arXiv:1611.09830
, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
−ukasz Kaiser, and Illia Polos ukhin. Attention is all you need. In
Advances in neural
information processing systems
, 2017.
Elena Voita, Rico Sennrich , and Ivan Titov. The b ottom-up evolution of representations
in the tran sformer: A study with machine translation and language mo deling ob jectives.
arXiv preprint arXiv:1909.01380
, 2019.
Alex Wan g, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task b enchmark and analysis platform for natural language understanding.
arXiv preprint arXiv:1804.07461
, 2018.
Alex Wang, Jan Hula, Patrick Xia, Ragh avendra Pappagari, R. Thomas McCoy, Roma
Patel, Na joung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Can you tell me
how to get past Sesame Street? Sentence-level pretraining b eyond language mo deling. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
,
2019a.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. Sup erGLUE: A stickier b enchmark for general-
purp ose language understanding systems.
arXiv preprint arXiv:1905.00537
, 2019b.
Wei Wang, Bin Bi, Ming Yan, Ch en Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT:
Incorp orating language structures into pre-training for deep language understanding.
arXiv preprint arXiv:1908.04577
, 2019c.
66
Exploring the Limits of Transfer Learning
Alex Wars tadt, Amanpreet Sin gh, and Samuel R. Bowman. Neural network acceptability
judgments.
arXiv preprint arXiv:1805.12471
, 2018.
Adina Williams, Nikita Nangia, and Samu el R. Bowman. A broad-coverage challenge corpus
for sentence understanding through inference.
arXiv preprint arXiv:1704.05426
, 2017.
Ronald J. Williams and David Zipser. A learning algorithm for continually running fully
recurrent neural networks.
Neural computation
, 1989.
Yonghui Wu , Mike Schuster, Zhifeng Chen, Quo c V. Le, Mohammad Norouzi, Wolfgan g
Macherey, Maxim Krikun , Yu an Cao, Qin Gao, Klaus Mach erey, et al. Go ogle's neural
machin e translation system: Bridging the gap b etween human and machin e translation.
arXiv preprint arXiv:1609.08144
, 2016.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carb onell, Ruslan Salakhutdinov, and Quo c V.
Le. XLNet: Generalized autoregres sive pretraining for language understanding.
arXiv
preprint arXiv:1906.08237
, 2019.
Jason Yosinski, Je˙ Clune, Yoshua Bengio, and Ho d Lipson. How transferable are features
in deep neural networks? In
Advances in neural information processing systems
, 2014.
Adams Wei Yu, David Dohan, Min h-Thang Luong, Rui Zhao, Kai Chen, Mohammad
Norouzi, and Quo c V. Le. QAnet: Combining lo cal convolution with global self-attention
for reading comprehension.
arXiv preprint arXiv:1804.09541
, 2018.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Ro es-
ner, and Yejin Choi. Defending against neural fake news.
arXiv preprint arXiv:1905.12616
,
2019.
Sheng Zhang, Xiao dong Liu, Jing jing Liu, Jianfeng Gao, Kevin Duh, an d Benjamin
Van Durme. ReCoRD: Bridging the gap b etween human and machine commonsense
reading comprehension.
arXiv preprint arXiv:1810.12885
, 2018.
Chen Zhu, Yu Cheng, Zhe Gan, S iqi Sun, Thomas Goldstein, and Jing jing Liu. Freelb: En-
hanced adversarial training for language understanding.
arXiv preprint arXiv:1909.11764
,
2019.
Yukun Zhu, Ryan Kiros, Rich Zemel, Rus lan Salakhutdinov, Raquel Urtas un, Antonio
Torralba, and Sanja Fidler. Aligning b o oks and movies: Towards story-like visual expla-
nations by watching movies and reading b o oks. In
Proceedings of the IEE E international
conference on computer vision
, 2015.
67
