APrimerinBERTology:WhatWeKnowAboutHowBERTWorks
AnnaRogers
CenterforSocialDataScience
UniversityofCopenhagen
arogers@sodas.ku.dk
OlgaKovaleva
Dept.ofComputerScience
Universityof
MassachusettsLowell
okovalev@cs.uml.edu
AnnaRumshisky
Dept.ofComputerScience
Universityof
MassachusettsLowell
arum@cs.uml.edu
Abstract
Transformer-basedmodelshavepushedstate

oftheartinmanyareasofNLP,butourunder-

standingofwhatisbehindtheirsuccessisstill

limited.Thispaperisthefirstsurveyofover

150studiesofthepopularBERTmodel.We

reviewthecurrentstateofknowledgeabout

howBERTworks,whatkindofinformation

itlearnsandhowitisrepresented,common

modificationstoitstrainingobjectivesand

architecture,theoverparameterizationissue,

andapproachestocompression.Wethen

outlinedirectionsforfutureresearch.
1Introduction

Sincetheirintroductionin2017,Transformers

(Vaswanietal.,2017)havetakenNLPbystorm,

offeringenhancedparallelizationandbettermod-

elingoflong-rangedependencies.Thebestknown

Transformer-basedmodelisBERT(Devlinetal.,

2019);itobtainedstate-of-the-artresultsinnume-

rousbenchmarksandisstillamust-havebaseline.
AlthoughitisclearthatBERTworksremark-
ablywell,itislessclear
why
,whichlimitsfurther
hypothesis-drivenimprovementofthearchitec-

ture.UnlikeCNNs,theTransformershavelittle

cognitivemotivation,andthesizeofthesemodels

limitsourabilitytoexperimentwithpre-training

andperformablationstudies.Thisexplainsalarge

numberofstudiesoverthepastyearthatat-

temptedtounderstandthereasonsbehindBERT's

performance.
Inthispaper,weprovideanoverviewofwhat
hasbeenlearnedtodate,highlightingthequestions

thatarestillunresolved.Wefirstconsiderthe

linguisticaspectsofit,namely,thecurrentevi-

denceregardingthetypesoflinguisticandworld

knowledgelearnedbyBERT,aswellaswhereand

how thisknowledgemaybestoredinthemodel.

Wethenturntothetechnicalaspectsofthemodel
andprovideanoverviewofthecurrentproposals

toimproveBERT'sarchitecture,pre-training,and

fine-tuning.Weconcludebydiscussingtheissue

ofoverparameterization,theapproachestocom-

pressingBERT,andthenascentareaofpruning

asamodelanalysistechnique.

2OverviewofBERTArchitecture

Fundamentally,BERTisastackofTransformer

encoderlayers(Vaswanietal.,2017)thatconsist

ofmultipleself-attention``heads''.Foreveryin-

puttokeninasequence,eachheadcomputeskey,

value,andqueryvectors,usedtocreateaweighted

representation.Theoutputsofallheadsinthe

samelayerarecombinedandrunthroughafully

connectedlayer.Eachlayeriswrappedwithaskip

connectionandfollowedbylayernormalization.
TheconventionalworkflowforBERTconsists
oftwostages:pre-trainingandfine-tuning.Pre-

trainingusestwoself-supervisedtasks:masked

languagemodeling(MLM,predictionofrandomly

maskedinputtokens)andnextsentencepredic-

tion(NSP,predictingiftwoinputsentencesare

adjacenttoeachother).Infine-tuningfordown-

streamapplications,oneormorefullyconnected

layersaretypicallyaddedontopofthefinal

encoderlayer.
Theinputrepresentationsarecomputedas
follows:Eachwordintheinputisfirsttokenized

intowordpieces(Wuetal.,2016),andthenthree

embeddinglayers(token,position,andsegment)

arecombinedtoobtainafixed-lengthvector.

Specialtoken
[CLS]
isusedforclassification
predictions,and
[SEP]
separatesinputsegments.
Google
1
andHuggingFace(Wolfetal.,2020)
providemanyvariantsofBERT,includingthe

original``base''and``large''versions.Theyvary

inthenumberofheads,layers,andhiddenstate

size.
1
https://github.com/google-research/bert
.
842
TransactionsoftheAssociationforComputationalLinguis
tics,vol.8,pp.842Œ866,2020.https://doi.org/10.1162/
tacl
a
00349
ActionEditor:DipanjasDas.Submissionbatch:4/2020;Rev
isionbatch:8/2020;Published12/2020.
c

2020AssociationforComputationalLinguistics.Distribu
tedunderaCC-BY4.0license.
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
3WhatKnowledgeDoesBERTHave?

Anumberofstudieshavelookedattheknow-

ledgeencodedinBERTweights.Thepopularap-

proachesincludefill-in-the-gapprobesofMLM,

analysisofself-attentionweights,andprobing

classifierswithdifferentBERTrepresentationsas

inputs.

3.1Syntactic Knowledge

Linetal.(2019)showedthat
BERTrepresenta-
tionsarehierarchicalratherthanlinear
,thatis,
thereissomethingakintosyntactictreestructure

inadditiontothewordorderinformation.Tenney

etal.(2019b)andLiuetal.(2019a)alsoshowed

that
BERTembeddingsencodeinformation
aboutpartsofspeech,syntacticchunks,and

roles
.Enoughsyntacticinformationseemstobe
capturedinthetokenembeddingsthemselvesto

recoversyntactictrees(Vilaresetal.,2020;Kim

etal.,2020;RosaandMare

cek,2019),although
probingclassifierscouldnotrecoverthelabels

ofdistantparentnodesinthesyntactictree(Liu

etal.,2019a).WarstadtandBowman(2020)report

evidenceofhierarchicalstructureinthreeoutof

fourprobingtasks.
Asfaras
how
syntaxisrepresented,itseems
that
syntacticstructureisnotdirectlyencoded
inself-attentionweights
.Htutetal.(2019)were
unabletoextractfullparsetreesfromBERT

headsevenwiththegoldannotationsfortheroot.

Jawaharetal.(2019)includeabriefillustrationof

adependencytreeextracteddirectlyfromself-

attentionweights,butprovidenoquantitative

evaluation.
However,
syntacticinformationcanberecov-
eredfromBERTtokenrepresentations
.Hewitt
andManning(2019)wereabletolearntransforma-

tionmatricesthatsuccessfullyrecoveredsyntactic

dependenciesinPennTreebankdatafromBERT's

tokenembeddings(seealsoManningetal.,2020).

Jawaharetal.(2019)experimentedwithtransfor-

mationsofthe[CLS]tokenusingTensorProduct

DecompositionNetworks(McCoyetal.,2019a),

concludingthatdependencytreesarethebest

matchamongfivedecompositionschemes(although

thereportedMSEdifferencesareverysmall).

MiaschiandDell'Orletta(2020)performarange

ofsyntacticprobingexperimentswithconcate-

natedtokenrepresentationsasinput.
Notethatalltheseapproacheslookforthe
evidenceofgold-standardlinguisticstructures,
Figure1:Parameter-freeprobeforsyntacticknow-

ledge:wordssharingsyntacticsubtreeshavelarger

impactoneachotherintheMLMprediction(Wuetal.,

2020).

andaddsomeamountofextraknowledgetothe

probe.Mostrecently,Wuetal.(2020)proposeda

parameter-freeapproachbasedonmeasuringthe

impactthatonewordhasonpredictinganother

wordwithinasequenceintheMLMtask(Figure1).

Theyconcludedthat
BERT``naturally''learns
somesyntacticinformation,althoughitisnot

verysimilartolinguisticannotatedresources
.
Thefill-in-the-gapprobesofMLMshowed
that
BERTtakessubject-predicateagreement
intoaccountwhenperformingtheclozetask

(Goldberg,2019;vanSchijndeletal.,2019),

evenformeaninglesssentencesandsentences

withdistractorclausesbetweenthesubjectand

theverb(Goldberg,2019).Astudyofnegative

polarityitems(NPIs)byWarstadtetal.(2019)

showedthat
BERTisbetterabletodetectthe
presenceofNPIs
(e.g.,``ever'')
andthewords
thatallowtheiruse
(e.g.,``whether'')
than
scopeviolations.
Theaboveclaimsofsyntacticknowledgeare
beliedbytheevidencethat
BERTdoesnot
``understand''negationandisinsensitiveto

malformedinput
.Inparticular,itspredictions
werenotaltered
2
evenwithshuffledwordorder,
2
Seealsotherecentfindingsonadversarialtriggers,which
getthemodeltoproduceacertainoutputeventhoughthey
843
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
truncatedsentences,removedsubjectsandobjects

(Ettinger,2019).Thiscouldmeanthat
either
BERT'ssyntacticknowledgeisincomplete,or

itdoesnotneedtorelyonitforsolvingits

tasks
.Thelatterseemsmorelikely,sinceGlava

s
andVuli

c(2020)reportthatanintermediate
fine-tuningstepwithsupervisedparsingdoes

notmakemuchdifferencefordownstreamtask

performance.

3.2Semantic Knowledge

Todate,morestudieshavebeendevotedto

BERT'sknowledgeofsyntacticratherthanse-

manticphenomena.However,wedohaveevi-

dencefromanMLMprobingstudythat
BERT
hassomeknowledgeofsemanticroles
(Ettinger,
2019).BERTevendisplayssomepreferencefor

theincorrectfillersforsemanticrolesthatare

semanticallyrelatedtothecorrectones,asop-

posedtothosethatareunrelated(e.g.,``totipa

chef''isbetterthan``totiparobin'',butworse

than``totipawaiter'').
Tenneyetal.(2019b)showedthat
BERTen-
codesinformationaboutentitytypes,relations,

semanticroles,andproto-roles
,sincethisinfor-
mationcanbedetectedwithprobingclassifiers.
BERTstruggleswithrepresentationsofnum-
bers.
Additionandnumberdecodingtasksshowed
thatBERTdoesnotformgoodrepresentationsfor

floatingpointnumbersandfailstogeneralizeaway

fromthetrainingdata(Wallaceetal.,2019b).A

partoftheproblemisBERT'swordpiecetokeniza-

tion,sincenumbersofsimilarvaluescanbe di-

videdupintosubstantiallydifferentwordchunks.
Out-of-the-box
BERTissurprisinglybrittle
tonamedentityreplacements
:Forexample,
replacingnamesinthecoreferencetaskchanges

85%ofpredictions(Balasubramanianetal.,2020).

Thissuggeststhatthemodeldoesnotactually

formagenericideaofnamedentities,although

itsF1scores onNERprobingtasksarehigh

(Tenneyetal.,2019a).Broscheit(2019)findsthat

fine-tuningBERTonWikipediaentitylinking

``teaches''itadditionalentityknowledge,which

wouldsuggestthatitdidnotabsorballthe

relevantentityinformationduringpre-trainingon

Wikipedia.
arenotwell-formedfromthepointofviewofahumanreader

(Wallaceetal., 2019a).
Figure2:BERTworldknowledge(Petronietal.,2019).

3.3WorldKnowledge

Thebulkofevidenceaboutcommonsenseknow-

ledgecapturedinBERTcomesfrompractitioners

usingittoextractsuchknowledge.Onedirect

probingstudyofBERTreportsthat
BERTstrug-
gleswithpragmaticinferenceandrole-based

eventknowledge
(Ettinger,2019).BERTalso
struggleswithabstractattributesofobjects,as

wellasvisualandperceptualpropertiesthatare

likelytobeassumedratherthanmentioned(Da

andKasai,2019).
TheMLMcomponentofBERTiseasytoadapt
forknowledgeinductionbyfillingintheblanks

(e.g.,``Catsliketochase[
]'').Petronietal.
(2019)showedthat,
forsomerelationtypes,va-
nillaBERTiscompetitivewithmethodsrelying

onknowledgebases
(Figure2),andRobertsetal.
(2020)showthesameforopen-domain QAusing

theT5model(Raffeletal.,2019).Davisonetal.

(2019)suggestthatitgeneralizesbettertounseen

data.InordertoretrieveBERT'sknowledge,we

needgoodtemplatesentences,andthereiswork

ontheirautomaticextractionandaugmentation

(Bouraouietal.,2019;Jiangetal.,2019b).
However,
BERTcannotreasonbasedonits
worldknowledge
.Forbesetal.(2019)showthat
BERTcan``guess''theaffordancesandproperties

ofmanyobjects,butcannotreasonaboutthe

relationshipbetweenpropertiesandaffordances.

Forexample,it``knows''thatpeoplecanwalk

intohouses,andthathousesarebig,butitcannot

inferthathousesarebiggerthanpeople.Zhouetal.

(2020)andRichardsonandSabharwal(2019)also

showthattheperformancedropswiththenumber

ofnecessaryinferencesteps.SomeofBERT's

worldknowledgesuccesscomesfromlearning

stereotypicalassociations(Poerneretal.,2019),

forexample,apersonwithanItalian-sounding

nameispredictedtobeItalian,evenwhenitis

incorrect.
844
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
3.4Limitations

Multipleprobingstudiesinsection3andsection4

reportthatBERTpossessesasurprisingamountof

syntactic,semantic,andworldknowledge.How-

ever,Tenneyetal.(2019a)remark,``thefactthat

alinguisticpatternisnotobservedbyourprobing

classifierdoesnotguaranteethatitisnotthere,and

theobservationofapatterndoesnottellushowit

isused.''Thereisalsotheissueofhowcomplexa

probeshouldbeallowedtobe(Liuetal.,2019a).

Ifamorecomplexproberecoversmoreinfor-

mation,towhatextentarewestillrelyingonthe

originalmodel?
Furthermore,differentprobingmethodsmay
leadtocomplementaryorevencontradictorycon-

clusions,which makesasingletest(asinmost

studies)insufficient(Warstadtetal.,2019).A

givenmethodmightalsofavoronemodel over

another,forexample,RoBERTatrailsBERTwith

onetreeextractionmethod,butleadswithanother

(Htutetal.,2019).Thechoiceoflinguisticformal-

ismalsomatters(KuznetsovandGurevych,2020).
Inviewofallthat,thealternativeistofocus
onidentifyingwhatBERTactuallyreliesonat

inferencetime.Thisdirectioniscurrentlypursued

bothatthelevelofarchitectureblocks(tobe

discussedindetailinsubsection6.3),andatthe

levelofinformationencodedinmodelweights.

Amnesicprobing(Elazaretal.,2020)aimsto

specificallyremovecertaininformationfromthe

modelandseehowitchangesperformance,

finding,forexample,thatlanguagemodelingdoes

relyonpart-of-speechinformation.
Anotherdirectionisinformation-theoreticprob-
ing.Pimenteletal.(2020)operationalizeprobing

asestimatingmutualinformationbetweenthe

learnedrepresentationandagivenlinguisticprop-

erty,whichhighlightsthatthefocusshouldbe

notontheamountofinformationcontainedin

arepresentation,butratheronhoweasilyitcan

beextractedfromit.Voita andTitov(2020)quan-

tifytheamountofeffortneededtoextractinfor-

mationfromagivenrepresentationasminimum

descriptionlengthneededtocommunicateboth

theprobesizeandtheamountofdatarequiredfor

ittodowellonatask.

4LocalizingLinguisticKnowledge

4.1BERTEmbeddings

InstudiesofBERT,theterm``embedding''refers

totheoutputofaTransformerlayer(typically,
thefinalone).Bothconventionalstaticembed-

dings(Mikolovetal.,2013)andBERT-style

embeddingscanbeviewedintermsofmutual

informationmaximization(Kongetal.,2019),

butthelatterare
contextualized
.Everytokenis
representedbyavectordependentonthepar-

ticularcontextofoccurrence,andcontainsatleast

someinformationaboutthatcontext(Miaschiand

Dell'Orletta,2020).
Severalstudiesreportedthat
distilledcontext-
ualizedembeddingsbetterencodelexicalseman-

ticinformation
(i.e.,theyarebetterattraditional
word-leveltaskssuchaswordsimilarity).The

methodstodistillacontextualizedrepresentation

intostaticincludeaggregatingtheinformation

acrossmultiplecontexts(Akbiketal.,2019;

Bommasanietal.,2020),encoding``semantically

bleached''sentencesthatrelyalmostexclusively

onthemeaningofagivenword(e.g.,"Thisis<>")

(Mayetal.,2019),andevenusingcontextualized

embeddingstotrainstaticembeddings(Wang

etal.,2020d).
Butthisisnottosaythatthereisnoroom
forimprovement.Ethayarajh(2019)measurehow

similarthe embeddingsforidenticalwordsare

ineverylayer,reportingthatlaterBERTlayers

producemorecontext-specificrepresentations.
3
TheyalsofindthatBERTembeddingsoccupya

narrowconeinthevectorspace,andthiseffect

increasesfromtheearliertolaterlayers.Thatis,

tworandomwordswillonaveragehaveamuch

highercosinesimilaritythanexpectedifem-

beddingsweredirectionallyuniform(isotro-

pic)
.Becauseisotropy wasshowntobebeneficial
forstaticwordembeddings(MuandViswanath,

2018),thismightbeafruitfuldirectiontoexplore

forBERT.
BecauseBERTembeddingsarecontextualized,
aninterestingquestionistowhatextentthey

capturephenomenalikepolysemyandhom-

onymy.Thereisindeedevidencethat
BERT's
contextualizedembeddingsformdistinctclus-

terscorrespondingtowordsenses
(Wiedemann
etal.,2019;SchmidtandHofmann,2020),making

BERTsuccessfulatwordsensedisambiguation

task.However,Mickusetal.(2019)notethat

therepresentationsofthesameworddepend
3
Voitaetal.(2019a) lookattheevolutionoftoken
embeddings,showingthatintheearlierTransformerlayers,

MLMforcestheacquisitionofcontextualinformationatthe

expenseofthe tokenidentity,whichgetsrecreatedinlater

layers.845
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Figure3:AttentionpatternsinBERT(Kovalevaetal.,2019).
onthepositionofthesentenceinwhichit

occurs
,likelyduetotheNSPobjective.Thisis
notdesirablefromthelinguisticpointofview,and

couldbeapromisingavenueforfuturework.
Theabovediscussionconcernstokenembed-
dings,butBERTistypicallyusedasasentence

ortextencoder.Thestandardwaytogenerate

sentenceortextrepresentationsforclassification

istousethe[CLS]token,butalternativesarealso

beingdiscussed,includingconcatenationoftoken

representations(Tanakaetal.,2020),normalized

mean(Tanakaetal.,2020),andlayeractivations

(Maetal.,2019).SeeToshniwaletal.(2020)fora

systematiccomparisonofseveralmethodsacross

tasksandsentenceencoders.

4.2Self-attentionHeads

Severalstudiesproposedclassificationofattention

headtypes.RaganatoandTiedemann(2018)dis-

cussattendingtothetokenitself,previous/next

tokens,andthesentenceend.Clarketal.(2019)

distinguishbetweenattendingtoprevious/next

tokens,
[CLS]
,
[SEP]
,punctuation,and``at-
tendingbroadly''overthesequence.Kovaleva

etal.(2019)proposefivepatterns,shownin

Figure3.

4.2.1HeadsWithLinguisticFunctions

The ``heterogeneous''attentionpatternshown

inFigure3
could
potentiallybelinguistically
interpretable,andanumberofstudiesfocusedon

identifyingthefunctionsofself-attentionheads. In

particular,
someBERTheadsseemtospecialize
incertaintypesofsyntacticrelations.
Htut
etal.(2019)andClarketal.(2019)reportthat

thereareBERTheadsthatattendedsignificantly

morethanarandombaselinetowordsincertain

syntacticpositions.Thedatasetsandmethods

usedinthesestudiesdiffer,buttheybothfind

thatthereareheadsthatattendtowordsin

obj
rolemorethanthepositional baseline.The
evidencefor
nsubj
,
advmod
,and
amod
varies
betweenthesetwostudies.Theoverallconclusion

isalsosupportedbyVoitaetal.'s(2019b)study

ofthebaseTransformerinmachinetranslation

context.Hooveretal.(2019)hypothesizethateven

complexdependencieslike
dobj
areencodedby
acombinationofheadsratherthanasinglehead,

but thisworkislimitedtoqualitativeanalysis.

ZhaoandBethard(2020)lookedspecificallyfor

theheadsencodingnegationscope.
BothClarketal.(2019)andHtutetal.(2019)
concludethat
nosingleheadhasthecomplete
syntactictreeinformation
,inlinewithevidence
ofpartialknowledgeofsyntax(cf.subsection3.1).

However,Clarketal.(2019)identifyaBERThead

thatcanbedirectlyusedasaclassifiertoperform

coreferenceresolutiononparwitharule-based

system,whichbyitselfwouldseemtorequire

quitealotofsyntacticknowledge.
Linetal.(2019)presentevidencethat
attention
weightsareweakindicatorsofsubject-verb

agreementandreflexiveanaphora.
Insteadof
servingasstrongpointersbetweentokensthat

shouldberelated,BERT'sself-attentionweights

wereclosetoauniformattentionbaseline,but

therewassomesensitivitytodifferenttypesof

distractorscoherentwithpsycholinguisticdata.

ThisisconsistentwithconclusionsbyEttinger

(2019).
Toourknowledge,morphologicalinformation
inBERTheadshasnotbeenaddressed,butwith

thesparseattentionvariantbyCorreiaetal.

(2019)inthebaseTransformer,someattention

headsappeartomergeBPE-tokenizedwords.

Forsemanticrelations,therearereportsofself-

attentionheadsencodingcoreframe-semantic

relations(Kovalevaetal.,2019),aswellaslexi-

cographicandcommonsenserelations(Cuietal.,

2020).
Theoverallpopularityofself-attentionasan
interpretabilitymechanismisduetotheideathat

``attentionweighthasaclearmeaning:howmuch
846
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
aparticularwordwillbeweightedwhencomput-

ingthenextrepresentationforthecurrentword''

(Clarketal.,2019).Thisviewiscurrentlydebated

(JainandWallace,2019;SerranoandSmith,

2019;WiegreffeandPinter,2019;Brunneretal.,

2020),andinamultilayermodelwhereattention

isfollowedbynonlineartransformations,the

patternsinindividualheadsdonotprovideafull

picture.Also,althoughmanycurrentpapersare

accompaniedbyattentionvisualizations,andthere

isagrowingnumberofvisualizationtools(Vig,

2019;Hooveretal.,2019),thevisualizationis

typicallylimitedtoqualitativeanalysis(oftenwith

cherry-pickedexamples)(BelinkovandGlass,

2019),andshouldnotbeinterpretedasdefinitive

evidence.

4.2.2AttentiontoSpecialTokens

Kovalevaetal.(2019)showthat
mostself-
attentionheadsdonotdirectlyencodeany

non-triviallinguisticinformation
,atleastwhen
fine-tunedonGLUE(Wangetal.,2018),since

onlyfewerthan50%ofheadsexhibitthe

``heterogeneous''pattern.Muchofthemodelpro-

ducedtheverticalpattern(attentionto
[CLS]
,
[SEP]
,andpunctuationtokens),consistentwith
theobservationsbyClarketal.(2019).Thisre-

dundancyislikelyrelatedtotheoverparameteri-

zationissue(seesection6).
Morerecently,Kobayashietal.(2020)showed
thatthenormsofattention-weightedinputvectors,

whichyieldamoreintuitiveinterpretationofself-

attention,reducetheattentiontospecialtokens.

However,evenwhentheattentionweightsare

normed,itisstillnotthecasethatmostheads

thatdothe``heavylifting''areevenpotentially

interpretable(Prasannaetal.,2020).
Onemethodologicalchoiceininmanystudies
ofattentionistofocuson inter-wordattention

andsimplyexcludespecialtokens(e.g.,Linetal.

[2019]andHtutetal.[2019]).However,ifatten-

tiontospecialtokensactuallymattersatinference

time,drawingconclusionspurelyfrominter-word

attentionpatternsdoesnotseemwarranted.
Thefunctionsofspecialtokensarenotyetwell
understood.
[CLS]
istypicallyviewedasanag-
gregatedsentence-levelrepresentation(although

alltokenrepresentationsalsocontainatleast

somesentence-levelinformation,asdiscussedin

subsection4.1);inthatcase,wemaynotsee,for

example,fullsyntactictreesininter-wordatten-
tionbecausepartofthatinformationisactually

packedin
[CLS]
.
Clarketal.(2019)experimentwithencoding
WikipediaparagraphswithbaseBERTtoconsider

specificallytheattentiontospecialtokens,noting

thatheadsinearlylayersattendmoreto
[CLS]
,
inmiddlelayersto
[SEP]
,andinfinallayers
toperiodsandcommas.Theyhypothesizethatits

functionmightbeoneof``no-op'',asignalto

ignoretheheadifitspatternisnotapplicableto

thecurrentcase.Asaresult,forexample,
[SEP]
getsincreasedattentionstartinginlayer5,butits

importanceforpredictiondrops.However,after

fine-tuningboth
[SEP]
and
[CLS]
getalotof
attention,dependingonthetask(Kovalevaetal.,

2019).Interestingly,BERTalsopaysalotof

attentiontopunctuation,whichClarketal.(2019)

explainbythefactthatperiodsandcommasare

simplyalmostasfrequentasthespecialtokens,

andsothemodelmightlearntorelyonthemfor

thesamereasons.

4.3BERTLayers

ThefirstlayerofBERTreceivesasinputa

combinationoftoken,segment,andpositional

embeddings.
Itstandstoreasonthat
thelowerlayershave
themostinformationaboutlinearwordorder.

Linetal.(2019)reportadecreaseintheknowledge

oflinearwordorderaroundlayer4inBERT-base.

Thisisaccompaniedbyanincreasedknowledge

ofhierarchicalsentencestructure,asdetectedby

theprobingtasksofpredictingthetokenindex,

themainauxiliaryverbandthesentencesubject.
Thereisawideconsensusinstudieswith
differenttasks,datasets,andmethodologiesthat

syntacticinformationismostprominentinthe

middlelayersofBERT.
4
Hewittand Manning
(2019)hadthemostsuccessreconstructingsyn-

tactictreedepthfromthemiddleBERTlayers(6-9

forbase-BERT,14-19forBERT-large).Goldberg

(2019)reportsthebestsubject-verbagreement

aroundlayers8-9,andtheperformanceonsyntac-

ticprobingtasksusedbyJawaharetal.(2019)also

seemstopeakaroundthemiddle ofthemodel.

Theprominenceofsyntacticinformationinthe

middleBERTlayersisrelatedtoLiuetal.'s
4
TheseBERTresultsarealsocompatiblewithfindings
byVigandBelinkov(2019),whoreportthehighestattention

totokensindependencyrelationsinthemiddlelayersof

GPT-2.847
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Figure4:BERT layertransferability(columns

correspondtoprobingtasks,Liuetal.(2019a).

(2019a)observationthatthemiddlelayersof

Transformersarebest-performingoverallandthe

mosttransferableacrosstasks(seeFigure4).
Thereis
conflictingevidenceaboutsyntactic
chunks
.Tenneyetal.(2019a)concludethat``the
basicsyntacticinformationappearsearlierinthe

networkwhilehigh-levelsemanticfeaturesappear

atthehigherlayers'',drawingparallelsbetween

thisorderandtheorderofcomponentsinatypical

NLPpipelineŠfromPOS-taggingtodependency

parsingtosemanticrolelabeling.Jawaharetal.

(2019)alsoreportthatthelowerlayersweremore

usefulforchunking, whilemiddlelayerswere

moreusefulforparsing.Atthesametime,the

probingexperimentsbyLiuetal.(2019a)find

theopposite:BothPOS-taggingandchunking

wereperformedbestatthemiddlelayers,inboth

BERT-baseandBERT-large.However,allthree

studies usedifferentsuitesofprobingtasks.
ThefinallayersofBERTarethemosttask-
specific
.Inpre-training,thismeansspecificityto
theMLMtask,whichexplainswhythemiddle

layersaremoretransferable(Liuetal.,2019a).In

fine-tuning,itexplainswhythefinallayerschange

themost(Kovalevaetal.,2019),andwhyrestoring

theweightsoflowerlayersoffine-tunedBERT

totheiroriginalvaluesdoesnotdramaticallyhurt

themodelperformance(Haoetal.,2019).
Tenneyetal.(2019a)suggestthatwhereas
syntacticinformationappearsearlyinthemodel

andcanbelocalized,
semanticsisspreadacross
theentiremodel
,whichexplainswhycertain
non-trivialexamplesgetsolvedincorrectlyatfirst

butcorrectlyatthelaterlayers.Thisisrathertobe

expected:Semanticspermeatesalllanguage,and

linguistsdebatewhethermeaninglessstructures

canexistatall(Goldberg,2006,p.166Œ182).But

thisraisesthequestionofwhatstackingmore

TransformerlayersinBERT actuallyachievesin
termsofthespreadofsemanticknowledge,and

whetherthatisbeneficial.Tenneyetal.compared

BERT-baseandBERT-large,andfoundthatthe

overallpatternofcumulativescoregainsisthe

same,onlymorespreadoutinthelargermodel.
NotethatTenneyetal.'s(2019a)experiments
concernsentence-levelsemanticrelations;Cui

etal.(2020)reportthattheencodingofConceptNet

semanticrelationsistheworstintheearlylayers

andincreasestowardsthetop.Jawaharetal.

(2019)place``surfacefeaturesinlowerlayers,

syntacticfeaturesinmiddlelayersandsemantic

featuresinhigherlayers'',buttheirconclusionis

surprising,giventhatonlyonesemantictaskin

thisstudyactuallytoppedatthelastlayer,and

threeotherspeakedaroundthemiddleandthen

considerablydegradedbythefinallayers.

5TrainingBERT

Thissectionreviewstheproposalstooptimizethe

trainingandarchitectureoftheoriginalBERT.

5.1ModelArchitectureChoices

Todate,themostsystematicstudyofBERTar-

chitecturewasperformedbyWangetal.(2019b),

whoexperimentedwiththenumberoflayers,

heads,andmodelparameters,varyingoneoption

andfreezingtheothers.Theyconcludedthat
the
numberofheadswasnotassignificantasthe

numberoflayers
.Thatisconsistentwiththefind-
ingsofVoitaetal.(2019b)andMicheletal.

(2019)(section6),andalsotheobservationby

Liuetal.(2019a)thatthemiddlelayerswerethe

mosttransferable.Largerhiddenrepresentation

sizewasconsistentlybetter,butthegainsvaried

bysetting.
Allinall,
changesinthenumberofheadsand
layersappeartoperformdifferentfunctions
.
Theissueofmodeldepthmustberelatedto

theinformationflowfromthemosttask-specific

layersclosertotheclassifier(Liuetal.,2019a),to

theinitiallayerswhichappeartobethemosttask-

invariant(Haoetal.,2019),andwherethetokens

resembletheinputtokensthemost(Brunneretal.,

2020)(seesubsection4.3).Ifthatisthecase,

adeepermodelhasmorecapacitytoencode

informationthatisnottask-specific.
Ontheotherhand,manyself-attentionheads
in vanillaBERTseemto naturallylearnthesame

patterns(Kovalevaetal.,2019).Thisexplains
848
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
whypruningthemdoesnothavetoomuchimpact.

Thequestionthatarisesfromthisishowfarwe

couldgetwithintentionallyencouragingdiverse

self-attentionpatterns:Theoretically,thiswould

meanincreasingtheamountofinformationinthe

modelwiththesamenumberofweights.Raganato

etal.(2020)showforTransformer-basedmachine

translationwecansimplypre-setthepatternsthat

wealreadyknowthemodelwouldlearn,instead

oflearningthemfromscratch.
VanillaBERTissymmetricandbalancedin
termsofself-attentionandfeed-forwardlayers,but

itmaynothavetobe.ForthebaseTransformer,

Pressetal.(2020)reportbenefitsfrommore

self-attentionsublayersatthebottomandmore

feedforwardsublayersatthetop.

5.2ImprovementstotheTrainingRegime

Liuetal.(2019b)demonstrate
thebenefitsof
large-batchtraining
:With8kexamples,boththe
languagemodelperplexityanddownstreamtask

performanceareimproved.Theyalsopublishtheir

recommendationsforotherparameters.Youetal.

(2019)reportthatwithabatchsizeof32kBERT's

trainingtimecanbesignificantlyreducedwithno

degradationinperformance.Zhouetal.(2019)

observethatthenormalizationofthetrained

[CLS]
tokenstabilizesthetrainingandslightly
improvesperformanceontextclassificationtasks.
Gongetal.(2019)notethat,becauseself-
attentionpatternsinhigherandlowerlayersare

similar,
themodeltrainingcanbedoneina
recursivemanner
,wheretheshallowerversion
istrainedfirstandthenthetrainedparametersare

copiedtodeeperlayers.Sucha``warm-start''can

leadtoa25%fastertrainingwithoutsacrificing

performance.

5.3Pre-trainingBERT

TheoriginalBERTisabidirectionalTransformer

pre-trainedontwotasks:NSPandMLM

(section2).Multiplestudieshavecomeupwith

alternativetrainingobjectives
toimproveon
BERT,andthesecouldbecategorizedasfollows:

Howtomask.
Raffeletal.(2019)system-
aticallyexperimentwithcorruptionrateand

corruptedspanlength.Liuetal.(2019b)

proposediversemasksfortrainingexamples

withinanepoch,whileBaevskietal.(2019)
maskeverytokeninasequenceinsteadof

arandomselection.Clinchantetal.(2019)

replacethe MASKtokenwith
[UNK]
token,
tohelpthemodellearnarepresentationfor

unknownsthatcouldbeusefulfortransla-

tion.Songetal.(2020)maximizetheamount

ofinformationavailabletothemodelby

conditioningonbothmaskedandunmasked

tokens,andlettingthemodelseehowmany

tokensaremissing.

Whattomask.
Maskscanbeappliedto
fullwordsinsteadofword-pieces(Devlin

etal.,2019; Cuietal.,2019).Similarly,we

canmaskspansratherthansingletokens

(Joshietal.,2020),predictinghowmany

aremissing(Lewisetal.,2019).Masking

phrasesandnamedentities(Sunetal.,

2019b)improvesrepresentationofstructured

knowledge.

Wheretomask.
LampleandConneau
(2019)use arbitrarytextstreamsinsteadof

sentencepairsandsubsamplefrequentout-

putssimilartoMikolovetal.(2013).Bao

etal.(2020)combinethestandardautoencod-

ingMLMwithpartiallyautoregressiveLM

objectiveusingspecialpseudomasktokens.

Alternativestomasking.
Raffeletal.(2019)
experimentwithreplacinganddropping

spans;Lewisetal.(2019)exploredeletion,

infilling,sentencepermutationanddocu-

mentrotation;andSunetal.(2019c)predict

whetheratokeniscapitalizedandwhether

itoccursinothersegmentsofthesame

document.Yangetal.(2019)trainondif-

ferentpermutationsofwordorderintheinput

sequence,maximizingtheprobabilityofthe

originalwordorder(cf.the
n
-gramwordor-
derreconstructiontask(Wangetal.,2019a)).

Clarketal.(2020)detectstokensthatwere

replacedbyageneratornetworkratherthan

masked.

NSPalternatives.
RemovingNSPdoesnot
hurtorslightlyimprovesperformance(Liu

et al.,2019b;Joshietal.,2020;Clinchant

etal.,2019).Wangetal.(2019a)and

Chengetal.(2019)replaceNSPwiththe

taskofpredictingboththenextandthe

previoussentences.Lanetal.(2020)replace

thenegativeNSPexamplesbyswapped
849
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
sentencesfrompositiveexamples,ratherthan

sentencesfromdifferentdocuments.ERNIE

2.0includessentencereorderingandsentence

distanceprediction.Baietal.(2020)replace

bothNSPandtokenpositionembeddingsby

acombinationofparagraph,sentence,and

tokenindexembeddings.LiandChoi(2020)

experimentwithutteranceorderprediction

taskformultipartydialogue(andalsoMLM

atthelevelofutterancesandthewhole

dialogue).

Othertasks.
Sunetal.(2019c)propose
simultaneouslearningofseventasks,in-

cludingdiscourserelationclassificationand

predictingwhetherasegmentisrelevantfor

IR.Guuetal.(2020) includealatentknowl-

edgeretrieverinlanguagemodelpretrain-

ing.Wangetal.(2020c)combineMLMwith

aknowledgebasecompletionobjective.Glass

etal.(2020)replaceMLMwithspanpredic-

tiontask(asinextractivequestionanswer-

ing),wherethemodelisexpectedtoprovide

theanswernotfromitsownweights,but

froma
different
passagecontainingthecor-
rectanswer(arelevantsearchenginequery

snippet).
Anotherobvioussourceofimprovementispre-
trainingdata. Severalstudiesexploredthebenefits

ofincreasingthecorpusvolume(Liuetal.,2019b;

Conneauetal.,2019;Baevskietal.,2019)and

longertraining(Liuetal.,2019b).Thedata

alsodoesnothavetoberawtext:Thereisa

numbereffortsto
incorporateexplicitlinguistic
information
,bothsyntactic(Sundararamanetal.,
2019)andsemantic(Zhangetal.,2020).Wu

etal.(2019b)andKumaretal.(2020)include

thelabelforagivensequencefromanannotated

taskdataset.SchickandSch
utze(2020)separately
learnrepresentationsforrarewords.
AlthoughBERTisalreadyactivelyusedasa
sourceofworldknowledge(seesubsection3.3),

thereisalsoworkon
explicitlysupplying
structuredknowledge
.Oneapproachisentity-
enhancedmodels.Forexample,Petersetal.

(2019a);Zhangetal.(2019)includeentity

embeddingsasinputfortrainingBERT,while

Poerneretal.(2019)adaptentityvectorstoBERT

representations.Asmentionedabove,Wangetal.

(2020c)integrateknowledgenotthroughentity
Figure5:Pre-trainedweightshelpBERTfindwider

optimainfine-tuningonMRPC(right)thantraining

fromscratch(left)(Haoetal.,2019).

embeddings,butthroughtheadditionalpre-

trainingobjectiveofknowledgebasecompletion.

Sunetal.(2019b,c)modifythestandardMLMtask

tomasknamedentitiesratherthanrandomwords,

andYinetal.(2020)trainwithMLMobjective

overbothtextandlinearizedtabledata.Wangetal.

(2020a)enhanceRoBERTawithbothlinguistic

andfactualknowledgewithtask-specificadapters.
Pre-trainingisthemostexpensivepartoftrain-
ingBERT,anditwouldbeinformativetoknow

howmuchbenefititprovides.Onsometasks,a

randomlyinitializedandfine-tunedBERTobtains

competitiveorhigherresultsthanthepre-trained

BERTwiththetaskclassifierandfrozenweights

(Kovalevaetal.,2019).Theconsensusinthe

communityisthatpre-trainingdoeshelpinmost

situations,butthedegreeanditsexactcontribution

requiresfurtherinvestigation.Prasannaetal.

(2020)foundthat
most
weightsofpre-trained
BERTareuseful infine-tuning,althoughthere

are``better''and``worse''subnetworks.Oneex-

planationis thatpre-trainedweightshelpthefine-

tunedBERTfindwiderandflatterareaswith

smallergeneralizationerror,whichmakesthe

modelmorerobusttooverfitting(seeFigure5

fromHaoetal.[2019]).
Giventhelargenumberandvarietyofpro-
posedmodifications,onewouldwishtoknowhow

muchimpacteachofthemhas.However,dueto

theoveralltrendtowardslargemodelsizes,syste-

maticablationshavebecomeexpensive.Most

newmodelsclaimsuperiorityonstandardbench-

marks,but gainsareoftenmarginal,andestimates

ofmodelstabilityandsignificancetestingare

veryrare.
850
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
5.4Fine-tuningBERT

Pre-training+fine-tuningworkflowisacrucial

partofBERT.Theformerissupposedtoprovide

task-independentknowledge,andthelatterwould

presumablyteachthemodeltorelymoreonthe

representationsusefulforthetaskathand.
Kovalevaetal.(2019)didnotfindthattobethe
caseforBERTfine-tunedonGLUEtasks:
5
during
fine-tuning,themostchangesforthreeepochs

occurredinthelasttwolayersofthemodels,but

thosechangescausedself-attentiontofocuson

[SEP]
ratherthanonlinguisticallyinterpretable
patterns.Itisunderstandablewhyfine-tuning

wouldincreasetheattentionto
[CLS]
,butnot
[SEP]
.IfClarketal.(2019)arecorrectthat
[SEP]
servesas``no-op''indicator,fine-tuning
basicallytellsBERTwhattoignore.
Severalstudiesexploredthepossibilitiesof
improvingthefine-tuningofBERT:

Takingmorelayersintoaccount
:learning
acomplementaryrepresentationoftheinfor-

mationindeepandoutputlayers(Yangand

Zhao,2019),usingaweightedcombination

ofalllayersinsteadofthefinalone(Suand

Cheng,2019;KondratyukandStraka,2019),

andlayerdropout(KondratyukandStraka,

2019).

Two-stagefine-tuning
introducesaninter-
mediatesupervisedtrainingstagebetween

pre-trainingandfine-tuning(Phangetal.,

2019;Gargetal.,2020;AraseandTsujii,

2019;Pruksachatkunetal.,2020;Glava

s
andVuli

c,2020).Ben-Davidetal.(2020)
proposeapivot-basedvariantofMLMto

fine-tuneBERTfordomainadaptation.

Adversarialtokenperturbations
improve
therobustnessofthemodel(Zhuetal.,2019).

Adversarialregularization
incombination
with
BregmanProximalPointOptimization
helpsalleviatepre-trainedknowledgeforget-

tingandthereforepreventsBERTfrom

overfittingtodownstreamtasks(Jiangetal.,

2019a).

Mixoutregularization
improvesthestab-
ilityofBERTfine-tuningevenforasmall
5
KondratyukandStraka(2019)suggestthatfine-tuning
onUniversalDependenciesdoesresultinsyntactically

meaningfulattentionpatterns,buttherewasnoquantitative

evaluation.numberoftrainingexamples(Leeetal.,

2019).
Withlargemodels,evenfine-tuningbecomes
expensive,butHoulsbyetal.(2019)showthat

itcanbesuccessfullyapproximatedwithadapter

modules.Theyachievecompetitiveperformance

on26classificationtasksatafractionofthecom-

putational cost.AdaptersinBERTwerealsoused

formultitasklearning(SticklandandMurray,

2019)andcross-lingualtransfer(Artetxeetal.,

2019).Analternativetofine-tuningisextracting

featuresfromfrozenrepresentations,butfine-

tuningworksbetterforBERT(Petersetal.,

2019b).
Abigmethodologicalchallengeinthe
currentNLPisthatthereportedperformance

improvementsofnewmodelsmaywellbewithin

variationinducedbyenvironmentfactors(Crane,

2018).BERTisnotanexception.Dodgeetal.

(2020)reportsignificantvariationforBERT

fine-tunedonGLUEtasksduetobothweight

initializationandtrainingdataorder.Theyalso

proposeearlystoppingontheless-promising

seeds.
Althoughwehopethattheaboveobservations
maybeusefulforthepractitioners,thissection

doesnotexhaustthecurrentresearchonfine-

tuninganditsalternatives.Forexample,wedonot

coversuchtopicsasSiamesearchitectures,policy

gradienttraining,automatedcurriculumlearning,

andothers.

6HowBigShouldBERTBe?

6.1Overparameterization

Transformer-basedmodelskeepgrowingbyor-

dersofmagnitude:The110Mparametersofbase

BERTarenowdwarfedby17Bparametersof

Turing-NLG(Microsoft,2020),whichisdwarfed

by175BofGPT-3(Brownetal.,2020).Thistrend

raisesconcernsaboutcomputationalcomplexity

ofself-attention(Wuetal., 2019a),environmental

issues(Strubelletal.,2019;Schwartzetal.,2019),

faircomparisonofarchitectures(Aˇenmacher

andHeumann,2020),andreproducibility.
Humanlanguageisincrediblycomplex,and
wouldperhapstakemanymoreparametersto

describefully,butthecurrentmodelsdonotmake

gooduseoftheparameterstheyalreadyhave.

Voitaetal.(2019b)showedthat
allbutafew
Transformerheadscouldbeprunedwithout
851
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Compression PerformanceSpeedupModelEvaluation
BERT-base(Devlinetal.,2019)

1100%

1BERT
12
AllGLUEtasks,SQuAD
BERT-small

3.891%

BERT
4
y
AllGLUEtasks
Distillation
DistilBERT(Sanhetal.,2019)

1.590%
x

1.6BERT
6
AllGLUEtasks,SQuAD
BERT
6
-PKD(Sunetal.,2019a)

1.698%

1.9BERT
6
NoWNLI,CoLA,STS-B;RACE
BERT
3
-PKD(Sunetal.,2019a)

2.492%

3.7BERT
3
NoWNLI,CoLA,STS-B;RACE
Aguilaretal.(2019),Exp.3

1.693%

BERT
6
CoLA,MRPC,QQP,RTE
BERT-48(Zhaoetal.,2019)

62 87%

77BERT
12
y
MNLI,MRPC,SST-2
BERT-192(Zhaoetal.,2019)

5.793%

22BERT
12
y
MNLI,MRPC,SST-2
TinyBERT(Jiaoetal.,2019)

7.596%

9.4BERT
4
y
NoWNLI;SQuAD
MobileBERT (Sunetal.,2020)

4.3100%

4BERT
24
y
NoWNLI;SQuAD
PD(Turcetal.,2019)

1.698%

2
:
5
z
BERT
6
y
NoWNLI,CoLAandSTS-B
WaLDORf (Tianetal.,2019)

4.493%

9BERT
8
yk
SQuAD
MiniLM(Wangetal.,2020b)

1.65 99%

2BERT
6
NoWNLI,STS-B,MNLI
mm
;SQuAD
MiniBERT(Tsaietal.,2019)

6

98%

27

mBERT
3
y
CoNLL-18POSandmorphology
BiLSTM-soft (Tangetal.,2019)

11091%

434
z
BiLSTM
1
MNLI,QQP,SST-2
Quanti-

zationQ-BERT-MP(Shenetal.,2019)

1398%
{

BERT
12
MNLI,SST-2,CoNLL-03,SQuAD
BERT-QAT(Zafriretal.,2019)

499%

BERT
12
NoWNLI,MNLI;SQuAD
GOBO(ZadehandMoshovos,2020)

9
:
8
99%

BERT
12
MNLI
Pruning
McCarleyetal.(2020),ff2

2
:
2
z
98%
z

1
:
9
z
BERT
24
SQuAD,NaturalQuestions
RPP(Guoetal.,2019)

1
:
7
z
99%
z

BERT
24
NoWNLI,STS-B;SQuAD
SoftMvP(Sanhetal.,2020)

3394%
{

BERT
12
MNLI,QQP,SQuAD
IMP(Chenetal.,2020),rewind50%

1.4Œ2.5 94Œ100%

BERT
12
NoMNLI-mm;SQuAD
Other
ALBERT-base(Lanetal.,2020)

997%

BERT
12
y
MNLI,SST-2
ALBERT-xxlarge(Lanetal.,2020)

0.47107%

BERT
12
y
MNLI,SST-2
BERT-of-Theseus(Xuetal.,2020)

1.698%

1.9BERT
6
NoWNLI
PoWER-BERT(Goyaletal.,2020)N/A 99%

2Œ4.5BERT
12
NoWNLI;RACE
Table1:ComparisonofBERTcompressionstudies.Compression,
performanceretention,andinference
timespeedupfiguresaregivenwithrespect toBERT
base
,unlessindicatedotherwise.Performance
retentionismeasuredasaratioofaveragescoresachievedbyagiv
enmodelandbyBERT
base
.The
subscriptinthemodeldescriptionreflectsthenumberoflayersu
sed.

Smallervocabularyused.
y
The
dimensionalityofthehiddenlayersisreduced.
k
Convolutionallayersused.
z
ComparedtoBERT
large
.

ComparedtomBERT.
x
AsreportedinJiaoetal.(2019)
:
{
Incomparisontothedevset.
significantlossesinperformance
.ForBERT,
Clarketal.(2019)observethatmostheadsin

thesamelayershowsimilarself-attentionpatterns

(perhapsrelatedtothefactthattheoutputofall

self-attentionheadsina layerispassedthrough

thesameMLP),whichexplainswhyMicheletal.

(2019)wereabletoreducemostlayerstoasingle

head.
Dependingonthetask,someBERTheads/
layersarenot onlyredundant(Kaoetal.,

2020),butalsoharmfultothedownstreamtask

performance.
Positiveeffectfromheaddisabling
wasreportedformachinetranslation(Micheletal.,

2019),abstractivesummarization(Baanetal.,

2019),andGLUEtasks(Kovalevaetal.,2019).

Additionally,Tenneyetal.(2019a)examine

thecumulativegainsoftheirstructuralprobing

classifier,observingthatin5out of8probing

taskssomelayerscauseadropinscores(typically

inthefinallayers).Gordonetal.(2020)findthat

30%Œ40%oftheweightscanbeprunedwithout

impactondownstreamtasks.
Ingeneral,largerBERTmodelsperformbetter
(Liuetal.,2019a;Robertsetal.,2020),butnot

always:BERT-baseoutperformedBERT-large

onsubject-verbagreement(Goldberg,2019)and

sentencesubjectdetection(Linetal.,2019).Given

thecomplexityoflanguage,andamountsofpre-

trainingdata,itisnotclearwhyBERTends

upwithredundantheadsandlayers.Clarketal.

(2019)suggestthatonepossiblereasonistheuse

ofattentiondropouts,whichcausessomeattention

weightstobezeroed-outduringtraining.

6.2CompressionTechniques

Giventheaboveevidenceofoverparameteriza-

tion,itdoesnotcomeasasurprisethat
BERTcan
beefficientlycompressedwithminimalaccu-

racyloss
,whichwouldbehighlydesirablefor
real-worldapplications.Sucheffortstodateare

summarizedinTable1.Themainapproachesare

knowledgedistillation,quantization,andpruning.
Thestudiesinthe
knowledgedistillation
framework
(Hintonetal.,2014)useasmaller
852
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
student-networktrainedtomimicthebehaviorof

alargerteacher-network.ForBERT,thishasbeen

achievedthroughexperimentswith lossfunctions

(Sanhetal.,2019;Jiaoetal.,2019),mimick-

ingtheactivationpatternsofindividualportions

oftheteachernetwork(Sunetal.,2019a),and

knowledgetransferatthepre-training(Turcetal.,

2019;Jiaoetal.,2019;Sunetal.,2020)orfine-

tuningstage(Jiaoetal.,2019).McCarleyetal.

(2020)suggestthatdistillationhassofarworked

betterforGLUEthanforreadingcomprehen-

sion,andreportgoodresultsforQAfromacom-

binationofstructuredpruningandtask-specific

distillation.
Quantization
decreasesBERT'smemory
footprintthroughloweringtheprecisionofits

weights(Shenetal.,2019;Zafriretal.,2019).

Notethatthisstrategyoftenrequirescompatible

hardware.
Asdiscussedinsection6,individualself-
attentionheadsandBERTlayerscanbedisabled

withoutsignificantdropinperformance(Michel

etal.,2019;Kovalevaetal.,2019;Baanetal.,

2019).
Pruning
isacompressiontechniquethat
takesadvantageofthatfact,typicallyreducingthe

amountofcomputationviazeroingoutofcertain

partsofthelargemodel.Instructuredpruning,

architectureblocksaredropped,asinLayerDrop

(Fanetal.,2019).Inunstructured,theweightsin

theentiremodelareprunedirrespectiveoftheir

location,asinmagnitudepruning(Chenetal.,

2020)ormovementpruning(Sanhetal.,2020).
Prasannaetal.(2020)andChenetal.(2020)
exploreBERTfromtheperspectiveofthelot-

terytickethypothesis(FrankleandCarbin,2019),

lookingspecificallyatthe``winning''subnet-

worksinpre-trainedBERT.Theyindependently

findthatsuchsubnetworksdoexist,andthattrans-

ferabilitybetweensubnetworksfordifferenttasks

varies.
IftheultimategoaloftrainingBERTiscom-
pression,Lietal.(2020)recommendtraining

largermodelsandcompressingthemheavily

ratherthancompressingsmallermodelslightly.
OthertechniquesincludedecomposingBERT's
embeddingmatrixintosmallermatrices(Lanetal.,

2020),progressivemodulereplacing(Xuetal.,

2020),anddynamiceliminationofintermediate

encoderoutputs(Goyaletal.,2020).SeeGanesh

etal.(2020)foramoredetaileddiscussionof

compressionmethods.
6.3PruningandModelAnalysis

Thereisanascentdiscussionaroundpruningasa

modelanalysistechnique. Thebasicideaisthat

acompressedmodelaprioriconsistsofelements

thatareusefulforprediction;thereforebyfinding

outwhattheydowemayfindoutwhatthewhole

networkdoes.Forinstance,BERThasheads

thatseemtoencodeframe-semanticrelations,but

disablingthemmightnothurtdownstreamtask

performance(Kovalevaetal.,2019);thissuggests

thatthisknowledgeisnotactuallyused.
ForthebaseTransformer,Voitaetal.(2019b)
identifythefunctionsofself-attentionheadsand

thencheckwhichofthemsurvivethepruning,

findingthatthesyntacticandpositionalheadsare

thelastonestogo.ForBERT,Prasannaetal.

(2020)gointheoppositedirection:pruningonthe

basisofimportancescores,andinterpretingthe

remaining``good''subnetwork.Withrespectto

self-attentionheadsspecifically,itdoesnotseem

tobethecasethatonlytheheadsthatpotentially

encodenon-triviallinguisticpatternssurvivethe

pruning.
Themodelsandmethodologyinthesestud-
iesdiffer,sotheevidenceisinconclusive.In

particular,Voitaetal.(2019b)findthatbefore

pruningthemajorityofheadsaresyntactic,and

Prasannaetal.(2020)findthatthemajorityof

headsdonothavepotentiallynon-trivialattention

patterns.
Animportantlimitationofthecurrenthead
andlayerablationstudies(Micheletal.,2019;

Kovalevaetal.,2019)isthattheyinherently

assumethatcertainknowledgeiscontainedin

heads/layers.However,thereisevidenceof

morediffuserepresentationsspreadacrossthe

fullnetwork,suchasthegradualincreasein

accuracyondifficultsemanticparsingtasks

(Tenneyetal.,2019a)ortheabsenceof

headsthatwouldperformparsing``ingeneral''

(Clarketal.,2019;Htutetal.,2019).If

so,ablatingindividualcomponentsharmsthe

weight-sharingmechanism.Conclusionsfrom

componentablationsarealsoproblematicifthe

sameinformationisduplicatedelsewhereinthe

network.

7DirectionsforFurtherResearch

BERTologyhasclearlycomealongway,butit

isfairtosaywestillhavemorequestionsthan

answersabouthowBERTworks.Inthissection,
853
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
welistwhatwebelievetobethemostpromising

directionsforfurtherresearch.

Benchmarksthatrequireverbalreasoning.

AlthoughBERTenabledbreakthroughsonmany

NLPbenchmarks,agrowinglistofanalysispapers

areshowingthatitslanguageskillsarenotas

impressiveastheyseem.Inparticular,theywere

showntorelyonshallowheuristicsinnaturallan-

guageinference(McCoyetal.,2019b;Zellers

etal.,2019;Jinetal.,2020),readingcompre-

hension(Sietal.,2019;Rogersetal.,2020;

Sugawaraetal.,2020;Yogatamaetal.,2019),

argumentreasoningcomprehension(Nivenand

Kao,2019),andtextclassification(Jinetal.,

2020).Suchheuristicscanevenbeusedtorecon-

structanonŒpubliclyavailablemodel(Krishna

etal.,2020).Aswithanyoptimizationmethod,if

thereisashortcutinthedata,wehavenoreason

toexpectBERTtonotlearnit.Butharder datasets

thatcannotberesolvedwithshallowheuristicsare

unlikelytoemergeiftheirdevelopmentisnotas

valuedasmodelingwork.

Benchmarksforthefullrangeoflinguistic

competence.
Althoughthelanguagemodels
seemtoacquireagreatdealofknowledgeabout

language,wedonotcurrentlyhavecomprehensive

stresstestsfordifferentaspectsoflinguistic

knowledge.Astepinthisdirectionisthe

``Checklist''behavioraltesting(Ribeiroetal.,

2020),thebestpaperatACL2020.Ideally,such

testswouldmeasurenotonlyerrors,butalso

sensitivity(Ettinger,2019).

Developingmethodsto``teach''reasoning.

Whilelargepre-trainedmodelshavealotofknow-

ledge, theyoftenfailifanyreasoningneedstobe

performedontopofthefactstheypossess(Talmor

etal.,2019,seealsosubsection3.3).Forinstance,

Richardsonetal.(2020)proposeamethodto

``teach''BERTquantification,conditionals,com-

paratives,andBooleancoordination.

Learningwhathappensatinferencetime.

MostBERTanalysispapersfocusondifferent

probesofthemodel,withthegoaltofindwhat

thelanguagemodel``knows''.However,probing

studieshavelimitations(subsection3.4),andto

thispoint,farfewerpapershavefocusedon

discoveringwhatknowledgeactuallygetsused.

Severalpromisingdirectionsarethe``amnesic

probing''(Elazaretal.,2020),identifying
featuresimportantforpredictionforagiventask

(ArkhangelskaiaandDutta,2019),andpruningthe

modeltoremovethenon-importantcomponents

(Voitaetal.,2019b;Micheletal.,2019;Prasanna

etal.,2020).

8Conclusion

Inalittleoverayear,BERThasbecomea

ubiquitousbaselineinNLPexperimentsand

inspirednumerousstudiesanalyzingthemodel

andproposingvariousimprovements.Thestream

ofpapersseemstobeacceleratingratherthan

slowingdown,andwehopethatthissurveyhelps

the communitytofocusonthebiggestunresolved

questions.

9Acknowledgments

Wethanktheanonymousreviewersfortheir

valuablefeedback.Thisworkisfundedinpart

byNSFawardnumberIIS-1844740toAnna

Rumshisky.

References

GustavoAguilar,YuanLing,YuZhang,Benjamin
Yao,XingFan,andEdwardGuo.2019.Knowl-

edgeDistillationfromInternalRepresentations.

arXivpreprintarXiv:1910.03723
.
AlanAkbik,TanjaBergmann,andRoland
Vollgraf.2019.PooledContextualizedEmbed-

dingsforNamedEntityRecognition.In
Pro-
ceedingsofthe2019ConferenceoftheNorth

AmericanChapteroftheAssociationforCom-

putationalLinguistics:HumanLanguageTech-

nologies,Volume1(LongandShortPapers)
,
pages724Œ728,Minneapolis,Minnesota.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/N19

-1078
YukiAraseandJun'ichiTsujii.2019.Transfer
Fine-Tuning:ABERTCaseStudy.In
Pro-
ceedingsofthe2019ConferenceonEmpirical

MethodsinNaturalLanguageProcessingand

the9thInternationalJointConferenceonNat-

uralLanguageProcessing(EMNLP-IJCNLP)
,
pages5393Œ5404,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1542
854
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
EkaterinaArkhangelskaiaandSouravDutta.
2019.Whatchalookin'at?DeepLIFTing

BERT'sAttentioninQuestionAnswering.

arXivpreprintarXiv:1910.06431
.
MikelArtetxe,SebastianRuder,andDani
Yogatama.2019.OntheCross-lingualTrans-

ferabilityofMonolingualRepresentations.

arXiv:1911.03310[cs]
.
DOI:
https://doi
.org/10.18653/v1/2020.acl-main.421
MatthiasAˇenmacherandChristianHeumann.
2020.OnthecomparabilityofPre-Trained

LanguageModels.
arXiv:2001.00781[cs,stat]
.
JorisBaan,MaartjeterHoeve,Marliesvander
Wees,AnneSchuth,andMaartendeRijke.

2019.UnderstandingMulti-HeadAttention

inAbstractiveSummarization.
arXivpreprint
arXiv:1911.03898
.
AlexeiBaevski,SergeyEdunov,YinhanLiu,
LukeZettlemoyer,andMichaelAuli.2019.

Cloze-drivenPretrainingofSelf-Attention

Networks.In
Proceedingsofthe2019Confer-
enceonEmpiricalMethodsinNaturalLan-

guageProcessingandthe9thInternational

JointConferenceonNaturalLanguagePro-

cessing(EMNLP-IJCNLP)
,pages5360Œ5369,
HongKong,China.AssociationforCompu-

tationalLinguistics.
DOI:
https://doi.org
/10.18653/v1/D19-1539
HeBai,PengShi,JimmyLin,LuchenTan,Kun
Xiong,WenGao,andMingLi.2020.Sega

BERT:Pre-trainingofSegment-awareBERT

forLanguageUnderstanding.
arXiv:2004.
14996[cs]
.
SriramBalasubramanian,NamanJain,Gaurav
Jindal,AbhijeetAwasthi,andSunitaSarawagi.

2020.What'sinaName?AreBERTNamed

EntityRepresentationsjustasGoodfor

anyotherName?In
Proceedingsofthe
5thWorkshoponRepresentationLearning

forNLP
,pages205Œ214,Online.Associ-
ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/2020

.repl4nlp-1.24
HangboBao,LiDong,FuruWei,WenhuiWang,
NanYang,XiaodongLiu,YuWang,Songhao

Piao,JianfengGao,MingZhou,andHsiao-

WuenHon.2020.UniLMv2:Pseudo-Masked
LanguageModelsforUnifiedLanguageModel

Pre-Training.
arXiv:2002.12804[cs]
.
YonatanBelinkovandJamesGlass.2019.Anal-
ysisMethodsinNeuralLanguageProcessing:

ASurvey.
TransactionsoftheAssociation
forComputationalLinguistics
,7:49Œ72.
DOI:
https://doi.org/10.1162/tacl
a
00254
EyalBen-David,CarmelRabinovitz,andRoi
Reichart.2020.PERL:Pivot-basedDomain

AdaptationforPre-trainedDeepContextual-

izedEmbeddingModels.
arXiv:2006.09075
[cs]
.
DOI:
https://doi.org/10.1162
/tacl
a
00328
RishiBommasani,KellyDavis,andClaire
Cardie.2020.InterpretingPretrainedContex-

tualizedRepresentationsviaReductionsto

StaticEmbeddings.In
Proceedingsofthe58th
AnnualMeetingoftheAssociationforCompu-

tationalLinguistics
,pages4758Œ4781.
DOI:
https://doi.org/10.18653/v1/2020

.acl-main.431
ZiedBouraoui,JoseCamacho-Collados,and
StevenSchockaert.2019.InducingRelational

KnowledgefromBERT.
arXiv:1911.12753
[cs]
.
DOI:
https://doi.org/10.1609
/aaai.v34i05.6242
SamuelBroscheit.2019.InvestigatingEntity
Knowledgein BERTwithSimpleNeural

End-To-EndEntityLinking.In
Proceedings
ofthe23rdConferenceonComputational

NaturalLanguageLearning(CoNLL)
,
pages677Œ685,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/K19

-1063
TomB.Brown,BenjaminMann,NickRyder,
MelanieSubbiah,JaredKaplan,Prafulla

Dhariwal,ArvindNeelakantan,PranavShyam,

GirishSastry,AmandaAskell,Sandhini

Agarwal,ArielHerbert-Voss,Gretchen

Krueger,TomHenighan,RewonChild,Aditya

Ramesh,Daniel M.Ziegler,JeffreyWu,

ClemensWinter,ChristopherHesse,Mark

Chen,EricSigler,MateuszLitwin,Scott

Gray,BenjaminChess,JackClark,Christopher

Berner,SamMcCandlish,AlecRadford,

IlyaSutskever,andDarioAmodei.2020.
855
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Language ModelsareFew-ShotLearners.

arXiv:2005.14165[cs]
.
GinoBrunner,YangLiu,DamianPascual,
OliverRichter,MassimilianoCiaramita,and

RogerWattenhofer.2020.OnIdentifiabilityin

Transformers.In
InternationalConferenceon
LearningRepresentations
.
TianlongChen,JonathanFrankle,ShiyuChang,
SijiaLiu,YangZhang,ZhangyangWang,and

MichaelCarbin.2020.TheLotteryTicket

HypothesisforPre-trainedBERTNetworks.

arXiv:2007.12223[cs,stat]
.
XingyiCheng,WeidiXu,KunlongChen,Wei
Wang,BinBi,MingYan,ChenWu,LuoSi,

WeiChu,andTaifengWang.2019.Symmetric

RegularizationbasedBERTforPair-Wise

SemanticReasoning.
arXiv:1909.03405[cs]
.
KevinClark,UrvashiKhandelwal,OmerLevy,
andChristopherD.Manning.2019.What

DoesBERTLookat?AnAnalysisof

BERT'sAttention.In
Proceedingsofthe
2019ACLWorkshopBlackboxNLP:Analyzing

andInterpretingNeuralNetworksforNLP
,
pages276Œ286,Florence,Italy.Association

forComputationalLinguistics.
DOI:
https://
doi.org/10.18653/v1/W19-4828
,
PMID:
31709923
KevinClark,Minh-ThangLuong,QuocV.Le,
andChristopherD.Manning.2020.ELECTRA:

Pre-TrainingTextEncodersasDiscriminators

RatherThanGenerators.In
International
ConferenceonLearningRepresentations
.
StephaneClinchant,KweonWooJung,and
VassilinaNikoulina.2019.OntheuseofBERT

forNeuralMachineTranslation.In
Proceedings
ofthe3rdWorkshoponNeuralGenerationand

Translation
,pages108Œ117,HongKong.Asso-
ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-5611
AlexisConneau,KartikayKhandelwal,Naman
Goyal,VishravChaudhary,GuillaumeWenzek,

FranciscoGuzm

an,EdouardGrave,MyleOtt,
LukeZettlemoyer,andVeselinStoyanov.

2019.UnsupervisedCross-LingualRepresen-

tationLearningatScale.
arXiv:1911.02116
[cs]
.
DOI:
https://doi.org/10.18653
/v1/2020.acl-main.747
Gonc˘aloM.Correia,VladNiculae,andAndr

e
F.T.Martins.2019.AdaptivelySparseTrans-

formers.In
Proceedingsofthe2019Conference
onEmpiricalMethodsinNaturalLan-

guageProcessingandthe9thInternational

JointConferenceonNaturalLanguagePro-

cessing(EMNLP-IJCNLP)
,pages2174Œ2184,
HongKong,China. AssociationforCompu-

tationalLinguistics.
DOI:
https://doi
.org/10.18653/v1/D19-1223
MattCrane.2018.QuestionableAnswersin
QuestionAnsweringResearch:Reproducibility

andVariabilityofPublishedResults.
Trans-
actionsoftheAssociationforComputational

Linguistics
,6:241Œ252.
DOI:
https://doi
org/10.1162/tacl
a
00018
LeyangCui,SijieCheng,YuWu,andYue
Zhang.2020.DoesBERTSolveCommon-

senseTaskviaCommonsenseKnowledge?

arXiv:2008.03945[cs]
.
YimingCui,WanxiangChe,TingLiu,BingQin,
ZiqingYang,ShijinWang,andGuopingHu.

2019.Pre-TrainingwithWholeWordMasking

forChineseBERT.
arXiv:1906.08101[cs]
.
JeffDaandJungoKasai.2019.Crackingthe
ContextualCommonsenseCode:Understand-

ingCommonsenseReasoningAptitudeof

DeepContextualRepresentations.In
Proceed-
ingsoftheFirstWorkshoponCommonsense

InferenceinNaturalLanguageProcessing
,
pages1Œ12,HongKong,China.Association

forComputationalLinguistics.
JoeDavison,JoshuaFeldman,andAlexander
Rush.2019.CommonsenseKnowledgeMining

fromPretrainedModels.In
Proceedingsof
the2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages1173Œ1178,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1109
JacobDevlin,Ming-WeiChang,KentonLee,
andKristinaToutanova.2019.BERT:Pre-

trainingofDeepBidirectionalTransformers

forLanguageUnderstanding.In
Proceedings
ofthe2019ConferenceoftheNorth
856
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
AmericanChapteroftheAssociationfor

ComputationalLinguistics:HumanLanguage

Technologies,Volume1(LongandShort

Papers)
,pages4171Œ4186.
JesseDodge,GabrielIlharco,RoySchwartz,
AliFarhadi,HannanehHajishirzi,andNoah

Smith.2020.Fine-TuningPretrainedLanguage

Models:WeightInitializations,DataOrders,

andEarlyStopping.
arXiv:2002.06305[cs]
.
YanaiElazar,ShauliRavfogel,AlonJacovi,and
YoavGoldberg.2020.WhenBertForgets

HowToPOS:AmnesicProbingofLinguistic

PropertiesandMLMPredictions.
arXiv:2006.
00995[cs]
.
KawinEthayarajh.2019.HowContextual
areContextualizedWordRepresentations?

ComparingtheGeometryofBERT,ELMo,

andGPT-2Embeddings.In
Proceedingsof
the2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages55Œ65,HongKong,China.Associ-

ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1006
AllysonEttinger.2019.WhatBERTisnot:
Lessonsfromanewsuiteofpsycholinguis-

ticdiagnosticsforlanguagemodels.
arXiv:
1907.13528[cs]
.
DOI:
https://doi.org
/10.1162/tacl
a
00298
AngelaFan,EdouardGrave,andArmandJoulin.
2019.ReducingTransformerDepthonDemand

withStructuredDropout.In
International
ConferenceonLearningRepresentations
.
MaxwellForbes,AriHoltzman,andYejinChoi.
2019.DoNeuralLanguageRepresentations

LearnPhysicalCommonsense?In
Proceedings
ofthe41stAnnualConferenceoftheCognitive

ScienceSociety(CogSci2019)
,page7.
JonathanFrankleandMichaelCarbin.2019.The
LotteryTicketHypothesis:FindingSparse,

TrainableNeuralNetworks.In
International
ConferenceonLearningRepresentations
.
PrakharGanesh,Yao Chen,XinLou,
MohammadAliKhan,YinYang,Deming
Chen,MarianneWinslett,HassanSajjad,and

PreslavNakov.2020.Compressinglarge-scale

transformer-basedmodels:Acasestudyon

BERT.
arXivpreprintarXiv:2002.11985
.
SiddhantGarg,ThuyVu,andAlessandro
Moschitti.2020.TANDA:TransferandAdapt

Pre-TrainedTransformerModelsforAnswer

SentenceSelection.In
AAAI
.
DOI:
https://
doi.org/10.1609/aaai.v34i05.6282
MichaelGlass,AlfioGliozzo, RishavChakravarti,
AnthonyFerritto,LinPan,G.P.Shrivatsa

Bhargav,DineshGarg,andAviSil.2020.

SpanSelectionPre-trainingforQuestion

Answering.In
Proceedingsofthe58thAnnual
MeetingoftheAssociationforComputational

Linguistics
,pages2773Œ2782,Online.Asso-
ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/2020

.acl-main.247
GoranGlava

sandIvanVuli

c.2020.IsSupervised
SyntacticParsingBeneficialforLanguage

Understanding?AnEmpiricalInvestigation.

arXiv:2008.06788[cs]
.
AdeleGoldberg.2006.
ConstructionsatWork:
TheNatureofGeneralizationinLanguage
,
OxfordUniversityPress,USA.
YoavGoldberg.2019.AssessingBERT'ssyntac-
ticabilities.
arXivpreprintarXiv:1901.05287
.
LinyuanGong,DiHe,ZhuohanLi,TaoQin,
LiweiWang,andTieyanLiu.2019.Efficient

trainingofBERTbyprogressivelystacking.

In
InternationalConferenceonMachine
Learning
,pages2337Œ2346.
MitchellA. Gordon,KevinDuh,andNicholas
Andrews.2020.CompressingBERT:Studying

theeffectsofweightpruningontransfer

learning.
arXivpreprintarXiv:2002.08307
.
SaurabhGoyal,AnamitraRoyChoudhary,
VenkatesanChakaravarthy,SaurabhManishRaje,

YogishSabharwal,andAshishVerma.2020.

Power-bert:AcceleratingBERTinferencefor

classificationtasks.
arXivpreprintarXiv:2001.
08950
.
Fu-Ming Guo,SijiaLiu,FinlayS.Mungall,Xue
Lin,andYanzhiWang.2019.Reweighted

ProximalPruningforLarge-ScaleLanguage

Representation.
arXiv:1909.12486[cs,stat]
.
857
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
KelvinGuu,KentonLee,ZoraTung,Panupong
Pasupat,andMing-WeiChang.2020.REALM:

Retrieval-AugmentedLanguageModelPre-

Training.
arXiv:2002.08909[cs]
.
YaruHao,LiDong,FuruWei,andKeXu.
2019.VisualizingandUnderstandingthe

EffectivenessofBERT.In
Proceedingsof
the2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages4143Œ4152,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1424
JohnHewittandChristopherD.Manning.2019.
AStructuralProbeforFindingSyntaxinWord

Representations.In
Proceedingsofthe2019
ConferenceoftheNorthAmericanChapterof

theAssociationforComputationalLinguistics:

HumanLanguageTechnologies,Volume1

(LongandShortPapers)
,pages4129Œ4138.
GeoffreyHinton,OriolVinyals,andJeffDean.
2014.DistillingtheKnowledgeinaNeural

Network.In
DeepLearningandRepresentation
LearningWorkshop:NIPS2014
.
BenjaminHoover,HendrikStrobelt,and
SebastianGehrmann.2019.exBERT:AVisual

AnalysisTooltoExploreLearnedRepresen-

tationsinTransformersModels.
arXiv:1910.
05276[cs]
.
DOI:
https://doi.org/10
.18653/v1/2020.acl-demos.22
NeilHoulsby,AndreiGiurgiu,Stanislaw
Jastrzebski,BrunaMorrone,Quentinde

Laroussilhe,AndreaGesmundo,Mona

Attariyan,andSylvainGelly.2019.Parameter-

EfficientTransferLearningforNLP.
arXiv:
1902.00751[cs,stat]
.
PhuMonHtut,JasonPhang,ShikhaBordia,and
SamuelR.Bowman.2019.Doattentionheads

inBERTtracksyntacticdependencies?
arXiv
preprintarXiv:1911.12246
.
SarthakJainandByronC.Wallace.2019.
AttentionisnotExplanation.In
Proceedings
ofthe2019ConferenceoftheNorthAmerican

ChapteroftheAssociationforComputational

Linguistics:HumanLanguageTechnologies,
Volume1(LongandShortPapers)
,
pages3543Œ3556.
GaneshJawahar,Beno
^
tSagot,Djam

eSeddah,
SamuelUnicomb,GerardoI
~
niguez,M

arton
Karsai,YannickL

eo,M

artonKarsai,Carlos
Sarraute,

EricFleury,etal.2019.Whatdoes
BERTlearnaboutthestructureoflanguage?

In
57thAnnualMeetingoftheAssociation
forComputationalLinguistics(ACL),Florence,

Italy
.
DOI:
https://doi.org/10.18653
/v1/P19-1356
HaomingJiang,PengchengHe,WeizhuChen,
XiaodongLiu,JianfengGao,andTuoZhao.

2019a.SMART:RobustandEfficientFine-

TuningforPre-trainedNaturalLanguage

ModelsthroughPrincipledRegularizedOpti-

mization.
arXivpreprintarXiv:1911.03437
.
DOI:
https://doi.org/10.18653/v1
/2020.acl-main.197
,
PMID:
33121726,
PMCID:
PMC7218724
ZhengbaoJiang,FrankF.Xu,JunAraki,and
GrahamNeubig.2019b.HowCanWeKnow

WhatLanguageModelsKnow?
arXiv:1911.
12543[cs]
.
DOI:
https://doi.org/10
.1162/tacl
a
00324
XiaoqiJiao,YichunYin,LifengShang,XinJiang,
XiaoChen,LinlinLi,FangWang,andQun

Liu.2019.TinyBERT:DistillingBERTfor

naturallanguageunderstanding.
arXivpreprint
arXiv:1909.10351
.
DiJin,ZhijingJin,JoeyTianyiZhou,and
PeterSzolovits.2020.IsBERTReallyRobust?

AStrongBaselineforNaturalLanguageAttack

onTextClassificationandEntailment.In
AAAI
2020
.
DOI:
https://doi.org/10.1609
/aaai.v34i05.6311
MandarJoshi,DanqiChen,YinhanLiu,
DanielS.Weld,LukeZettlemoyer,and

OmerLevy.2020.SpanBERT:Improving

Pre-TrainingbyRepresentingandPredicting

Spans.
TransactionsoftheAssociationfor
ComputationalLinguistics
,8:64Œ77.
DOI:
https://doi.org/10.1162/tacl
a
00300
Wei-TsungKao,Tsung-HanWu,Po-HanChi,
Chun-ChengHsieh,andHung-YiLee.2020.

FurtherboostingBERT-basedmodelsby

duplicatingexistinglayers:Someintriguing
858
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
phenomenainsideBERT.
arXivpreprint
arXiv:2001.09309
.
TaeukKim,JihunChoi,DanielEdmiston,and
Sang-gooLee.2020.Arepre-trainedlanguage

modelsawareofphrases?simplebutstrong

baselinesforgrammarinduction.In
ICLR2020
.
GoroKobayashi,TatsukiKuribayashi,ShoYokoi,
andKentaroInui.2020.AttentionModuleis

NotOnlyaWeight:AnalyzingTransformers

withVectorNorms.
arXiv:2004.10102[cs]
.
DanKondratyukandMilanStraka.2019.75
Languages,1Model:ParsingUniversalDepen-

denciesUniversally.In
Proceedingsofthe
2019ConferenceonEmpirical Methodsin

NaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages2779Œ2795,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1279
LingpengKong,CypriendeMassond'Autume,
LeiYu,WangLing,ZihangDai, andDani

Yogatama.2019.Amutualinformationmax-

imizationperspectiveoflanguagerepresenta-

tion learning.In
InternationalConferenceon
LearningRepresentations
.
OlgaKovaleva,AlexeyRomanov,AnnaRogers,
andAnnaRumshisky.2019.Revealingthe

DarkSecretsofBERT.In
Proceedingsof
the2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages4356Œ4365,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1445
KalpeshKrishna,GauravSinghTomar,AnkurP.
Parikh,NicolasPapernot,andMohitIyyer.

2020.ThievesonSesameStreet!Model

ExtractionofBERT-BasedAPIs.In
ICLR2020
.
VarunKumar,AshutoshChoudhary,andEunah
Cho.2020.DataAugmentationusingPre-

TrainedTransformerModels.
arXiv:2003.
02245[cs]
.
IliaKuznetsovandIrynaGurevych.2020.A
MatterofFraming:TheImpactofLinguistic

FormalismonProbingResults.
arXiv:2004.
14999[cs]
.
GuillaumeLampleandAlexisConneau.2019.
Cross-LingualLanguageModelPretraining.

arXiv:1901.07291[cs]
.
ZhenzhongLan,MingdaChen,Sebastian
Goodman,KevinGimpel,PiyushSharma,and

RaduSoricut.2020a.ALBERT:ALiteBERT

forSelf-SupervisedLearningofLanguage

Representations.In
ICLR
.
CheolhyoungLee,KyunghyunCho,andWanmo
Kang.2019.Mixout:Effectiveregularization

tofinetunelarge-scalepretrainedlanguage

models.
arXivpreprintarXiv:1909.11299
.
MikeLewis,YinhanLiu,NamanGoyal,Marjan
Ghazvininejad,AbdelrahmanMohamed,Omer

Levy,VesStoyanov,andLukeZettlemoyer.

2019.BART:DenoisingSequence-to-Sequence

Pre-TrainingforNaturalLanguageGenera-

tion,Translation,andComprehension.
arXiv:
1910.13461[cs,stat]
.
DOI:
https://doi
.org/10.18653/v1/2020.acl-main.703
ChangmaoLiandJinhoD.Choi.2020.Trans-
formerstoLearnHierarchicalContextsin

MultipartyDialogueforSpan-basedQuestion

Answering.In
Proceedingsofthe58thAnnual
MeetingoftheAssociationforComputa-

tionalLinguistics
,pages5709Œ5714,Online.
AssociationforComputationalLinguistics.
ZhuohanLi,EricWallace,ShengShen,Kevin
Lin,KurtKeutzer,Dan Klein,andJosephE.

Gonzalez.2020.Trainlarge,thencompress:

Rethinkingmodelsizeforefficienttraining

andinferenceoftransformers.
arXivpreprint
arXiv:2002.11794
.
YongjieLin,YiChernTan,andRobertFrank.
2019.OpenSesame:GettinginsideBERT's

LinguisticKnowledge.In
Proceedingsofthe
2019ACLWorkshopBlackboxNLP:Analyzing

andInterpretingNeuralNetworksforNLP
,
pages241Œ253.
NelsonF.Liu,MattGardner,YonatanBelinkov,
MatthewE.Peters,andNoahA.Smith.2019a.

LinguisticKnowledgeandTransferabilityof
859
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
ContextualRepresentations.In
Proceedings
ofthe2019ConferenceoftheNorth

AmericanChapteroftheAssociationfor

ComputationalLinguistics:HumanLanguage

Technologies,Volume1(LongandShort

Papers)
,pages1073Œ1094,Minneapolis,
Minnesota.AssociationforComputational

Linguistics.
YinhanLiu,MyleOtt,NamanGoyal,Jingfei
Du,MandarJoshi,DanqiChen,OmerLevy,

MikeLewis,LukeZettlemoyer,andVeselin

Stoyanov.2019b.RoBERTa:ARobustlyOpti-

mizedBERTPretrainingApproach.
arXiv:
1907.11692[cs]
.
XiaofeiMa,ZhiguoWang,PatrickNg,Ramesh
Nallapati, andBingXiang.2019.Universal

TextRepresentationfromBERT:AnEmpirical

Study.
arXiv:1910.07973[cs]
.
ChristopherD.Manning,KevinClark,John
Hewitt,UrvashiKhandelwal,andOmerLevy.

2020.Emergentlinguisticstructurein artificial

neuralnetworkstrainedbyself-supervision.

ProceedingsoftheNationalAcademyofSci-

ences
,page201907367.
DOI:
https://
doi.org/10.1073/pnas.1907367117
,
PMID:
32493748
ChandlerMay,AlexWang,ShikhaBordia,
SamuelR.Bowman,andRachelRudinger.

2019.OnMeasuringSocialBiasesinSentence

Encoders.In
Proceedingsofthe2019
ConferenceoftheNorthAmericanChapterof

theAssociationforComputationalLinguistics:

HumanLanguageTechnologies,Volume1

(LongandShortPapers)
,pages622Œ628,
Minneapolis,Minnesota.Associationfor

ComputationalLinguistics.
J.S.McCarley,RishavChakravarti,andAvirup
Sil.2020.StructuredPruningofaBERT-based

QuestionAnsweringModel.
arXiv:1910.06360
[cs]
.
R.ThomasMcCoy,TalLinzen,EwanDunbar,
andPaulSmolensky.2019a.RNNsimplicitly

implementtensor-productrepresentations.

In
InternationalConferenceonLearning
Representations
.
TomMcCoy,ElliePavlick,andTalLinzen.
2019b.RightfortheWrongReasons:Diag-
nosingSyntacticHeuristicsinNaturalLan-

guageInference.In
Proceedingsofthe57th
AnnualMeetingoftheAssociationforCom-

putationalLinguistics
,pages3428Œ3448,
Florence,Italy.AssociationforComputational

Linguistics.
DOI:
https://doi.org/10
.18653/v1/P19-1334
AlessioMiaschiandFeliceDell'Orletta.2020.
ContextualandNon-ContextualWordEmbed-

dings:Anin-depthLinguisticInvestigation.In

Proceedingsofthe5thWorkshoponRepresen-

tationLearningforNLP
,pages110Œ119.
DOI:
https://doi.org/10.18653/v1/2020

.repl4nlp-1.15
PaulMichel,OmerLevy,andGrahamNeubig.
2019.AreSixteenHeadsReallyBetterthan

One?
AdvancesinNeuralInformationProcess-
ingSystems32(NIPS2019)
.
TimotheeMickus,DenisPaperno,Mathieu
Constant,andKeesvanDeemeter.2019.What

doyoumean,BERT?assessingBERTasa

distributionalsemanticsmodel.
arXivpreprint
arXiv:1911.05758
.
Microsoft.2020.Turing-NLG:A17-billion-
parameterlanguagemodelbymicrosoft.
TomasMikolov,IlyaSutskever,KaiChen,GregS.
Corrado,andJeffDean.2013.Distributed

representationsofwordsandphrasesand

theircompositionality.In
AdvancesinNeural
InformationProcessingSystems26(NIPS

2013)
,pages3111Œ3119.
JiaqiMuandPramodViswanath.2018.All-but-
the-top:Simpleandeffectivepostprocessing

forwordrepresentations.In
International
ConferenceonLearningRepresentations
.
TimothyNivenandHung-YuKao.2019.
ProbingNeuralNetworkComprehension

ofNaturalLanguageArguments.In
Pro-
ceedingsofthe57thAnnual Meetingofthe

AssociationforComputationalLinguistics
,
pages4658Œ4664,Florence,Italy.Associ-

ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/P19

-1459
MatthewE.Peters,MarkNeumann,RobertLogan,
RoySchwartz,VidurJoshi,SameerSingh,and
860
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
NoahA.Smith.2019a.KnowledgeEnhanced

ContextualWordRepresentations.In
Proceed-
ingsofthe2019ConferenceonEmpirical

MethodsinNaturalLanguageProcessingand

the9thInternationalJointConferenceonNat-

uralLanguageProcessing(EMNLP-IJCNLP)
,
pages43Œ54,HongKong,China.Associ-

ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1005
,
PMID:
31383442
MatthewE.Peters,SebastianRuder,andNoah
A.Smith.2019b.ToTuneorNottoTune?

AdaptingPretrainedRepresentationsto

DiverseTasks.In
Proceedingsofthe4th
WorkshoponRepresentationLearningfor

NLP(RepL4NLP-2019)
,pages7Œ14,Florence,
Italy.AssociationforComputationalLin-

guistics.
DOI:
https://doi.org/10.18653
/v1/W19-4302
,
PMCID:
PMC6351953
FabioPetroni,TimRockt
aschel,Sebastian
Riedel,PatrickLewis,AntonBakhtin,Yuxiang

Wu,andAlexanderMiller.2019.Language

ModelsasKnowledgeBases?In
Proceedings
ofthe2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages2463Œ2473,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1250
JasonPhang,ThibaultF

evry,andSamuelR.
Bowman.2019.SentenceEncodersonSTILTs:

SupplementaryTrainingonIntermediate

Labeled-DataTasks.
arXiv:1811.01088[cs]
.
TiagoPimentel,JosefValvoda,RowanHall
Maudslay,RanZmigrod,AdinaWilliams,and

RyanCotterell.2020.Information-Theoretic

ProbingforLinguisticStructure.
arXiv:2004.
03061[cs]
.
DOI:
https://doi.org/10
.18653/v1/2020.acl-main.420
NinaPoerner,UlliWaltinger,andHinrich
Sch
utze.2019.BERTisnotaknowledgebase
(yet):Factualknowledgevs.name-basedrea-

soninginunsupervisedqa.
arXivpreprintarXiv:
1911.03681
.
SaiPrasanna,AnnaRogers,andAnnaRumshisky.
2020.WhenBERTPlaystheLottery, All
TicketsAreWinning.In
Proceedingsofthe
2020ConferenceonEmpiricalMethodsinNat-

uralLanguageProcessing
.Online.Association
forComputationalLinguistics.
OfirPress,NoahA.Smith,andOmerLevy.
2020.ImprovingTransformerModelsbyRe-

orderingtheirSublayers.In
Proceedingsofthe
58thAnnualMeetingoftheAssociationfor

ComputationalLinguistics
,pages2996Œ3005,
Online.AssociationforComputationalLin-

guistics.
DOI:
https://doi.org/10.18653
/v1/2020.acl-main.270
YadaPruksachatkun,JasonPhang,HaokunLiu,
PhuMonHtut,XiaoyiZhang,RichardYuanzhe

Pang,ClaraVania,KatharinaKann,and

SamuelR.Bowman.2020.Intermediate-Task

TransferLearningwithPretrainedLanguage

Models:WhenandWhyDoesItWork?

In
Proceedingsofthe58thAnnualMeet-
ingoftheAssociationforComputational

Linguistics
,pages5231Œ5247,Online.Asso-
ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/2020

.acl-main.467
ColinRaffel,NoamShazeer,AdamRoberts,
KatherineLee,SharanNarang,Michael

Matena,YanqiZhou,WeiLi,andPeterJ.Liu.

2019.ExploringtheLimitsofTransferLearning

withaUnifiedText-to-TextTransformer.

arXiv:1910.10683[cs,stat]
.
AlessandroRaganato,YvesScherrer,andJ
org
Tiedemann.2020.FixedEncoderSelf-Attention

PatternsinTransformer-BasedMachineTrans-

lation.
arXiv:2002.10260[cs]
.
AlessandroRaganatoandJ
orgTiedemann.2018.
AnAnalysisofEncoderRepresentationsin

Transformer-BasedMachineTranslation.In

Proceedingsofthe2018EMNLPWorkshop

BlackboxNLP:AnalyzingandInterpreting

NeuralNetworksforNLP
,pages287Œ297,
Brussels,Belgium.AssociationforComputa-

tionalLinguistics.
DOI:
https://doi.org
/10.18653/v1/W18-5431
MarcoTulioRibeiro,TongshuangWu,Carlos
Guestrin,andSameerSingh.2020.Beyond

Accuracy:BehavioralTestingofNLPModels

withCheckList.In
Proceedingsofthe58th
AnnualMeetingoftheAssociationforCom-

putationalLinguistics
,pages4902Œ4912,
861
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Online.AssociationforComputationalLin-

guistics.
DOI:
https://doi.org/10.18653
/v1/2020.acl-main.442
KyleRichardson,HaiHu,LawrenceS.Moss,
andAshishSabharwal.2020.ProbingNatural

LanguageInferenceModelsthroughSemantic

Fragments.In
AAAI2020
.
DOI:
https://
doi.org/10.1609/aaai.v34i05.6397
KyleRichardsonandAshishSabharwal. 2019.
WhatDoesMyQAModelKnow?Devising

ControlledProbesusingExpertKnowledge.

arXiv:1912.13337[cs]
.
DOI:
https://doi
.org/10.1162/tacl
a
00331
AdamRoberts,ColinRaffel,andNoamShazeer.
2020.HowMuchKnowledgeCanYouPack

IntotheParametersofaLanguageModel?

arXivpreprintarXiv:2002.08910
.
AnnaRogers,OlgaKovaleva,MatthewDowney,
andAnnaRumshisky.2020.GettingCloserto

AICompleteQuestionAnswering:ASetof

PrerequisiteRealTasks.In
AAAI
,page11.
DOI:
https://doi.org/10.1609/aaai
.v34i05.6398
RudolfRosaandDavidMare

cek.2019.Inducing
syntactictreesfromBERTrepresentations.

arXivpreprintarXiv:1906.11511
.
VictorSanh,LysandreDebut,JulienChaumond,
andThomasWolf.2019.DistilBERT,adistilled

versionofBERT:Smaller,faster,cheaperand

lighter.In
5thWorkshoponEnergyEfficient
MachineLearningandCognitiveComputing-

NeurIPS2019
.
VictorSanh,ThomasWolf,andAlexanderM.
Rush.2020.MovementPruning:Adaptive

SparsitybyFine-Tuning.
arXiv:2005.07683
[cs]
.
TimoSchickandHinrichSch
utze.2020.
BERTRAM:ImprovedWordEmbeddings

HaveBigImpactonContextualizedModelPer-

formance.In
Proceedingsofthe58thAnnual
MeetingoftheAssociationforComputational

Linguistics
,pages3996Œ4007,Online.Asso-
ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/2020

.acl-main.368
FlorianSchmidtandThomasHofmann.2020.
BERTasaTeacher:ContextualEmbeddings

forSequence-LevelReward.
arXivpreprint
arXiv:2003.02738
.
RoySchwartz,JesseDodge,NoahA.Smith,
andOrenEtzioni.2019.GreenAI.
arXiv:
1907.10597[cs,stat]
.
SofiaSerranoandNoahA.Smith.2019.Is
AttentionInterpretable?
arXiv:1906.03731
[cs]
.
DOI:
https://doi.org/10.18653
/v1/P19-1282
ShengShen,ZhenDong,JiayuYe,Linjian
Ma,ZheweiYao,AmirGholami,MichaelW.

Mahoney,andKurtKeutzer.2019.Q-BERT:

HessianBasedUltraLowPrecisionQuanti-

zationofBERT.
arXivpreprintarXiv:1909.
05840
.
DOI:
https://doi.org/10.1609
/aaai.v34i05.6409
ChengleiSi,ShuohangWang,Min-YenKan,
andJingJiang.2019.WhatdoesBERTLearn

fromMultiple-ChoiceReadingComprehension

Datasets?
arXiv:1910.12391[cs]
.
KaitaoSong,XuTan,TaoQin,JianfengLu,and
Tie-YanLiu.2020.MPNet:MaskedandPer-

mutedPre-trainingforLanguageUnderstand-

ing.
arXiv:2004.09297[cs]
.
AsaCooperSticklandandIainMurray.2019.
BERTandPALs:ProjectedAttentionLayers

forEfficientAdaptationinMulti-TaskLearn-

ing.In
InternationalConferenceonMachine
Learning
,pages5986Œ5995.
EmmaStrubell,AnanyaGanesh,andAndrew
McCallum.2019.EnergyandPolicy Consider-

ationsforDeepLearninginNLP.In
ACL2019
.
Ta-ChunSuandHsiang-ChihCheng.2019.
SesameBERT:AttentionforAnywhere.
arXiv:
1910.03176[cs]
.
SakuSugawara,PontusStenetorp,KentaroInui,
andAkikoAizawa.2020.AssessingtheBench-

markingCapacityofMachineReadingCom-

prehensionDatasets.In
AAAI
.
DOI:
https://
doi.org/10.1609/aaai.v34i05.6422
SiqiSun,YuCheng,ZheGan,andJingjing
Liu.2019a.PatientKnowledgeDistillationfor

BERTModelCompression.In
Proceedings
862
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
ofthe2019ConferenceonEmpiricalMethods

inNaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages4314Œ4323.
DOI:
https://doi.org
/10.18653/v1/D19-1441
YuSun,ShuohuanWang,YukunLi,ShikunFeng,
XuyiChen,HanZhang,XinTian,Danxiang

Zhu,HaoTian,andHuaWu.2019b.ERNIE:

EnhancedRepresentationthroughKnowledge

Integration.
arXiv:1904.09223[cs]
.
YuSun,ShuohuanWang,YukunLi,Shikun
Feng,HaoTian,HuaWu,andHaifengWang.

2019c.ERNIE2.0:AContinualPre-Training

FrameworkforLanguageUnderstanding.

arXiv:1907.12412[cs]
.
DOI:
https://doi
.org/10.1609/aaai.v34i05.6428
ZhiqingSun,HongkunYu,XiaodanSong,Renjie
Liu,YimingYang,andDennyZhou.2020.

MobileBERT:Task-AgnosticCompressionof

BERTforResourceLimitedDevices.
DhanasekarSundararaman,VivekSubramanian,
GuoyinWang,ShijingSi,DinghanShen,

DongWang,andLawrenceCarin.2019.

Syntax-InfusedTransformerandBERTmodels

forMachineTranslationandNaturalLanguage

Understanding.
arXiv:1911.06156[cs,stat]
.
DOI:
https://doi.org/10.1109/IALP48816
.2019.9037672
,
PMID:
31938450,
PMCID:
PMC6959198
AlonTalmor,YanaiElazar,YoavGoldberg,
andJonathanBerant.2019.oLMpicsŒOn

whatLanguageModelPre-TrainingCaptures.

arXiv:1912.13283[cs]
.
HirotakaTanaka,HiroyukiShinnou,RuiCao,
JingBai,andWenMa.2020.Document

ClassificationbyWordEmbeddingsofBERT.

In
ComputationalLinguistics
,Communica-
tionsinComputerandInformationScience,

pages145Œ154,Singapore,Springer.
RaphaelTang,YaoLu,LinqingLiu,Lili
Mou,OlgaVechtomova,andJimmyLin.

2019.DistillingTask-SpecificKnowledgefrom

BERTintoSimpleNeuralNetworks.
arXiv
preprintarXiv:1903.12136
.
IanTenney,DipanjanDas,andElliePavlick.
2019a.BERTRediscoverstheClassicalNLP
Pipeline.In
Proceedingsofthe57thAnnual
MeetingoftheAssociationforComputa-

tionalLinguistics
,pages4593Œ4601.
DOI:
https://doi.org/10.18653/v1/P19

-1452
IanTenney,PatrickXia,BerlinChen,Alex
Wang,AdamPoliak,R.ThomasMcCoy,

NajoungKim,BenjaminVanDurme,Samuel

R.Bowman,DipanjanDas,andElliePavlick.

2019b.Whatdoyoulearnfromcontext?

Probingforsentencestructureincontextu-

alizedwordrepresentations.In
International
ConferenceonLearningRepresentations
.
JamesYiTian,AlexanderP.Kreuzer,Pai-Hung
Chen,andHans-MartinWill.2019.WaLDORf:

WastelessLanguage-modelDistillationOn

Reading-comprehension.
arXivpreprintarXiv:
1912.06638
.
ShubhamToshniwal,HaoyueShi,BowenShi,
LingyuGao,KarenLivescu,andKevin

Gimpel.2020.ACross-TaskAnalysisofText

SpanRepresentations.In
Proceedingsofthe
5thWorkshoponRepresentationLearning

forNLP
,pages166Œ176,Online.Associ-
ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/2020

.repl4nlp-1.20
HenryTsai,JasonRiesa,MelvinJohnson,
NaveenArivazhagan,XinLi,andAmelia

Archer.2019.SmallandPracticalBERT

ModelsforSequenceLabeling.
arXivpreprint
arXiv:1909.00100
.
DOI:
https://doi.org
/10.18653/v1/D19-1374
IuliaTurc,Ming-WeiChang,KentonLee,
andKristinaToutanova.2019.Well-Read

StudentsLearnBetter:TheImpactofStudent

InitializationonKnowledgeDistillation.
arXiv
preprintarXiv:1908.08962
.
MartenvanSchijndel,AaronMueller,andTal
Linzen.2019.Quantitydoesn'tbuyquality

syntaxwithneurallanguagemodels.In
Pro-
ceedingsofthe2019ConferenceonEmpirical

MethodsinNaturalLanguageProcessingand

the9thInternationalJointConferenceonNat-

uralLanguageProcessing(EMNLP-IJCNLP)
,
pages5831Œ5837,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
863
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
https://doi.org/10.18653/v1/D19

-1592
AshishVaswani,NoamShazeer,NikiParmar,
JakobUszkoreit,LlionJones,AidanN.Gomez,

•ukaszKaiser,andIlliaPolosukhin.2017.

AttentionisAllyouNeed.In
Advances
inNeuralInformationProcessingSystems
,
pages5998Œ6008.
JesseVig.2019.VisualizingAttentionin
Transformer-BasedLanguageRepresentation

Models.
arXiv:1904.02679[cs,stat]
.
JesseVigandYonatanBelinkov.2019.Analyzing
theStructureofAttentioninaTransformer

LanguageModel.In
Proceedingsofthe
2019ACLWorkshopBlackboxNLP:Analyz-

ingandInterpretingNeuralNetworksfor

NLP
,pages63Œ76,Florence,Italy.Associ-
ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/W19

-4808
DavidVilares,MichalinaStrzyz,AndersS˝gaard,
andCarlosG

omez-Rodr

guez.2020.Parsingas
pretraining.In
Thirty-FourthAAAIConference
onArtificialIntelligence(AAAI-20)
.
DOI:
https://doi.org/10.1609/aaai.v34i05

.6446
ElenaVoita,RicoSennrich,andIvanTitov.2019a.
TheBottom-upEvolutionofRepresentations

intheTransformer:AStudywithMachine

TranslationandLanguageModelingObjec-

tives.In
Proceedingsofthe2019Conference
onEmpiricalMethodsinNaturalLanguage

Processingandthe9thInternationalJoint

ConferenceonNaturalLanguageProcessing

(EMNLP-IJCNLP)
,pages4387Œ4397.
DOI:
https://doi.org/10.18653/v1/D19

-1448
ElenaVoita,DavidTalbot,FedorMoiseev,Rico
Sennrich,andIvanTitov.2019b.Analyzing

Multi-HeadSelf-Attention:SpecializedHeads

DotheHeavyLifting,theRestCanBePruned.

arXivpreprintarXiv:1905.09418
.
DOI:
https://doi.org/10.18653/v1/P19

-1580
ElenaVoitaandIvanTitov.2020.Information-
TheoreticProbingwithMinimumDescription

Length.
arXiv:2003.12298[cs]
.
EricWallace,ShiFeng,NikhilKandpal,Matt
Gardner,andSameerSingh.2019a.Uni-

versalAdversarialTriggersforAttacking

andAnalyzingNLP.In
Proceedingsofthe
2019ConferenceonEmpiricalMethodsin

NaturalLanguageProcessingandthe9th

InternationalJointConferenceonNatural

LanguageProcessing(EMNLP-IJCNLP)
,
pages2153Œ2162,HongKong,China.Asso-

ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19

-1221
EricWallace,YizhongWang,SujianLi,Sameer
Singh,andMattGardner.2019b.DoNLP

ModelsKnowNumbers?ProbingNumeracy

inEmbeddings.
arXivpreprintarXiv:1909.
07940
.
DOI:
https://doi.org/10.18653
/v1/D19-1534
AlexWang,AmapreetSingh,JulianMichael,
FelixHill,OmerLevy,andSamuelR.

Bowman.2018.GLUE:AMulti-TaskBench-

markandAnalysisPlatformforNaturalLan-

guageUnderstanding.In
Proceedingsofthe
2018EMNLPWorkshopBlackboxNLP:Ana-

lyzingandInterpretingNeuralNetworksfor

NLP
,pages353Œ355,Brussels,Belgium.Asso-
ciationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/W18

-5446
RuizeWang,DuyuTang,NanDuan,Zhongyu
Wei,XuanjingHuang,Jianshuji,Guihong

Cao,DaxinJiang,andMingZhou.2020a.K-

Adapter:InfusingKnowledgeintoPre-Trained

ModelswithAdapters.
arXiv:2002.01808[cs]
.
WeiWang,BinBi,MingYan,ChenWu,Zuyi
Bao,LiweiPeng,andLuoSi.2019a.Struct-

BERT:IncorporatingLanguageStructuresinto

Pre-TrainingforDeepLanguageUnderstand-

ing.
arXiv:1908.04577[cs]
.
WenhuiWang,FuruWei,LiDong,HangboBao,
NanYang,andMingZhou.2020b.MiniLM:

DeepSelf-AttentionDistillationforTask-

AgnosticCompressionofPre-TrainedTrans-

formers.
arXivpreprintarXiv:2002.10957
.
XiaozhiWang,TianyuGao,ZhaochengZhu,
ZhiyuanLiu,JuanziLi,andJianTang.2020c.

KEPLER:AUnified ModelforKnowledge
864
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
EmbeddingandPre-trainedLanguageRepre-

sentation.
arXiv:1911.06136[cs]
.
YileWang,LeyangCui,andYueZhang.2020d.
HowCanBERTHelpLexicalSemanticsTasks?

arXiv:1911.02929[cs]
.
ZihanWang,StephenMayhew,DanRoth,etal.
2019b.Cross-LingualAbilityofMultilingual

BERT:AnEmpiricalStudy.
arXivpreprint
arXiv:1912.07840
.
AlexWarstadtandSamuelR.Bowman.2020.
Canneuralnetworksacquireastructuralbias

fromrawlinguisticdata?In
Proceedingsofthe
42ndAnnualVirtualMeetingoftheCognitive

ScienceSociety
.Online.
AlexWarstadt,YuCao,IoanaGrosu,Wei
Peng,HagenBlix,YiningNie,AnnaAlsop,

ShikhaBordia,HaokunLiu,AliciaParrish,

etal.2019.InvestigatingBERT'sKnowledge

ofLanguage:FiveAnalysisMethodswith

NPIs.In
Proceedingsofthe2019Conference
onEmpiricalMethodsinNaturalLanguage

Processingandthe9thInternationalJoint

ConferenceonNaturalLanguageProcessing

(EMNLP-IJCNLP)
,pages2870Œ2880.
DOI:
https://doi.org/10.18653/v1/D19-1286
GregorWiedemann,SteffenRemus,Avi
Chawla,andChrisBiemann.2019.Does

BERTMakeAny Sense?InterpretableWord

SenseDisambiguationwithContextualized

Embeddings.
arXivpreprintarXiv:1909.10430
.
SarahWiegreffeandYuvalPinter.2019.Atten-
tionisnotnotExplanation.In
Proceedings
ofthe2019ConferenceonEmpiricalMeth-

odsinNaturalLanguageProcessingandthe

9thInternationalJointConferenceonNatu-

ralLanguageProcessing(EMNLP-IJCNLP)
,
pages11Œ20,HongKong,China.Associ-

ationforComputationalLinguistics.
DOI:
https://doi.org/10.18653/v1/D19-1002
ThomasWolf,LysandreDebut,VictorSanh,
JulienChaumond,ClementDelangue,Anthony

Moi,PierricCistac,TimRault,R

emiLouf,
MorganFuntowicz,andJamieBrew.2020.

HuggingFace'sTransformers:State-of-the-Art

NaturalLanguageProcessing.
arXiv:1910.
03771[cs]
.
FelixWu,AngelaFan,AlexeiBaevski,Yann
Dauphin,andMichaelAuli.2019a.PayLess

AttentionwithLightweightandDynamic

Convolutions.In
InternationalConferenceon
LearningRepresentations
.
XingWu,ShangwenLv,LiangjunZang,
JizhongHan,andSonglinHu.2019b.Con-

ditionalBERTContextualAugmentation.In

ICCS2019:ComputationalScienceICCS

2019
,pages84Œ95.Springer.
DOI:
https://
doi.org/10.1007/978-3-030-22747-0
7
YonghuiWu,MikeSchuster,ZhifengChen,
QuocV.Le,MohammadNorouzi,Wolfgang

Macherey,MaximKrikun,YuanCao,QinGao,

KlausMacherey,etal.2016.Google'sNeural

MachineTranslationSystem:BridgingtheGap

betweenHumanandMachineTranslation.
ZhiyongWu,YunChen,BenKao,andQun
Liu.2020.PerturbedMasking:Parameter-free

ProbingforAnalyzingandInterpretingBERT.

In
Proceedingsofthe58thAnnualMeetingof
theAssociationforComputationalLinguistics
,
pages4166Œ4176,Online.Associationfor

ComputationalLinguistics.
CanwenXu,WangchunshuZhou,TaoGe,Furu
Wei,andMingZhou.2020.BERT-of-Theseus:

CompressingBERTbyProgressiveModule

Replacing.
arXivpreprintarXiv:2002.02925
.
JunjieYangandHaiZhao.2019.Deepening
HiddenRepresentationsfromPre-TrainedLan-

guageModelsforNaturalLanguageUnder-

standing.
arXiv:1911.01940[cs]
.
ZhilinYang,ZihangDai,YimingYang,Jaime
Carbonell,RuslanSalakhutdinov,andQuocV.

Le.2019.XLNet:GeneralizedAutoregressive

PretrainingforLanguageUnderstanding.

arXiv:1906.08237[cs]
.
Pengcheng Yin,GrahamNeubig,Wen-tauYih,
andSebastianRiedel.2020.TaBERT:Pretrain-

ingforJointUnderstandingofTextualand

TabularData.In
Proceedingsofthe58thAnnual
MeetingoftheAssociationforComputational

Linguistics
,pages8413Œ8426,Online.Associ-
ationforComputationalLinguistics.
DaniYogatama,CypriendeMassond'Autume,
JeromeConnor,TomasKocisky,Mike

Chrzanowski,LingpengKong,Angeliki
865
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
Lazaridou,WangLing,LeiYu,ChrisDyer,

andPhilBlunsom.2019.LearningandEvaluat-

ingGeneralLinguisticIntelligence.
arXiv:
1901.11373[cs,stat]
.
YangYou,JingLi,SashankReddi,JonathanHseu,
SanjivKumar,SrinadhBhojanapalli,Xiaodan

Song,JamesDemmel,andCho-JuiHsieh.2019.

LargeBatchOptimizationforDeepLearning:

TrainingBERTin76Minutes.
arXivpreprint
arXiv:1904.00962
,1(5).
AliHadiZadehandAndreasMoshovos.2020.
GOBO:QuantizingAttention-BasedNLP

ModelsforLowLatencyandEnergyEfficient

Inference.
arXiv:2005.03842[cs,stat]
.
DOI:
https://doi.org/10.1109/MICRO50266

.2020.00071
OfirZafrir,GuyBoudoukh,PeterIzsak,and
MosheWasserblat.2019.Q8BERT:Quantized

8bitBERT.
arXivpreprintarXiv:1910.06188
.
RowanZellers,AriHoltzman,YonatanBisk,Ali
Farhadi,and Yejin Choi.2019.HellaSwag:Can

aMachineReallyFinishYourSentence?In

Proceedingsofthe57thAnnualMeetingof

theAssociationforComputationalLinguistics
,
pages4791Œ4800.
ZhengyanZhang,XuHan,ZhiyuanLiu,Xin
Jiang,MaosongSun,andQun Liu.2019.

ERNIE:EnhancedLanguageRepresenta-

tionwithInformativeEntities.In
Proceedings
ofthe57thAnnualMeetingoftheAssociation

forComputationalLinguistics
,pages1441Œ1451,
Florence,Italy.AssociationforComputa-

tionalLinguistics.
DOI:
https://doi.org
/10.18653/v1/P19-1139
ZhuoshengZhang,YuweiWu,HaiZhao,
ZuchaoLi,ShuailiangZhang,XiZhou,and

XiangZhou.2020.Semantics-awareBERTfor

LanguageUnderstanding.In
AAAI2020
.
SanqiangZhao,RaghavGupta,YangSong,
andDennyZhou.2019.ExtremeLanguage

ModelCompressionwithOptimalSubwords

andSharedProjections.
arXiv preprint
arXiv:1909.11687
.
YiyunZhaoandStevenBethard.2020.How
doesBERT'sattentionchangewhenyoufine-

tune?Ananalysismethodologyandacasestudy

innegationscope.In
Proceedingsofthe58th
AnnualMeetingoftheAssociationforCom-

putationalLinguistics
,pages4729Œ4747,
Online.AssociationforComputationalLinguis-

tics.
DOI:
https://doi.org/10.18653
/v1/2020.acl-main.429
,
PMCID:
PMC7660194
WenxuanZhou,JunyiDu,andXiangRen.2019.
ImprovingBERTFine-tuningwithEmbedding

Normalization.
arXivpreprintarXiv:1911.
03918
.
XuhuiZhou,YueZhang,LeyangCui,andDandan
Huang.2020.EvaluatingCommonsensein

Pre-TrainedLanguageModels.In
AAAI2020
.
DOI:
https://doi.org/10.1609
/aaai.v34i05.6523
ChenZhu,YuCheng,ZheGan,SiqiSun,Tom
Goldstein,andJingjingLiu.2019.FreeLB:

EnhancedAdversarialTrainingforLanguage

Understanding.
arXiv:1909.11764[cs]
.
866
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf by guest on 24 May 2022
