DeepResidualLearningfor ImageRecognition
KaimingHeXiangyu ZhangShaoqingRen JianSun
MicrosoftResearch
{
kahe,v-xiangz,v-shren,jiansun
}
@microsoft.com
Abstract
Deeperneuralnetworksaremoredifﬁculttotrain.We
present aresiduallearningframeworktoeasethetraining

ofnetworksthataresubstantiallydeeperthanthoseused

previously.Weexplicitlyreformulate thelayersaslearn-

ing residualfunctionswithreferencetothelayerinputs, in-

steadoflearningunreferencedfunctions.Weprovidecom-

prehensive empiricalevidenceshowingthattheseresidual

networksareeasiertooptimize,andcangainaccuracyfrom

considerablyincreaseddepth.OntheImageNetdatasetwe

evaluateresidualnetswithadepth ofupto152layers—8
×
deeperthanVGGnets[40]butstillhavinglowercomplex-

ity.An ensembleoftheseresidualnetsachieves3.57%error

ontheImageNet
test
set.Thisresultwonthe 1stplace onthe
ILSVRC2015classiﬁcationtask.Wealsopresentanalysis

onCIFAR-10with100and1000layers.
Thedepthofrepresentations isofcentralimportance
formanyvisualrecognitiontasks.Solelydueto ourex-

tremelydeeprepresentations,weobtain a28%relativeim-

provementontheCOCOobjectdetectiondataset.Deep

residualnetsarefoundationsofoursubmissions toILSVRC

&COCO2015 competitions
1
,wherewealsowonthe1st
placesonthetasksofImageNetdetection,ImageNetlocal-

ization,COCOdetection,andCOCOsegmentation.

1.Introduction
Deepconvolutionalneuralnetworks [22,21]haveled
toaseriesofbreakthroughsforimageclassiﬁcation [21,

49,39].Deepnetworksnaturallyintegratelow/mid/high-

levelfeatures[49]andclassiﬁersinanend-to-endmulti-

layerfashion,and the“levels” offeaturescanbeenriched

bythenumberofstackedlayers(depth).Recentevidence

[40,43]revealsthatnetworkdepthisofcrucialimportance,

and theleadingresults [40,43,12,16]onthechallenging

ImageNetdataset [35]allexploit“verydeep”[40]models,

withadepthofsixteen[40]tothirty[16].Manyothernon-

trivialvisualrecognitiontasks[7,11,6,32,27]havealso
1
http://image-net.org/challenges/LSVRC/2015/
and
http://mscoco.org/dataset/#detections-challenge2015
.
0
1
2
3
4
5
6
0
10
20
iter. (1e4)
training error (%)
0
1
2
3
4
5
6
0
10
20
iter. (1e4)
test error (%)
56-layer

20-layer
56-layer

20-layer
Figure1.Trainingerror(left)andtesterror(right)onCIFAR-10

with20-layerand56-layer“plain”networks.Thedeepernetwork

hashighertrainingerror,andthustesterror.Similarphenomena

onImageNetispresentedinFig.4.

greatlybeneﬁtedfromverydeepmodels.
Drivenbythesigniﬁcanceofdepth,aquestionarises:
Is
learningbetternetworksaseasyasstackingmorelayers?

Anobstacletoansweringthisquestionwasthenotorious

problemofvanishing/explodinggradients[14,1,8],which

hamperconvergencefromthebeginning.Thisproblem,

however,hasbeenlargelyaddressedby normalizedinitial-

ization[23,8,36,12]andintermediatenormalizationlayers

[16],whichenablenetworkswithtensoflayerstostartcon-

vergingforstochasticgradientdescent(SGD)withback-

propagation[22].
Whendeepernetworksareableto startconverging,a
degradation
problemhasbeenexposed:withthenetwork
depthincreasing,accuracygetssaturated(whichmightbe

unsurprising)andthendegradesrapidly.Unexpectedly,

suchdegradationis
notcausedbyoverﬁtting
,andadding
morelayersto asuitablydeepmodelleadsto
highertrain-
ingerror
,asreportedin[10,41]andthoroughlyveriﬁedby
ourexperiments.Fig.1showsatypicalexample.
Thedegradation(oftrainingaccuracy)indicatesthatnot
allsystems aresimilarlyeasy tooptimize.Letusconsidera

shallowerarchitectureanditsdeepercounterpartthatadds

morelayersontoit.There existsa solution
byconstruction
tothedeepermodel:theaddedlayersare
identity
mapping,
andtheotherlayersarecopiedfromthelearnedshallower

model.Theexistenceofthisconstructedsolutionindicates

thatadeepermodelshouldproducenohighertrainingerror

thanitsshallowercounterpart.But experimentsshowthat

ourcurrentsolversonhandareunabletoﬁndsolutionsthat
1
770


	

	

	


	

F
(
x
)
+
x
x
F
(
x
)
x
Figure2. Residuallearning:abuildingblock.
arecomparablygoodorbetterthantheconstructedsolution

(orunabletodosoinfeasibletime).
Inthispaper,weaddressthedegradationproblemby
introducinga
deepresiduallearning
framework.In-
steadofhopingeachfewstackedlayersdirectlyﬁta

desiredunderlyingmapping,weexplicitlylettheselay-

ersﬁtaresidualmapping.Formally,denotingthedesired

underlyingmappingas
H
(
x
)
,welet thestackednonlinear
layersﬁt anothermappingof
F
(
x
):=
H
(
x
)
−
x
.Theorig-
inalmappingisrecastinto
F
(
x
)+
x
.Wehypothesizethatit
iseasiertooptimizetheresidualmappingthanto optimize

theoriginal,unreferencedmapping.To theextreme,ifan

identitymappingwereoptimal,itwouldbeeasiertopush

theresidualtozerothantoﬁtanidentitymappingbyastack

ofnonlinearlayers.
Theformulationof
F
(
x
)+
x
canberealizedbyfeedfor-
wardneuralnetworkswith“shortcutconnections”(Fig.2).

Shortcutconnections[2,33,48]are thoseskippingoneor

morelayers.Inourcase,theshortcutconnectionssimply

performidentity
mapping,andtheiroutputsareaddedto
theoutputsofthestackedlayers(Fig.2).Identityshort-

cutconnectionsaddneitherextraparameternorcomputa-

tionalcomplexity.Theentirenetworkcanstillbetrained

end-to-endbySGDwithbackpropagation,andcanbeeas-

ilyimplementedusingcommonlibraries(
e.g
.,Caffe[19])
withoutmodifyingthesolvers.
We presentcomprehensive experimentsonImageNet
[35]toshowthedegradationproblemandevaluateour

method.Weshowthat:1)Ourextremelydeepresidualnets

areeasytooptimize,butthecounterpart“plain”nets(that

simplystacklayers)exhibithighertrainingerrorwhen the

depthincreases;2)Ourdeepresidualnetscaneasilyenjoy

accuracygains fromgreatly increaseddepth,producingre-

sultssubstantiallybetterthanpreviousnetworks.
SimilarphenomenaarealsoshownontheCIFAR-10set
[20],suggestingthattheoptimizationdifﬁcultiesandthe

effectsofour methodarenotjustakintoaparticulardataset.

Wepresentsuccessfullytrainedmodelsonthisdatasetwith

over 100layers,andexploremodelswithover 1000layers.
OntheImageNetclassiﬁcationdataset[35],weobtain
excellentresultsbyextremelydeepresidualnets.Our152-

layerresidualnetisthedeepest networkeverpresentedon

ImageNet,whilestillhavinglowercomplexitythanVGG

nets[40]. Ourensemblehas
3.57%
top-5erroronthe
ImageNet
test
set,and
wonthe1stplaceintheILSVRC
2015 classiﬁcationcompetition
.Theextremelydeeprep-
resentationsalsohaveexcellentgeneralizationperformance

onotherrecognitiontasks,andleadustofurther
winthe
1stplaceson:ImageNetdetection,ImageNetlocalization,

COCOdetection,andCOCOsegmentation
inILSVRC&
COCO2015competitions.Thisstrongevidenceshowsthat

theresiduallearningprincipleisgeneric,andweexpectthat

itisapplicableinothervisionandnon-visionproblems.
2.RelatedWork

ResidualRepresentations.
In imagerecognition,VLAD
[18]isarepresentationthatencodesbytheresidualvectors

withrespecttoadictionary,andFisherVector[30]canbe

formulatedasaprobabilisticversion[18]ofVLAD.Both

ofthemarepowerfulshallowrepresentationsforimagere-

trievalandclassiﬁcation[4,47]. Forvectorquantization,

encodingresidualvectors[17]isshowntobemoreeffec-

tivethanencodingoriginalvectors.
Inlow-levelvisionandcomputergraphics,forsolv-
ingPartialDifferentialEquations(PDEs),thewidelyused

Multigridmethod[3]reformulatesthesystemassubprob-

lemsatmultiple scales,whereeachsubproblemisrespon-

siblefortheresidualsolutionbetweenacoarserandaﬁner

scale.AnalternativetoMultigridishierarchicalbasispre-

conditioning[44, 45],whichrelieson variablesthatrepre-

sentresidual vectorsbetweentwoscales.Ithasbeenshown

[3,44,45]thatthesesolversconvergemuchfasterthanstan-

dardsolversthatareunawareoftheresidualnatureofthe

solutions.Thesemethodssuggestthatagoodreformulation

orpreconditioningcan simplifytheoptimization.

ShortcutConnections.
Practicesandtheoriesthatleadto
shortcutconnections[2,33,48]havebeenstudiedforalong

time.Anearlypracticeoftraining multi-layerperceptrons

(MLPs)istoaddalinearlayerconnectedfromthenetwork

inputtotheoutput[33,48].In[43,24],a fewinterme-

diatelayersaredirectlyconnectedtoauxiliaryclassiﬁers

foraddressingvanishing/explodinggradients.Thepapers

of[38,37,31,46]proposemethodsforcenteringlayerre-

sponses,gradients,andpropagatederrors,implementedby

shortcutconnections.In[43],an“inception”layeriscom-

posedofashortcutbranchandafewdeeperbranches.
Concurrentwithourwork,“highwaynetworks”[41,42]
presentshortcutconnectionswithgatingfunctions[15].

Thesegatesaredata-dependentandhaveparameters,in

contrasttoouridentityshortcutsthatareparameter-free.

Whenagatedshortcutis“closed” (approachingzero),the

layersinhighwaynetworksrepresent
non-residual
func-
tions.Onthecontrary,ourformulationalwayslearns

residualfunctions;ouridentityshortcutsareneverclosed,

andallinformationisalwayspassedthrough,withaddi-

tionalresidual functionstobe learned.In addition,high-
2
771

waynetworkshavenotdemonstratedaccuracygainswith

extremelyincreaseddepth(
e.g
.,over100layers).
3.DeepResidualLearning

3.1.ResidualLearning
Letusconsider
H
(
x
)
asanunderlyingmappingtobe
ﬁtbyafewstackedlayers(notnecessarilytheentirenet),

withx
denotingtheinputstotheﬁrstoftheselayers.Ifone
hypothesizesthatmultiplenonlinearlayerscanasymptoti-

cally approximate complicatedfunctions
2
,thenitisequiv-
alenttohypothesizethattheycanasymptoticallyapproxi-

matetheresidualfunctions,
i.e
.,
H
(
x
)
−
x
(assumingthat
theinputandoutputareofthesamedimensions).So

ratherthan expectstackedlayersto approximate
H
(
x
)
,we
explicitlylettheselayersapproximatearesidualfunction

F(
x
):=
H
(
x
)
−
x
.Theoriginalfunctionthusbecomes
F
(
x
)+
x
.Althoughbothformsshouldbeabletoasymptot-
icallyapproximatethedesiredfunctions(ashypothesized),

theeaseoflearningmight bedifferent.
Thisreformulationismotivatedbythecounterintuitive
phenomenaaboutthedegradationproblem(Fig.1,left).As

wediscussed in theintroduction,iftheaddedlayerscan

be constructedas identity mappings,adeepermodelshould

havetrainingerror nogreater thanitsshallowercounter-

part.Thedegradationproblem suggeststhatthesolvers

mighthavedifﬁcultiesinapproximatingidentitymappings

bymultiplenonlinearlayers.Withtheresiduallearningre-

formulation,ifidentitymappingsareoptimal,thesolvers

maysimplydrivetheweightsofthemultiplenonlinearlay-

erstoward zerotoapproachidentitymappings.
Inrealcases,itisunlikelythatidentitymappingsareop-
timal,butourreformulationmayhelptopreconditionthe

problem.Iftheoptimalfunctionisclosertoanidentity

mappingthantoazeromapping,itshouldbeeasierforthe

solvertoﬁndtheperturbations withreferencetoanidentity

mapping,thantolearnthefunctionasanewone.Weshow

byexperiments(Fig.7)thatthelearnedresidualfunctionsin

generalhavesmallresponses,suggestingthatidentitymap-

pingsprovidereasonablepreconditioning.
3.2.IdentityMappingbyShortcuts
Weadoptresiduallearningtoeveryfewstackedlayers.
AbuildingblockisshowninFig.2.Formally,inthispaper

weconsiderabuildingblockdeﬁnedas:
y
=
F
(
x
,
{
W
i
}
)+
x
.
(1)
Here
x
and
y
aretheinputandoutputvectorsof thelay-
ersconsidered.Thefunction
F
(
x
,
{
W
i
}
)
representsthe
residualmappingtobelearned.FortheexampleinFig.2

thathastwo layers,
F
=
W
2

(
W
1
x
)
inwhich

denotes
2
Thishypothesis,however,isstillanopenquestion.See[28].
ReLU[29]andthebiasesareomittedforsimplifyingno-

tations.Theoperation
F
+
x
isperformedbyashortcut
connectionandelement-wiseaddition. Weadoptthesec-

ondnonlinearityaftertheaddition(
i.e
.,

(
y
)
,seeFig.2).
TheshortcutconnectionsinEqn.(1)introduceneitherex-
traparameternorcomputationcomplexity.Thisisnotonly

attractiveinpracticebutalsoimportantinourcomparisons

betweenplainandresidualnetworks.Wecanfairlycom-

pareplain/residualnetworksthatsimultaneouslyhave the

samenumberofparameters,depth,width,andcomputa-

tionalcost(except forthenegligibleelement-wiseaddition).
Thedimensionsof
x
and
F
mustbeequalinEqn.(1).
Ifthisisnotthecase(
e.g
.,whenchangingtheinput/output
channels),wecanperformalinearprojection
W
s
bythe
shortcutconnectionstomatchthedimensions:
y
=
F
(
x
,
{
W
i
}
)+
W
s
x
.
(2)
Wecanalsouseasquarematrix
W
s
inEqn.(1).Butwewill
showbyexperimentsthattheidentitymapping issufﬁcient

foraddressingthedegradationproblemandiseconomical,

andthus
W
s
isonlyusedwhenmatchingdimensions.
Theformoftheresidualfunction
F
isﬂexible.Exper-
imentsinthispaperinvolveafunction
F
thathastwoor
threelayers(Fig.5),whilemorelayersarepossible.Butif

Fhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:
y
=
W
1
x
+
x
,forwhichwehavenot observedadvantages.
Wealsonotethatalthoughtheabovenotationsareabout
fully-connectedlayersforsimplicity,theyareapplicableto

convolutionallayers.Thefunction
F
(
x
,
{
W
i
}
)
canrepre-
sentmultipleconvolutionallayers.Theelement-wiseaddi-

tionisperformedontwofeaturemaps,channelbychannel.

3.3.NetworkArchitectures
We have testedvariousplain/residualnets,andhave ob-
servedconsistentphenomena.Toprovideinstancesfordis-

cussion,we describetwomodelsforImageNetasfollows.

PlainNetwork.
Ourplainbaselines(Fig.3,middle)are
mainlyinspiredbythephilosophyofVGGnets[40](Fig.3,

left).Theconvolutionallayersmostlyhave3
×
3ﬁltersand
followtwosimpledesign rules:(i)forthesameoutput

featuremapsize,thelayershavethesamenumberofﬁl-

ters;and(ii)ifthefeaturemapsizeishalved, thenum-

berofﬁltersisdoubledso asto preservethe timecom-

plexityperlayer.Weperformdownsamplingdirectlyby

convolutionallayersthathaveastrideof 2.Thenetwork

endswithaglobalaveragepooling layeranda1000-way

fully-connectedlayerwithsoftmax.Thetotalnumberof

weightedlayersis34inFig.3(middle).
Itisworthnoticingthatourmodelhas
fewer
ﬁltersand
lower
complexitythanVGGnets[40](Fig.3,left).Our34-
layerbaselinehas3.6billionFLOPs(multiply-adds),which

isonly18%ofVGG-19(19.6billionFLOPs).
3
772

	
	
	
	

	

	

	

	

	

		
	
	
	
	
	
	
	
	
	
	

	

	

	

	

	

	

	

	

	

	

		
	
	
	
	
	


˘ˇˆ
	
	

	

	
	
	
	
	

	

	

	

	
	
	
	
	
	
	
	
	
	
˙

˙


˘ˇˆ
˝˛˝˛
˚˘˜ˆ 
˝˛˝˛
˚˘˜ˆ 
˝˛˝˛
˚˘˜ˆ 

˝˛˝˛
˚˘˜ˆ 
˝˛˝˛
˚˘˜ˆ 
˝˛˝˛
˚˘˜ˆ 
˝˛˝˛
˚˘˜ˆ 
	
	

	
	
	
	

	

	

	

	

	

		
	
	
	
	
	
	
	
	
	
	

	

	

	

	

	

	

	

	

	

	

		
	
	
	
	
	


˘ˇˆ
	

	
Figure3.ExamplenetworkarchitecturesforImageNet.
Left
:the
VGG-19model[40](19.6billionFLOPs)asareference.
Mid-
dle
:aplainnetworkwith34parameterlayers(3.6billionFLOPs).
Right
:aresidualnetworkwith34parameterlayers(3.6billion
FLOPs).Thedottedshortcutsincreasedimensions.
Table1
shows
moredetailsandothervariants.
ResidualNetwork.
Basedontheaboveplainnetwork,we
insertshortcutconnections(Fig.3,right)whichturnthe

networkintoitscounterpartresidualversion.Theidentity

shortcuts(Eqn.(1))canbedirectlyusedwhentheinputand

outputareofthesamedimensions(solidlineshortcutsin

Fig.3).When the dimensionsincrease(dottedlineshortcuts

inFig.3),weconsidertwo options:(A)The shortcutstill

performsidentitymapping,withextra zeroentriespadded

forincreasingdimensions.Thisoptionintroducesnoextra

parameter;(B)TheprojectionshortcutinEqn.(2)isusedto

matchdimensions(doneby1
×
1convolutions). Forboth
options,whentheshortcutsgoacrossfeaturemapsoftwo

sizes,theyareperformedwithastrideof2.

3.4.Implementation
OurimplementationforImageNetfollowsthepractice
in[21,40].Theimageisresizedwithitsshortersideran-

domlysampledin
[256
,
480]
forscaleaugmentation[40].
A224
×
224cropisrandomlysampledfromanimageorits
horizontalﬂip,withtheper-pixelmeansubtracted[21].The

standardcoloraugmentationin[21]isused.Weadoptbatch

normalization(BN)[16]rightaftereachconvolutionand

beforeactivation,following[16].Weinitializetheweights

asin[12]andtrainallplain/residualnets fromscratch.We

useSGDwithamini-batchsizeof256.Thelearningrate

startsfrom0.1andisdividedby10whentheerrorplateaus,

andthemodelsaretrainedforupto
60
×
10
4
iterations.We
useaweightdecayof0.0001andamomentumof0.9.We

donotusedropout[13],followingthepracticein[16].
Intesting,forcomparisonstudiesweadoptthestandard
10-croptesting[21].Forbestresults,weadoptthefully-

convolutionalformasin[40,12],andaveragethescores

atmultiplescales(imagesareresizedsuchthattheshorter

sideisin
{
224
,
256
,
384
,
480
,
640
}
).
4.Experiments

4.1.ImageNetClassiﬁcation
Weevaluateourmethodonthe ImageNet 2012classiﬁ-
cationdataset[35]thatconsistsof1000classes.Themodels

are trainedonthe 1.28milliontrainingimages, andevalu-

ated onthe50kvalidationimages.Wealsoobtainaﬁnal

resultonthe100ktestimages,reportedbythetestserver.

Weevaluatebothtop-1andtop-5errorrates.

PlainNetworks.
Weﬁrstevaluate18-layerand34-layer
plainnets.The34-layerplainnetisinFig.3(middle).The

18-layerplainnetisofasimilarform.SeeTable1forde-

tailedarchitectures.
Theresults inTable2showthatthedeeper34-layerplain
nethashighervalidationerrorthantheshallower18-layer

plainnet.Torevealthereasons,inFig.4(left)wecom-

paretheirtraining/validationerrorsduringthetrainingpro-

cedure.Wehaveobservedthedegradationproblem-the
4
773

layername
outputsize
18-layer
34-layer
50-layer
101-layer
152-layer
conv1
112
×
112
7
×
7,64,stride2
conv2
x
56
×
56
3
×
3maxpool,stride2

3
×
3,64
3
×
3,64

×
2

3
×
3,64
3
×
3,64

×
3


1
×
1,64
3
×
3,64
1
×
1,256


×
3


1
×
1,64
3
×
3,64
1
×
1,256


×
3


1
×
1,64
3
×
3,64
1
×
1,256


×
3
conv3
x
28
×
28

3
×
3,128
3
×
3,128

×
2

3
×
3,128
3
×
3,128

×
4


1
×
1,128
3
×
3,128
1
×
1,512


×
4


1
×
1,128
3
×
3,128
1
×
1,512


×
4


1
×
1,128
3
×
3,128
1
×
1,512


×
8
conv4
x
14
×
14

3
×
3,256
3
×
3,256

×
2

3
×
3,256
3
×
3,256

×
6


1
×
1,256
3
×
3,256
1
×
1,1024


×
6


1
×
1,256
3
×
3,256
1
×
1,1024


×
23


1
×
1,256
3
×
3,256
1
×
1,1024


×
36
conv5
x
7
×
7

3
×
3,512
3
×
3,512

×
2

3
×
3,512
3
×
3,512

×
3


1
×
1,512
3
×
3,512
1
×
1,2048


×
3


1
×
1,512
3
×
3,512
1
×
1,2048


×
3


1
×
1,512
3
×
3,512
1
×
1,2048


×
3
1
×
1
averagepool,1000-dfc,softmax
FLOPs
1.8
×
10
9
3.6
×
10
9
3.8
×
10
9
7.6
×
10
9
11.3
×
10
9
Table1.ArchitecturesforImageNet.Buildingblocksareshowninbrackets(seealsoFig.5),withthenumbersofblocksstacked.Down-

samplingisperformedbyconv3
1, conv4
1, andconv5
1withastrideof2.
0
10
20
30
40
50
20
30
40
50
60
iter. (1e4)
error (%)
plain-18
plain-34
0
10
20
30
40
50
20
30
40
50
60
iter. (1e4)
error (%)
ResNet-18
ResNet-34
18-layer
34-layer
18-layer
34-layer
Figure4.Trainingon
ImageNet
.Thincurvesdenote trainingerror,andboldcurvesdenote validationerrorofthecentercrops.Left:plain
networksof18and 34layers.Right:ResNetsof18and 34layers.Inthis plot,theresidualnetworkshavenoextraparametercomparedto

theirplaincounterparts.
plain
ResNet
18layers
27.94
27.88
34layers
28.54
25.03
Table2.Top-1error(%,10-croptesting)onImageNetvalidation.

HeretheResNetshavenoextraparametercomparedtotheirplain

counterparts.Fig.4showsthetrainingprocedures.

34-layerplainnethashigher
training
errorthroughoutthe
whole trainingprocedure,eventhoughthesolutionspace

ofthe18-layerplainnetworkisasubspaceofthatofthe

34-layerone.
Wearguethatthisoptimizationdifﬁcultyis
unlikely
to
becausedbyvanishinggradients.Theseplainnetworksare

trainedwithBN[16],whichensuresforwardpropagated

signalstohavenon-zerovariances.Wealsoverifythatthe

backwardpropagatedgradientsexhibithealthynormswith

BN.Soneitherforwardnorbackwardsignalsvanish.In

fact, the34-layerplainnetisstillabletoachievecompet-

itiveaccuracy(Table3),suggestingthatthesolverworks

tosomeextent.We conjecturethatthedeepplainnetsmay

haveexponentiallylowconvergencerates,whichimpactthe
reducingofthetrainingerror
3
.Thereasonfor suchopti-
mizationdifﬁcultieswillbestudiedinthefuture.

ResidualNetworks.
Nextweevaluate18-layerand34-
layerresidualnets(
ResNets
).Thebaselinearchitectures
arethesameastheaboveplainnets,expectthatashortcut

connectionisaddedtoeachpairof3
×
3ﬁltersasinFig.3
(right).In theﬁrstcomparison(Table2andFig.4right),

weuseidentitymapping forallshortcuts andzero-padding

forincreasingdimensions(optionA).Sotheyhave
noextra
parameter
comparedtotheplaincounterparts.
WehavethreemajorobservationsfromTable2and
Fig.4.First,thesituationisreversedwithresiduallearn-

ing–the34-layerResNetisbetterthanthe18-layerResNet

(by2.8%). Moreimportantly,the34-layerResNetexhibits

considerablylowertrainingerrorandisgeneralizabletothe

validationdata.Thisindicatesthatthedegradationproblem

iswelladdressedinthissettingandwe managetoobtain

accuracygainsfromincreaseddepth.
Second,comparedtoitsplaincounterpart,the34-layer
3
Wehaveexperimentedwithmoretrainingiterations(3
×
)andstillob-
servedthedegradationproblem,suggestingthatthisproblemcannotbe

feasiblyaddressedbysimplyusingmore iterations.
5
774

model
top-1err.top-5err.
VGG-16[40]
28.079.33
GoogLeNet[43]
-9.15
PReLU-net[12]
24.277.38
plain-34
28.5410.02
ResNet-34A
25.037.76
ResNet-34B
24.527.46
ResNet-34C
24.197.40
ResNet-50
22.856.71
ResNet-101
21.756.05
ResNet-152
21.435.71
Table3.Errorrates(%,
10-crop
testing)on ImageNetvalidation.
VGG-16isbasedonourtest.ResNet-50/101/152 areofoptionB

thatonlyusesprojectionsforincreasingdimensions.
method
top-1err.top-5err.
VGG[40](ILSVRC’14)
- 8.43
†
GoogLeNet[43](ILSVRC’14)
-7.89
VGG[40]
(v5)
24.47.1
PReLU-net[12]
21.595.71
BN-inception[16]
21.995.81
ResNet-34B
21.845.71
ResNet-34C
21.535.60
ResNet-50
20.745.25
ResNet-101
19.874.60
ResNet-152
19.384.49
Table4.Errorrates(%)of
single-model
resultsontheImageNet
validationset(except
†
reportedonthetestset).
method
top-5err.(
test
)
VGG[40](ILSVRC’14)
7.32
GoogLeNet[43](ILSVRC’14)
6.66
VGG[40]
(v5)
6.8
PReLU-net[12]
4.94
BN-inception[16]
4.82
ResNet(ILSVRC’15)
3.57
Table5.Errorrates(%)of
ensembles
.Thetop-5errorisonthe
testsetofImageNetandreportedbythetestserver.

ResNetreducesthetop-1errorby3.5%(Table2),resulting

fromthesuccessfullyreducedtrainingerror(Fig.4right
vs
.
left).Thiscomparisonveriﬁestheeffectivenessofresidual

learningonextremelydeepsystems.
Last,wealsonotethatthe18-layerplain/residualnets
arecomparablyaccurate(Table2),butthe18-layerResNet

convergesfaster(Fig.4right
vs
.left).Whenthenetis“not
overlydeep”(18layershere),thecurrentSGDsolverisstill

abletoﬁndgoodsolutionstotheplainnet.Inthiscase,the

ResNeteasestheoptimizationbyprovidingfasterconver-

gence attheearlystage.

Identityvs
.ProjectionShortcuts.
Wehaveshownthat



	








	

	
Figure5.Adeeperresidualfunction
F
forImageNet.Left:a
buildingblock(on56
×
56featuremaps)asinFig.3forResNet-
34.Right:a“bottleneck”buildingblockforResNet-50/101/152.

parameter-free,identityshortcutshelpwithtraining.Next

weinvestigateprojectionshortcuts(Eqn.(2)).InTable3we

comparethree options:(A)zero-paddingshortcutsareused

forincreasingdimensions,andallshortcutsareparameter-

free(thesameasTable2andFig.4right);(B)projec-

tionshortcutsareusedforincreasing dimensions,andother

shortcutsareidentity;and(C)allshortcutsareprojections.
Table 3showsthatallthreeoptionsareconsiderablybet-
terthantheplaincounterpart.Bis slightlybetterthanA.We

arguethatthisis becausethezero-paddeddimensionsinA

indeedhavenoresiduallearning.Cismarginallybetterthan

B,andweattribute thistotheextraparametersintroduced

bymany(thirteen)projectionshortcuts.Butthesmalldif-

ferencesamongA/B/Cindicatethatprojectionshortcutsare

notessentialforaddressingthedegradationproblem.Sowe

donotuseoptionCintherestofthispaper,toreducemem-

ory/timecomplexityandmodelsizes.Identityshortcutsare

particularlyimportantfornotincreasingthecomplexityof

thebottleneckarchitecturesthatareintroducedbelow.

DeeperBottleneckArchitectures.
Nextwedescribeour
deepernetsforImageNet.Becauseofconcerns onthetrain-

ingtimethatwecanafford,wemodifythebuildingblock

asa
bottleneck
design
4
.Foreachresidualfunction
F
,we
useastackof3layersinsteadof2(Fig.5).Thethreelayers

are1
×
1,3
×
3,and1
×
1convolutions,wherethe1
×
1layers
areresponsiblefor reducingandthenincreasing(restoring)

dimensions,leavingthe3
×
3 layerabottleneckwithsmaller
input/outputdimensions.Fig.5 showsanexample,where

bothdesignshavesimilartimecomplexity.
Theparameter-freeidentityshortcutsareparticularlyim-
portantforthebottleneckarchitectures.Iftheidentityshort-

cutinFig. 5(right)isreplacedwithprojection,onecan

showthatthetimecomplexityandmodelsizearedoubled,

astheshortcutisconnectedtothetwohigh-dimensional

ends.Soidentityshortcutsleadtomoreefﬁcientmodels

for thebottleneckdesigns.
50-layerResNet:
Wereplaceeach2-layerblockinthe
4
Deeper
non
-bottleneckResNets(
e.g
.,Fig.5left)alsogainaccuracy
fromincreaseddepth(asshownonCIFAR-10),butarenotaseconomical

asthebottleneckResNets.Sotheusageofbottleneckdesignsismainlydue

topracticalconsiderations.Wefurthernotethatthedegradationproblem

ofplainnetsis alsowitnessedforthebottleneckdesigns.
6
775

34-layernetwiththis3-layerbottleneckblock,resultingin

a50-layerResNet(Table 1).WeuseoptionBforincreasing

dimensions.Thismodelhas3.8billionFLOPs.
101-layerand152-layerResNets:
Weconstruct101-
layerand152-layerResNetsbyusing more3-layerblocks

(Table1).Remarkably,althoughthedepthissigniﬁcantly

increased,the152-layerResNet(11.3billion FLOPs)still

haslower complexity
thanVGG-16/19nets(15.3/19.6bil-
lionFLOPs).
The50/101/152-layerResNetsaremoreaccuratethan
the34-layerones byconsiderablemargins(Table3and4).

Wedonotobservethedegradationproblemandthusen-

joysigniﬁcant accuracygainsfromconsiderablyincreased

depth.Thebeneﬁtsofdeptharewitnessedforallevaluation

metrics(Table3and4).

ComparisonswithState-of-the-artMethods.
InTable4
wecompare withthe previousbestsingle-modelresults.

Ourbaseline34-layerResNetshaveachievedverycompet-

itiveaccuracy.Our152-layerResNethasasingle-model

top-5 validationerrorof4.49%.Thissingle-modelresult

outperformsallpreviousensembleresults(Table5).We

combinesixmodelsofdifferentdepthtoformanensemble

(onlywithtwo152-layeronesatthetimeofsubmitting).

Thisleadsto
3.57%
top-5 erroronthetestset(Table5).
Thisentrywonthe1stplaceinILSVRC 2015.

4.2.CIFAR-10and Analysis
WeconductedmorestudiesontheCIFAR-10dataset
[20],whichconsists of50ktrainingimagesand10ktest-

ingimagesin10classes.Wepresentexperimentstrained

onthetraining setandevaluatedon thetestset.Ourfocus

isonthebehaviorsofextremelydeepnetworks,butnoton

pushingthestate-of-the-artresults, soweintentionallyuse

simplearchitecturesasfollows.
Theplain/residualarchitecturesfollow theforminFig.3
(middle/right).Thenetworkinputsare32
×
32images,with
theper-pixelmeansubtracted.Theﬁrstlayeris3
×
3convo-
lutions.Thenweuseastackof
6
n
layerswith3
×
3convo-
lutionsonthefeaturemapsofsizes
{
32
,
16
,
8
}
respectively,
with2
n
layers foreachfeaturemapsize.Thenumbersof
ﬁltersare
{
16
,
32
,
64
}
respectively.Thesubsamplingisper-
formedbyconvolutionswithastrideof2.Thenetwork ends

withaglobalaveragepooling,a10-wayfully-connected

layer,andsoftmax. Therearetotally6
n
+2stackedweighted
layers.Thefollowingtablesummarizesthearchitecture:
outputmapsize
32
×
32
16
×
16
8
×
8
#layers
1+2
n
2
n
2
n
#ﬁlters
16
32
64
When shortcutconnectionsareused,theyareconnected

tothepairsof3
×
3layers(totally
3
n
shortcuts).Onthis
datasetweuseidentityshortcutsinallcases(
i.e
.,optionA),
method
error(%)
Maxout[9]
9.38
NIN[25]
8.81
DSN[24]
8.22
#layers
#params
FitNet[34]
19
2.5M
8.39
Highway[41,42]
19
2.3M
7.54
(7.72
±
0.16)
Highway[41,42]
32
1.25M
8.80
ResNet
20
0.27M
8.75
ResNet
32
0.46M
7.51
ResNet
44
0.66M
7.17
ResNet
56
0.85M
6.97
ResNet
110
1.7M
6.43
(6.61
±
0.16)
ResNet
1202
19.4M
7.93
Table6.Classiﬁcationerroronthe
CIFAR-10
testset.Allmeth-
odsarewithdataaugmentation.ForResNet-110,werunit5 times

andshow“best(mean
±
std)”asin[42].
soourresidualmodelshave exactlythesamedepth,width,

andnumberofparametersastheplaincounterparts.
Weuseaweightdecayof0.0001andmomentumof0.9,
andadopttheweightinitializationin[12]andBN[16]but

withnodropout.Thesemodelsaretrainedwithamini-

batchsizeof128ontwoGPUs.Westartwithalearning

rateof0.1,divideitby10at32kand48kiterations, and

terminatetrainingat64kiterations,whichisdeterminedon

a45k/5ktrain/valsplit.Wefollowthesimpledataaugmen-

tationin[24]fortraining:4pixelsarepaddedoneachside,

anda32
×
32cropisrandomlysampledfromthepadded
imageor itshorizontalﬂip.Fortesting,weonlyevaluate

thesingleviewof theoriginal32
×
32image.
Wecompare
n
=
{
3
,
5
,
7
,
9
}
,leadingto20,32,44,and
56-layernetworks.Fig. 6(left)showsthe behaviorsofthe

plainnets.Thedeepplainnetssufferfromincreaseddepth,

andexhibithighertrainingerrorwhengoingdeeper.This

phenomenonissimilartothatonImageNet(Fig. 4,left) and

onMNIST(see [41]),suggestingthat suchanoptimization

difﬁcultyisafundamentalproblem.
Fig.6(middle)showsthebehaviorsofResNets.Also
similartotheImageNetcases(Fig.4,right),ourResNets

managetoovercometheoptimizationdifﬁcultyanddemon-

strateaccuracygainswhenthedepthincreases.
Wefurtherexplore
n
=18
thatleadstoa110-layer
ResNet.Inthiscase,weﬁndthattheinitiallearningrate

of0.1isslightlytoolargetostartconverging
5
.Soweuse
0.01towarmupthetraininguntilthetrainingerrorisbelow

80%(about400iterations),and thengobackto0.1andcon-

tinuetraining.Therestofthe learningscheduleisasdone

previously.This110-layernetworkconvergeswell(Fig.6,

middle).Ithas
fewer
parametersthanotherdeepandthin
5
Withaninitiallearningrateof0.1,itstartsconverging(
<
90%error)
afterseveralepochs,butstillreachessimilaraccuracy.
7
776

0
1
2
3
4
5
6
0
5
10
20
iter. (1e4)
error (%)
plain-20
plain-32
plain-44
plain-56
0
1
2
3
4
5
6
0
5
10
20
iter. (1e4)
error (%)
ResNet-20
ResNet-32
ResNet-44
ResNet-56
ResNet-110
56-layer

20-layer
110-layer
20-layer
4
5
6
0
1
5
10
20
iter. (1e4)
error (%)
residual-110
residual-1202
Figure6.Trainingon
CIFAR-10
.Dashedlinesdenotetrainingerror,andboldlinesdenotetestingerror.
Left
:plainnetworks.Theerror
ofplain-110ishigherthan60%andnotdisplayed.
Middle
:ResNets.
Right
:ResNetswith110and1202layers.
0
20
40
60
80
100
1
2
3
layer index (sorted by magnitude)
std
plain-20
plain-56
ResNet-20
ResNet-56
ResNet-110
0
20
40
60
80
100
1
2
3
layer index (original)
std
plain-20
plain-56
ResNet-20
ResNet-56
ResNet-110
Figure7.Standarddeviations(std)oflayerresponsesonCIFAR-

10.Theresponsesaretheoutputsofeach3
×
3layer,afterBNand
beforenonlinearity.
Top
:thelayersareshownintheiroriginal
order.
Bottom
:theresponsesarerankedindescendingorder.
networkssuchasFitNet[34]andHighway[41](Table6),

yetisamongthestate-of-the-artresults(6.43%,Table6).

AnalysisofLayerResponses.
Fig.7showsthestandard
deviations(std)ofthelayerresponses.Theresponses are

theoutputsofeach3
×
3layer,afterBNandbeforeother
nonlinearity(ReLU/addition).ForResNets,thisanaly-

sisrevealstheresponsestrengthof theresidualfunctions.

Fig.7showsthatResNetshavegenerallysmallerresponses

thantheir plaincounterparts.Theseresultssupportourba-

sicmotivation(Sec.3.1)thattheresidualfunctions might

begenerally closertozerothan thenon-residualfunctions.

WealsonoticethatthedeeperResNethassmallermagni-

tudesof responses,asevidencedbythecomparisonsamong

ResNet-20,56,and110inFig.7.Whentherearemore

layers,anindividual layerofResNetstendstomodifythe

signalless.

ExploringOver1000layers.
Weexploreanaggressively
deepmodelofover1000layers.Weset
n
=200
that
leadstoa1202-layernetwork,whichistrainedas described

above.Ourmethodshows
nooptimizationdifﬁculty
,and
this
10
3
-layernetworkisabletoachieve
trainingerror
<
0.1%(Fig.6,right).Itstesterrorisstillfairlygood
(7.93%,Table6).
Buttherearestillopenproblemsonsuchaggressively
deepmodels.Thetestingresultofthis1202-layernetwork

isworsethanthatofour110-layernetwork,althoughboth
trainingdata
07+12
07++12
testdata
VOC07test
VOC12test
VGG-16
73.2
70.4
ResNet-101
76.4
73.8
Table7.ObjectdetectionmAP(%)onthePASCALVOC

2007/2012testsetsusing
baseline
FasterR-CNN.Seealsoap-
pendixforbetterresults.
metric
mAP@.5
mAP@[.5,.95]
VGG-16
41.5
21.2
ResNet-101
48.4
27.2
Table8.ObjectdetectionmAP(%)ontheCOCOvalidationset

usingbaseline
FasterR-CNN.Seealsoappendixforbetterresults.
havesimilartrainingerror.Wearguethat thisis becauseof

overﬁtting.The1202-layernetworkmaybeunnecessarily

large(19.4M)forthis smalldataset.Strongregularization

suchasmaxout[9]ordropout[13]isappliedtoobtainthe

bestresults([9,25,24,34])onthisdataset.Inthis paper,we

use nomaxout/dropoutandjustsimplyimposeregulariza-

tionviadeepandthinarchitecturesbydesign,withoutdis-

tractingfromthefocusonthedifﬁcultiesofoptimization.

Butcombiningwithstrongerregularizationmayimprove

results,whichwe willstudyinthefuture.

4.3.ObjectDetectiononPASCALandMSCOCO
Ourmethodhasgoodgeneralizationperformanceon
otherrecognitiontasks.Table7and8showtheobjectde-

tectionbaselineresultsonPASCALVOC2007 and2012

[5]and COCO[26].Weadopt
FasterR-CNN
[32]asthede-
tection method.Hereweareinterestedintheimprovements

ofreplacingVGG-16[40]withResNet-101.Thedetection

implementation(seeappendix)ofusingbothmodelsisthe

same,sothegainscanonlybeattributedtobetternetworks.

Mostremarkably,onthechallengingCOCOdatasetweob-

taina6.0%increaseinCOCO’s standardmetric(mAP@[.5,

.95]),whichisa28%relativeimprovement.Thisgainis

solelyduetothelearnedrepresentations.
Basedondeepresidual nets, wewonthe1st placesin
severaltracksinILSVRC&COCO2015competitions:Im-

ageNetdetection,ImageNetlocalization,COCOdetection,

andCOCOsegmentation.Thedetailsareintheappendix.
8
777

References
[1]Y. Bengio,P. Simard,andP.Frasconi.Learninglong-termdependen-
cieswithgradientdescentisdifﬁcult.
IEEETransactionsonNeural
Networks
,5(2):157–166,1994.
[2]C.M.Bishop.
Neuralnetworksforpatternrecognition
.Oxford
universitypress,1995.
[3]W.L.Briggs,S.F.McCormick,etal.
AMultigridTutorial
.Siam,
2000.
[4]K.Chatﬁeld,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil
isin thedetails:anevaluationofrecentfeatureencodingmethods.

InBMVC
,2011.
[5]M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zis-
serman.ThePascalVisualObjectClasses(VOC)Challenge.
IJCV
,
pages303–338,2010.
[6]R.Girshick.FastR-CNN.In
ICCV
,2015.
[7]R.Girshick,J.Donahue,T. Darrell,andJ.Malik.Richfeaturehier-
archiesforaccurateobjectdetectionandsemanticsegmentation.In

CVPR,2014.
[8]X.GlorotandY.Bengio.Understandingthedifﬁcultyoftraining
deepfeedforwardneuralnetworks.In
AISTATS
,2010.
[9]I.J.Goodfellow,D.Warde-Farley,M.Mirza,A.Courville,and
Y.Bengio.Maxout networks.
arXiv:1302.4389
,2013.
[10]K.HeandJ.Sun.Convolutionalneuralnetworksatconstrainedtime
cost.In
CVPR
,2015.
[11]K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep
convolutionalnetworksforvisualrecognition.In
ECCV
,2014.
[12]K.He,X.Zhang,S.Ren, andJ.Sun.Delvingdeepintorectiﬁers:
Surpassinghuman-levelperformanceonimagenetclassiﬁcation.In

ICCV,2015.
[13]G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,and
R.R.Salakhutdinov.Improvingneuralnetworksbypreventingco-

adaptationoffeaturedetectors.
arXiv:1207.0580
,2012.
[14]S. Hochreiter.Untersuchungenzudynamischenneuronalennetzen.
Diplomathesis,TUMunich
,1991.
[15]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Neural
computation
,9(8):1735–1780,1997.
[16]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeep
networktrainingbyreducinginternalcovariateshift. In
ICML
,2015.
[17]H.Jegou,M.Douze,andC.Schmid.Productquantizationfornearest
neighborsearch.
TPAMI
,33,2011.
[18]H.Jegou,F.Perronnin,M.Douze,J.Sanchez,P.Perez,and
C.Schmid.Aggregatinglocalimagedescriptorsintocompactcodes.

TPAMI
,2012.
[19]Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,
S.Guadarrama,andT.Darrell.Caffe:Convolutionalarchitecturefor

fastfeatureembedding.
arXiv:1408.5093
,2014.
[20]A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyim-
ages.
TechReport
,2009.
[21]A.Krizhevsky,I.Sutskever,and G.Hinton.Imagenetclassiﬁcation
withdeepconvolutionalneuralnetworks.In
NIPS
,2012.
[22]Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,
W.Hubbard,andL.D.Jackel.Backpropagationappliedtohand-

writtenzipcoderecognition.
Neuralcomputation
,1989.
[23]Y.LeCun,L.Bottou,G.B.Orr,andK.-R.M
¨
uller.Efﬁcientbackprop.
In
NeuralNetworks: TricksoftheTrade
,pages9–50.Springer,1998.
[24]C.-Y.Lee,S.Xie,P.Gallagher, Z.Zhang,andZ.Tu.Deeply-
supervisednets.
arXiv:1409.5185
,2014.
[25]M.Lin,Q.Chen,andS.Yan.Networkinnetwork.
arXiv:1312.4400
,
2013.
[26]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
P.Doll
´
ar,andC.L.Zitnick.MicrosoftCOCO:Commonobjectsin
context. In
ECCV
.2014.
[27]J.Long,E.Shelhamer, andT.Darrell. Fullyconvolutionalnetworks
forsemanticsegmentation.In
CVPR
,2015.
[28]G.Mont
´
ufar, R.Pascanu,K.Cho,andY.Bengio.Onthenumberof
linearregionsofdeepneuralnetworks.In
NIPS
,2014.
[29]V.NairandG.E.Hinton.Rectiﬁedlinearunitsimproverestricted
boltzmannmachines.In
ICML
,2010.
[30]F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor
imagecategorization.In
CVPR
,2007.
[31]T.Raiko,H.Valpola, andY.LeCun.Deeplearningmadeeasierby
lineartransformationsinperceptrons.In
AISTATS
,2012.
[32]S.Ren,K.He,R.Girshick,andJ.Sun.FasterR-CNN:Towards
real-timeobjectdetectionwithregionproposalnetworks. In
NIPS
,
2015.
[33]B.D. Ripley.
Patternrecognitionandneuralnetworks
.Cambridge
universitypress,1996.
[34]A.Romero,N.Ballas,S.E.Kahou,A.Chassang,C.Gatta,and
Y.Bengio.Fitnets:Hintsforthindeepnets.In
ICLR
,2015.
[35]O.Russakovsky,J.Deng,H.Su,J.Krause,S. Satheesh,S. Ma,
Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,etal.Imagenet

largescalevisualrecognitionchallenge.
arXiv:1409.0575
,2014.
[36]A.M.Saxe,J.L.McClelland,andS.Ganguli.Exactsolutionsto
thenonlineardynamicsoflearningindeeplinearneuralnetworks.

arXiv:1312.6120,2013.
[37]N. N. Schraudolph.Acceleratedgradientdescentbyfactor-centering
decomposition.Technicalreport,1998.
[38]N.N.Schraudolph.Centeringneuralnetworkgradientfactors.In
NeuralNetworks:Tricksof theTrade
,pages207–226.Springer,
1998.
[39]P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le-
Cun.Overfeat:Integratedrecognition,localizationanddetection

usingconvolutionalnetworks.In
ICLR
,2014.
[40]K.Simonyanand A.Zisserman.Verydeepconvolutionalnetworks
forlarge-scaleimagerecognition.In
ICLR
,2015.
[41]R.K.Srivastava,K.Greff, andJ.Schmidhuber.Highwaynetworks.
arXiv:1505.00387
,2015.
[42]R.K.Srivastava,K.Greff,andJ.Schmidhuber.Training verydeep
networks.
1507.06228
,2015.
[43]C.Szegedy,W.Liu,Y.Jia,P. Sermanet,S.Reed,D.Anguelov,D.Er-
han,V.Vanhoucke,andA.Rabinovich.Goingdeeperwithconvolu-

tions.In
CVPR
,2015.
[44]R.Szeliski.Fast surfaceinterpolationusinghierarchical basisfunc-
tions.
TPAMI
,1990.
[45]R.Szeliski.Locallyadaptedhierarchicalbasispreconditioning.In
SIGGRAPH
,2006.
[46]T. Vatanen,T. Raiko,H.Valpola,andY. LeCun.Pushingstochas-
ticgradienttowardssecond-ordermethods–backpropagationlearn-

ingwithtransformationsinnonlinearities.In
NeuralInformation
Processing
,2013.
[47]A.VedaldiandB.Fulkerson.VLFeat:Anopenandportablelibrary
ofcomputervisionalgorithms,2008.
[48]W.VenablesandB.Ripley. Modernappliedstatisticswiths-plus.
1999.
[49]M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-
tionalneuralnetworks.In
ECCV
,2014.
9
778

