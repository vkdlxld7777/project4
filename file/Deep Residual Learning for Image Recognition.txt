DeepResidualLearningfor ImageRecognition
KaimingHeXiangyu ZhangShaoqingRen JianSun
MicrosoftResearch
{
kahe,v-xiangz,v-shren,jiansun
}
@microsoft.com
Abstract
DeeperneuralnetworksaremoredifÔ¨Åculttotrain.We
present aresiduallearningframeworktoeasethetraining

ofnetworksthataresubstantiallydeeperthanthoseused

previously.Weexplicitlyreformulate thelayersaslearn-

ing residualfunctionswithreferencetothelayerinputs, in-

steadoflearningunreferencedfunctions.Weprovidecom-

prehensive empiricalevidenceshowingthattheseresidual

networksareeasiertooptimize,andcangainaccuracyfrom

considerablyincreaseddepth.OntheImageNetdatasetwe

evaluateresidualnetswithadepth ofupto152layers‚Äî8
√ó
deeperthanVGGnets[40]butstillhavinglowercomplex-

ity.An ensembleoftheseresidualnetsachieves3.57%error

ontheImageNet
test
set.Thisresultwonthe 1stplace onthe
ILSVRC2015classiÔ¨Åcationtask.Wealsopresentanalysis

onCIFAR-10with100and1000layers.
Thedepthofrepresentations isofcentralimportance
formanyvisualrecognitiontasks.Solelydueto ourex-

tremelydeeprepresentations,weobtain a28%relativeim-

provementontheCOCOobjectdetectiondataset.Deep

residualnetsarefoundationsofoursubmissions toILSVRC

&COCO2015 competitions
1
,wherewealsowonthe1st
placesonthetasksofImageNetdetection,ImageNetlocal-

ization,COCOdetection,andCOCOsegmentation.

1.Introduction
Deepconvolutionalneuralnetworks [22,21]haveled
toaseriesofbreakthroughsforimageclassiÔ¨Åcation [21,

49,39].Deepnetworksnaturallyintegratelow/mid/high-

levelfeatures[49]andclassiÔ¨Åersinanend-to-endmulti-

layerfashion,and the‚Äúlevels‚Äù offeaturescanbeenriched

bythenumberofstackedlayers(depth).Recentevidence

[40,43]revealsthatnetworkdepthisofcrucialimportance,

and theleadingresults [40,43,12,16]onthechallenging

ImageNetdataset [35]allexploit‚Äúverydeep‚Äù[40]models,

withadepthofsixteen[40]tothirty[16].Manyothernon-

trivialvisualrecognitiontasks[7,11,6,32,27]havealso
1
http://image-net.org/challenges/LSVRC/2015/
and
http://mscoco.org/dataset/#detections-challenge2015
.
0
1
2
3
4
5
6
0
10
20
iter. (1e4)
training error (%)
0
1
2
3
4
5
6
0
10
20
iter. (1e4)
test error (%)
56-layer

20-layer
56-layer

20-layer
Figure1.Trainingerror(left)andtesterror(right)onCIFAR-10

with20-layerand56-layer‚Äúplain‚Äùnetworks.Thedeepernetwork

hashighertrainingerror,andthustesterror.Similarphenomena

onImageNetispresentedinFig.4.

greatlybeneÔ¨Åtedfromverydeepmodels.
DrivenbythesigniÔ¨Åcanceofdepth,aquestionarises:
Is
learningbetternetworksaseasyasstackingmorelayers?

Anobstacletoansweringthisquestionwasthenotorious

problemofvanishing/explodinggradients[14,1,8],which

hamperconvergencefromthebeginning.Thisproblem,

however,hasbeenlargelyaddressedby normalizedinitial-

ization[23,8,36,12]andintermediatenormalizationlayers

[16],whichenablenetworkswithtensoflayerstostartcon-

vergingforstochasticgradientdescent(SGD)withback-

propagation[22].
Whendeepernetworksareableto startconverging,a
degradation
problemhasbeenexposed:withthenetwork
depthincreasing,accuracygetssaturated(whichmightbe

unsurprising)andthendegradesrapidly.Unexpectedly,

suchdegradationis
notcausedbyoverÔ¨Åtting
,andadding
morelayersto asuitablydeepmodelleadsto
highertrain-
ingerror
,asreportedin[10,41]andthoroughlyveriÔ¨Åedby
ourexperiments.Fig.1showsatypicalexample.
Thedegradation(oftrainingaccuracy)indicatesthatnot
allsystems aresimilarlyeasy tooptimize.Letusconsidera

shallowerarchitectureanditsdeepercounterpartthatadds

morelayersontoit.There existsa solution
byconstruction
tothedeepermodel:theaddedlayersare
identity
mapping,
andtheotherlayersarecopiedfromthelearnedshallower

model.Theexistenceofthisconstructedsolutionindicates

thatadeepermodelshouldproducenohighertrainingerror

thanitsshallowercounterpart.But experimentsshowthat

ourcurrentsolversonhandareunabletoÔ¨Åndsolutionsthat
1
770


	

	

	


	

F
(
x
)
+
x
x
F
(
x
)
x
Figure2. Residuallearning:abuildingblock.
arecomparablygoodorbetterthantheconstructedsolution

(orunabletodosoinfeasibletime).
Inthispaper,weaddressthedegradationproblemby
introducinga
deepresiduallearning
framework.In-
steadofhopingeachfewstackedlayersdirectlyÔ¨Åta

desiredunderlyingmapping,weexplicitlylettheselay-

ersÔ¨Åtaresidualmapping.Formally,denotingthedesired

underlyingmappingas
H
(
x
)
,welet thestackednonlinear
layersÔ¨Åt anothermappingof
F
(
x
):=
H
(
x
)
‚àí
x
.Theorig-
inalmappingisrecastinto
F
(
x
)+
x
.Wehypothesizethatit
iseasiertooptimizetheresidualmappingthanto optimize

theoriginal,unreferencedmapping.To theextreme,ifan

identitymappingwereoptimal,itwouldbeeasiertopush

theresidualtozerothantoÔ¨Åtanidentitymappingbyastack

ofnonlinearlayers.
Theformulationof
F
(
x
)+
x
canberealizedbyfeedfor-
wardneuralnetworkswith‚Äúshortcutconnections‚Äù(Fig.2).

Shortcutconnections[2,33,48]are thoseskippingoneor

morelayers.Inourcase,theshortcutconnectionssimply

performidentity
mapping,andtheiroutputsareaddedto
theoutputsofthestackedlayers(Fig.2).Identityshort-

cutconnectionsaddneitherextraparameternorcomputa-

tionalcomplexity.Theentirenetworkcanstillbetrained

end-to-endbySGDwithbackpropagation,andcanbeeas-

ilyimplementedusingcommonlibraries(
e.g
.,Caffe[19])
withoutmodifyingthesolvers.
We presentcomprehensive experimentsonImageNet
[35]toshowthedegradationproblemandevaluateour

method.Weshowthat:1)Ourextremelydeepresidualnets

areeasytooptimize,butthecounterpart‚Äúplain‚Äùnets(that

simplystacklayers)exhibithighertrainingerrorwhen the

depthincreases;2)Ourdeepresidualnetscaneasilyenjoy

accuracygains fromgreatly increaseddepth,producingre-

sultssubstantiallybetterthanpreviousnetworks.
SimilarphenomenaarealsoshownontheCIFAR-10set
[20],suggestingthattheoptimizationdifÔ¨Åcultiesandthe

effectsofour methodarenotjustakintoaparticulardataset.

Wepresentsuccessfullytrainedmodelsonthisdatasetwith

over 100layers,andexploremodelswithover 1000layers.
OntheImageNetclassiÔ¨Åcationdataset[35],weobtain
excellentresultsbyextremelydeepresidualnets.Our152-

layerresidualnetisthedeepest networkeverpresentedon

ImageNet,whilestillhavinglowercomplexitythanVGG

nets[40]. Ourensemblehas
3.57%
top-5erroronthe
ImageNet
test
set,and
wonthe1stplaceintheILSVRC
2015 classiÔ¨Åcationcompetition
.Theextremelydeeprep-
resentationsalsohaveexcellentgeneralizationperformance

onotherrecognitiontasks,andleadustofurther
winthe
1stplaceson:ImageNetdetection,ImageNetlocalization,

COCOdetection,andCOCOsegmentation
inILSVRC&
COCO2015competitions.Thisstrongevidenceshowsthat

theresiduallearningprincipleisgeneric,andweexpectthat

itisapplicableinothervisionandnon-visionproblems.
2.RelatedWork

ResidualRepresentations.
In imagerecognition,VLAD
[18]isarepresentationthatencodesbytheresidualvectors

withrespecttoadictionary,andFisherVector[30]canbe

formulatedasaprobabilisticversion[18]ofVLAD.Both

ofthemarepowerfulshallowrepresentationsforimagere-

trievalandclassiÔ¨Åcation[4,47]. Forvectorquantization,

encodingresidualvectors[17]isshowntobemoreeffec-

tivethanencodingoriginalvectors.
Inlow-levelvisionandcomputergraphics,forsolv-
ingPartialDifferentialEquations(PDEs),thewidelyused

Multigridmethod[3]reformulatesthesystemassubprob-

lemsatmultiple scales,whereeachsubproblemisrespon-

siblefortheresidualsolutionbetweenacoarserandaÔ¨Åner

scale.AnalternativetoMultigridishierarchicalbasispre-

conditioning[44, 45],whichrelieson variablesthatrepre-

sentresidual vectorsbetweentwoscales.Ithasbeenshown

[3,44,45]thatthesesolversconvergemuchfasterthanstan-

dardsolversthatareunawareoftheresidualnatureofthe

solutions.Thesemethodssuggestthatagoodreformulation

orpreconditioningcan simplifytheoptimization.

ShortcutConnections.
Practicesandtheoriesthatleadto
shortcutconnections[2,33,48]havebeenstudiedforalong

time.Anearlypracticeoftraining multi-layerperceptrons

(MLPs)istoaddalinearlayerconnectedfromthenetwork

inputtotheoutput[33,48].In[43,24],a fewinterme-

diatelayersaredirectlyconnectedtoauxiliaryclassiÔ¨Åers

foraddressingvanishing/explodinggradients.Thepapers

of[38,37,31,46]proposemethodsforcenteringlayerre-

sponses,gradients,andpropagatederrors,implementedby

shortcutconnections.In[43],an‚Äúinception‚Äùlayeriscom-

posedofashortcutbranchandafewdeeperbranches.
Concurrentwithourwork,‚Äúhighwaynetworks‚Äù[41,42]
presentshortcutconnectionswithgatingfunctions[15].

Thesegatesaredata-dependentandhaveparameters,in

contrasttoouridentityshortcutsthatareparameter-free.

Whenagatedshortcutis‚Äúclosed‚Äù (approachingzero),the

layersinhighwaynetworksrepresent
non-residual
func-
tions.Onthecontrary,ourformulationalwayslearns

residualfunctions;ouridentityshortcutsareneverclosed,

andallinformationisalwayspassedthrough,withaddi-

tionalresidual functionstobe learned.In addition,high-
2
771

waynetworkshavenotdemonstratedaccuracygainswith

extremelyincreaseddepth(
e.g
.,over100layers).
3.DeepResidualLearning

3.1.ResidualLearning
Letusconsider
H
(
x
)
asanunderlyingmappingtobe
Ô¨Åtbyafewstackedlayers(notnecessarilytheentirenet),

withx
denotingtheinputstotheÔ¨Årstoftheselayers.Ifone
hypothesizesthatmultiplenonlinearlayerscanasymptoti-

cally approximate complicatedfunctions
2
,thenitisequiv-
alenttohypothesizethattheycanasymptoticallyapproxi-

matetheresidualfunctions,
i.e
.,
H
(
x
)
‚àí
x
(assumingthat
theinputandoutputareofthesamedimensions).So

ratherthan expectstackedlayersto approximate
H
(
x
)
,we
explicitlylettheselayersapproximatearesidualfunction

F(
x
):=
H
(
x
)
‚àí
x
.Theoriginalfunctionthusbecomes
F
(
x
)+
x
.Althoughbothformsshouldbeabletoasymptot-
icallyapproximatethedesiredfunctions(ashypothesized),

theeaseoflearningmight bedifferent.
Thisreformulationismotivatedbythecounterintuitive
phenomenaaboutthedegradationproblem(Fig.1,left).As

wediscussed in theintroduction,iftheaddedlayerscan

be constructedas identity mappings,adeepermodelshould

havetrainingerror nogreater thanitsshallowercounter-

part.Thedegradationproblem suggeststhatthesolvers

mighthavedifÔ¨Åcultiesinapproximatingidentitymappings

bymultiplenonlinearlayers.Withtheresiduallearningre-

formulation,ifidentitymappingsareoptimal,thesolvers

maysimplydrivetheweightsofthemultiplenonlinearlay-

erstoward zerotoapproachidentitymappings.
Inrealcases,itisunlikelythatidentitymappingsareop-
timal,butourreformulationmayhelptopreconditionthe

problem.Iftheoptimalfunctionisclosertoanidentity

mappingthantoazeromapping,itshouldbeeasierforthe

solvertoÔ¨Åndtheperturbations withreferencetoanidentity

mapping,thantolearnthefunctionasanewone.Weshow

byexperiments(Fig.7)thatthelearnedresidualfunctionsin

generalhavesmallresponses,suggestingthatidentitymap-

pingsprovidereasonablepreconditioning.
3.2.IdentityMappingbyShortcuts
Weadoptresiduallearningtoeveryfewstackedlayers.
AbuildingblockisshowninFig.2.Formally,inthispaper

weconsiderabuildingblockdeÔ¨Ånedas:
y
=
F
(
x
,
{
W
i
}
)+
x
.
(1)
Here
x
and
y
aretheinputandoutputvectorsof thelay-
ersconsidered.Thefunction
F
(
x
,
{
W
i
}
)
representsthe
residualmappingtobelearned.FortheexampleinFig.2

thathastwo layers,
F
=
W
2

(
W
1
x
)
inwhich

denotes
2
Thishypothesis,however,isstillanopenquestion.See[28].
ReLU[29]andthebiasesareomittedforsimplifyingno-

tations.Theoperation
F
+
x
isperformedbyashortcut
connectionandelement-wiseaddition. Weadoptthesec-

ondnonlinearityaftertheaddition(
i.e
.,

(
y
)
,seeFig.2).
TheshortcutconnectionsinEqn.(1)introduceneitherex-
traparameternorcomputationcomplexity.Thisisnotonly

attractiveinpracticebutalsoimportantinourcomparisons

betweenplainandresidualnetworks.Wecanfairlycom-

pareplain/residualnetworksthatsimultaneouslyhave the

samenumberofparameters,depth,width,andcomputa-

tionalcost(except forthenegligibleelement-wiseaddition).
Thedimensionsof
x
and
F
mustbeequalinEqn.(1).
Ifthisisnotthecase(
e.g
.,whenchangingtheinput/output
channels),wecanperformalinearprojection
W
s
bythe
shortcutconnectionstomatchthedimensions:
y
=
F
(
x
,
{
W
i
}
)+
W
s
x
.
(2)
Wecanalsouseasquarematrix
W
s
inEqn.(1).Butwewill
showbyexperimentsthattheidentitymapping issufÔ¨Åcient

foraddressingthedegradationproblemandiseconomical,

andthus
W
s
isonlyusedwhenmatchingdimensions.
Theformoftheresidualfunction
F
isÔ¨Çexible.Exper-
imentsinthispaperinvolveafunction
F
thathastwoor
threelayers(Fig.5),whilemorelayersarepossible.Butif

Fhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:
y
=
W
1
x
+
x
,forwhichwehavenot observedadvantages.
Wealsonotethatalthoughtheabovenotationsareabout
fully-connectedlayersforsimplicity,theyareapplicableto

convolutionallayers.Thefunction
F
(
x
,
{
W
i
}
)
canrepre-
sentmultipleconvolutionallayers.Theelement-wiseaddi-

tionisperformedontwofeaturemaps,channelbychannel.

3.3.NetworkArchitectures
We have testedvariousplain/residualnets,andhave ob-
servedconsistentphenomena.Toprovideinstancesfordis-

cussion,we describetwomodelsforImageNetasfollows.

PlainNetwork.
Ourplainbaselines(Fig.3,middle)are
mainlyinspiredbythephilosophyofVGGnets[40](Fig.3,

left).Theconvolutionallayersmostlyhave3
√ó
3Ô¨Åltersand
followtwosimpledesign rules:(i)forthesameoutput

featuremapsize,thelayershavethesamenumberofÔ¨Ål-

ters;and(ii)ifthefeaturemapsizeishalved, thenum-

berofÔ¨Åltersisdoubledso asto preservethe timecom-

plexityperlayer.Weperformdownsamplingdirectlyby

convolutionallayersthathaveastrideof 2.Thenetwork

endswithaglobalaveragepooling layeranda1000-way

fully-connectedlayerwithsoftmax.Thetotalnumberof

weightedlayersis34inFig.3(middle).
Itisworthnoticingthatourmodelhas
fewer
Ô¨Åltersand
lower
complexitythanVGGnets[40](Fig.3,left).Our34-
layerbaselinehas3.6billionFLOPs(multiply-adds),which

isonly18%ofVGG-19(19.6billionFLOPs).
3
772

	
	
	
	

	

	

	

	

	

		
	
	
	
	
	
	
	
	
	
	

	

	

	

	

	

	

	

	

	

	

		
	
	
	
	
	


ÀòÀáÀÜ
	
	

	

	
	
	
	
	

	

	

	

	
	
	
	
	
	
	
	
	
	
Àô

Àô


ÀòÀáÀÜ
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 

ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
ÀùÀõÀùÀõ
ÀöÀòÀúÀÜ 
	
	

	
	
	
	

	

	

	

	

	

		
	
	
	
	
	
	
	
	
	
	

	

	

	

	

	

	

	

	

	

	

		
	
	
	
	
	


ÀòÀáÀÜ
	

	
Figure3.ExamplenetworkarchitecturesforImageNet.
Left
:the
VGG-19model[40](19.6billionFLOPs)asareference.
Mid-
dle
:aplainnetworkwith34parameterlayers(3.6billionFLOPs).
Right
:aresidualnetworkwith34parameterlayers(3.6billion
FLOPs).Thedottedshortcutsincreasedimensions.
Table1
shows
moredetailsandothervariants.
ResidualNetwork.
Basedontheaboveplainnetwork,we
insertshortcutconnections(Fig.3,right)whichturnthe

networkintoitscounterpartresidualversion.Theidentity

shortcuts(Eqn.(1))canbedirectlyusedwhentheinputand

outputareofthesamedimensions(solidlineshortcutsin

Fig.3).When the dimensionsincrease(dottedlineshortcuts

inFig.3),weconsidertwo options:(A)The shortcutstill

performsidentitymapping,withextra zeroentriespadded

forincreasingdimensions.Thisoptionintroducesnoextra

parameter;(B)TheprojectionshortcutinEqn.(2)isusedto

matchdimensions(doneby1
√ó
1convolutions). Forboth
options,whentheshortcutsgoacrossfeaturemapsoftwo

sizes,theyareperformedwithastrideof2.

3.4.Implementation
OurimplementationforImageNetfollowsthepractice
in[21,40].Theimageisresizedwithitsshortersideran-

domlysampledin
[256
,
480]
forscaleaugmentation[40].
A224
√ó
224cropisrandomlysampledfromanimageorits
horizontalÔ¨Çip,withtheper-pixelmeansubtracted[21].The

standardcoloraugmentationin[21]isused.Weadoptbatch

normalization(BN)[16]rightaftereachconvolutionand

beforeactivation,following[16].Weinitializetheweights

asin[12]andtrainallplain/residualnets fromscratch.We

useSGDwithamini-batchsizeof256.Thelearningrate

startsfrom0.1andisdividedby10whentheerrorplateaus,

andthemodelsaretrainedforupto
60
√ó
10
4
iterations.We
useaweightdecayof0.0001andamomentumof0.9.We

donotusedropout[13],followingthepracticein[16].
Intesting,forcomparisonstudiesweadoptthestandard
10-croptesting[21].Forbestresults,weadoptthefully-

convolutionalformasin[40,12],andaveragethescores

atmultiplescales(imagesareresizedsuchthattheshorter

sideisin
{
224
,
256
,
384
,
480
,
640
}
).
4.Experiments

4.1.ImageNetClassiÔ¨Åcation
Weevaluateourmethodonthe ImageNet 2012classiÔ¨Å-
cationdataset[35]thatconsistsof1000classes.Themodels

are trainedonthe 1.28milliontrainingimages, andevalu-

ated onthe50kvalidationimages.WealsoobtainaÔ¨Ånal

resultonthe100ktestimages,reportedbythetestserver.

Weevaluatebothtop-1andtop-5errorrates.

PlainNetworks.
WeÔ¨Årstevaluate18-layerand34-layer
plainnets.The34-layerplainnetisinFig.3(middle).The

18-layerplainnetisofasimilarform.SeeTable1forde-

tailedarchitectures.
Theresults inTable2showthatthedeeper34-layerplain
nethashighervalidationerrorthantheshallower18-layer

plainnet.Torevealthereasons,inFig.4(left)wecom-

paretheirtraining/validationerrorsduringthetrainingpro-

cedure.Wehaveobservedthedegradationproblem-the
4
773

layername
outputsize
18-layer
34-layer
50-layer
101-layer
152-layer
conv1
112
√ó
112
7
√ó
7,64,stride2
conv2
x
56
√ó
56
3
√ó
3maxpool,stride2

3
√ó
3,64
3
√ó
3,64

√ó
2

3
√ó
3,64
3
√ó
3,64

√ó
3


1
√ó
1,64
3
√ó
3,64
1
√ó
1,256


√ó
3


1
√ó
1,64
3
√ó
3,64
1
√ó
1,256


√ó
3


1
√ó
1,64
3
√ó
3,64
1
√ó
1,256


√ó
3
conv3
x
28
√ó
28

3
√ó
3,128
3
√ó
3,128

√ó
2

3
√ó
3,128
3
√ó
3,128

√ó
4


1
√ó
1,128
3
√ó
3,128
1
√ó
1,512


√ó
4


1
√ó
1,128
3
√ó
3,128
1
√ó
1,512


√ó
4


1
√ó
1,128
3
√ó
3,128
1
√ó
1,512


√ó
8
conv4
x
14
√ó
14

3
√ó
3,256
3
√ó
3,256

√ó
2

3
√ó
3,256
3
√ó
3,256

√ó
6


1
√ó
1,256
3
√ó
3,256
1
√ó
1,1024


√ó
6


1
√ó
1,256
3
√ó
3,256
1
√ó
1,1024


√ó
23


1
√ó
1,256
3
√ó
3,256
1
√ó
1,1024


√ó
36
conv5
x
7
√ó
7

3
√ó
3,512
3
√ó
3,512

√ó
2

3
√ó
3,512
3
√ó
3,512

√ó
3


1
√ó
1,512
3
√ó
3,512
1
√ó
1,2048


√ó
3


1
√ó
1,512
3
√ó
3,512
1
√ó
1,2048


√ó
3


1
√ó
1,512
3
√ó
3,512
1
√ó
1,2048


√ó
3
1
√ó
1
averagepool,1000-dfc,softmax
FLOPs
1.8
√ó
10
9
3.6
√ó
10
9
3.8
√ó
10
9
7.6
√ó
10
9
11.3
√ó
10
9
Table1.ArchitecturesforImageNet.Buildingblocksareshowninbrackets(seealsoFig.5),withthenumbersofblocksstacked.Down-

samplingisperformedbyconv3
1, conv4
1, andconv5
1withastrideof2.
0
10
20
30
40
50
20
30
40
50
60
iter. (1e4)
error (%)
plain-18
plain-34
0
10
20
30
40
50
20
30
40
50
60
iter. (1e4)
error (%)
ResNet-18
ResNet-34
18-layer
34-layer
18-layer
34-layer
Figure4.Trainingon
ImageNet
.Thincurvesdenote trainingerror,andboldcurvesdenote validationerrorofthecentercrops.Left:plain
networksof18and 34layers.Right:ResNetsof18and 34layers.Inthis plot,theresidualnetworkshavenoextraparametercomparedto

theirplaincounterparts.
plain
ResNet
18layers
27.94
27.88
34layers
28.54
25.03
Table2.Top-1error(%,10-croptesting)onImageNetvalidation.

HeretheResNetshavenoextraparametercomparedtotheirplain

counterparts.Fig.4showsthetrainingprocedures.

34-layerplainnethashigher
training
errorthroughoutthe
whole trainingprocedure,eventhoughthesolutionspace

ofthe18-layerplainnetworkisasubspaceofthatofthe

34-layerone.
WearguethatthisoptimizationdifÔ¨Åcultyis
unlikely
to
becausedbyvanishinggradients.Theseplainnetworksare

trainedwithBN[16],whichensuresforwardpropagated

signalstohavenon-zerovariances.Wealsoverifythatthe

backwardpropagatedgradientsexhibithealthynormswith

BN.Soneitherforwardnorbackwardsignalsvanish.In

fact, the34-layerplainnetisstillabletoachievecompet-

itiveaccuracy(Table3),suggestingthatthesolverworks

tosomeextent.We conjecturethatthedeepplainnetsmay

haveexponentiallylowconvergencerates,whichimpactthe
reducingofthetrainingerror
3
.Thereasonfor suchopti-
mizationdifÔ¨Åcultieswillbestudiedinthefuture.

ResidualNetworks.
Nextweevaluate18-layerand34-
layerresidualnets(
ResNets
).Thebaselinearchitectures
arethesameastheaboveplainnets,expectthatashortcut

connectionisaddedtoeachpairof3
√ó
3Ô¨ÅltersasinFig.3
(right).In theÔ¨Årstcomparison(Table2andFig.4right),

weuseidentitymapping forallshortcuts andzero-padding

forincreasingdimensions(optionA).Sotheyhave
noextra
parameter
comparedtotheplaincounterparts.
WehavethreemajorobservationsfromTable2and
Fig.4.First,thesituationisreversedwithresiduallearn-

ing‚Äìthe34-layerResNetisbetterthanthe18-layerResNet

(by2.8%). Moreimportantly,the34-layerResNetexhibits

considerablylowertrainingerrorandisgeneralizabletothe

validationdata.Thisindicatesthatthedegradationproblem

iswelladdressedinthissettingandwe managetoobtain

accuracygainsfromincreaseddepth.
Second,comparedtoitsplaincounterpart,the34-layer
3
Wehaveexperimentedwithmoretrainingiterations(3
√ó
)andstillob-
servedthedegradationproblem,suggestingthatthisproblemcannotbe

feasiblyaddressedbysimplyusingmore iterations.
5
774

model
top-1err.top-5err.
VGG-16[40]
28.079.33
GoogLeNet[43]
-9.15
PReLU-net[12]
24.277.38
plain-34
28.5410.02
ResNet-34A
25.037.76
ResNet-34B
24.527.46
ResNet-34C
24.197.40
ResNet-50
22.856.71
ResNet-101
21.756.05
ResNet-152
21.435.71
Table3.Errorrates(%,
10-crop
testing)on ImageNetvalidation.
VGG-16isbasedonourtest.ResNet-50/101/152 areofoptionB

thatonlyusesprojectionsforincreasingdimensions.
method
top-1err.top-5err.
VGG[40](ILSVRC‚Äô14)
- 8.43
‚Ä†
GoogLeNet[43](ILSVRC‚Äô14)
-7.89
VGG[40]
(v5)
24.47.1
PReLU-net[12]
21.595.71
BN-inception[16]
21.995.81
ResNet-34B
21.845.71
ResNet-34C
21.535.60
ResNet-50
20.745.25
ResNet-101
19.874.60
ResNet-152
19.384.49
Table4.Errorrates(%)of
single-model
resultsontheImageNet
validationset(except
‚Ä†
reportedonthetestset).
method
top-5err.(
test
)
VGG[40](ILSVRC‚Äô14)
7.32
GoogLeNet[43](ILSVRC‚Äô14)
6.66
VGG[40]
(v5)
6.8
PReLU-net[12]
4.94
BN-inception[16]
4.82
ResNet(ILSVRC‚Äô15)
3.57
Table5.Errorrates(%)of
ensembles
.Thetop-5errorisonthe
testsetofImageNetandreportedbythetestserver.

ResNetreducesthetop-1errorby3.5%(Table2),resulting

fromthesuccessfullyreducedtrainingerror(Fig.4right
vs
.
left).ThiscomparisonveriÔ¨Åestheeffectivenessofresidual

learningonextremelydeepsystems.
Last,wealsonotethatthe18-layerplain/residualnets
arecomparablyaccurate(Table2),butthe18-layerResNet

convergesfaster(Fig.4right
vs
.left).Whenthenetis‚Äúnot
overlydeep‚Äù(18layershere),thecurrentSGDsolverisstill

abletoÔ¨Åndgoodsolutionstotheplainnet.Inthiscase,the

ResNeteasestheoptimizationbyprovidingfasterconver-

gence attheearlystage.

Identityvs
.ProjectionShortcuts.
Wehaveshownthat



	








	

	
Figure5.Adeeperresidualfunction
F
forImageNet.Left:a
buildingblock(on56
√ó
56featuremaps)asinFig.3forResNet-
34.Right:a‚Äúbottleneck‚ÄùbuildingblockforResNet-50/101/152.

parameter-free,identityshortcutshelpwithtraining.Next

weinvestigateprojectionshortcuts(Eqn.(2)).InTable3we

comparethree options:(A)zero-paddingshortcutsareused

forincreasingdimensions,andallshortcutsareparameter-

free(thesameasTable2andFig.4right);(B)projec-

tionshortcutsareusedforincreasing dimensions,andother

shortcutsareidentity;and(C)allshortcutsareprojections.
Table 3showsthatallthreeoptionsareconsiderablybet-
terthantheplaincounterpart.Bis slightlybetterthanA.We

arguethatthisis becausethezero-paddeddimensionsinA

indeedhavenoresiduallearning.Cismarginallybetterthan

B,andweattribute thistotheextraparametersintroduced

bymany(thirteen)projectionshortcuts.Butthesmalldif-

ferencesamongA/B/Cindicatethatprojectionshortcutsare

notessentialforaddressingthedegradationproblem.Sowe

donotuseoptionCintherestofthispaper,toreducemem-

ory/timecomplexityandmodelsizes.Identityshortcutsare

particularlyimportantfornotincreasingthecomplexityof

thebottleneckarchitecturesthatareintroducedbelow.

DeeperBottleneckArchitectures.
Nextwedescribeour
deepernetsforImageNet.Becauseofconcerns onthetrain-

ingtimethatwecanafford,wemodifythebuildingblock

asa
bottleneck
design
4
.Foreachresidualfunction
F
,we
useastackof3layersinsteadof2(Fig.5).Thethreelayers

are1
√ó
1,3
√ó
3,and1
√ó
1convolutions,wherethe1
√ó
1layers
areresponsiblefor reducingandthenincreasing(restoring)

dimensions,leavingthe3
√ó
3 layerabottleneckwithsmaller
input/outputdimensions.Fig.5 showsanexample,where

bothdesignshavesimilartimecomplexity.
Theparameter-freeidentityshortcutsareparticularlyim-
portantforthebottleneckarchitectures.Iftheidentityshort-

cutinFig. 5(right)isreplacedwithprojection,onecan

showthatthetimecomplexityandmodelsizearedoubled,

astheshortcutisconnectedtothetwohigh-dimensional

ends.SoidentityshortcutsleadtomoreefÔ¨Åcientmodels

for thebottleneckdesigns.
50-layerResNet:
Wereplaceeach2-layerblockinthe
4
Deeper
non
-bottleneckResNets(
e.g
.,Fig.5left)alsogainaccuracy
fromincreaseddepth(asshownonCIFAR-10),butarenotaseconomical

asthebottleneckResNets.Sotheusageofbottleneckdesignsismainlydue

topracticalconsiderations.Wefurthernotethatthedegradationproblem

ofplainnetsis alsowitnessedforthebottleneckdesigns.
6
775

34-layernetwiththis3-layerbottleneckblock,resultingin

a50-layerResNet(Table 1).WeuseoptionBforincreasing

dimensions.Thismodelhas3.8billionFLOPs.
101-layerand152-layerResNets:
Weconstruct101-
layerand152-layerResNetsbyusing more3-layerblocks

(Table1).Remarkably,althoughthedepthissigniÔ¨Åcantly

increased,the152-layerResNet(11.3billion FLOPs)still

haslower complexity
thanVGG-16/19nets(15.3/19.6bil-
lionFLOPs).
The50/101/152-layerResNetsaremoreaccuratethan
the34-layerones byconsiderablemargins(Table3and4).

Wedonotobservethedegradationproblemandthusen-

joysigniÔ¨Åcant accuracygainsfromconsiderablyincreased

depth.ThebeneÔ¨Åtsofdeptharewitnessedforallevaluation

metrics(Table3and4).

ComparisonswithState-of-the-artMethods.
InTable4
wecompare withthe previousbestsingle-modelresults.

Ourbaseline34-layerResNetshaveachievedverycompet-

itiveaccuracy.Our152-layerResNethasasingle-model

top-5 validationerrorof4.49%.Thissingle-modelresult

outperformsallpreviousensembleresults(Table5).We

combinesixmodelsofdifferentdepthtoformanensemble

(onlywithtwo152-layeronesatthetimeofsubmitting).

Thisleadsto
3.57%
top-5 erroronthetestset(Table5).
Thisentrywonthe1stplaceinILSVRC 2015.

4.2.CIFAR-10and Analysis
WeconductedmorestudiesontheCIFAR-10dataset
[20],whichconsists of50ktrainingimagesand10ktest-

ingimagesin10classes.Wepresentexperimentstrained

onthetraining setandevaluatedon thetestset.Ourfocus

isonthebehaviorsofextremelydeepnetworks,butnoton

pushingthestate-of-the-artresults, soweintentionallyuse

simplearchitecturesasfollows.
Theplain/residualarchitecturesfollow theforminFig.3
(middle/right).Thenetworkinputsare32
√ó
32images,with
theper-pixelmeansubtracted.TheÔ¨Årstlayeris3
√ó
3convo-
lutions.Thenweuseastackof
6
n
layerswith3
√ó
3convo-
lutionsonthefeaturemapsofsizes
{
32
,
16
,
8
}
respectively,
with2
n
layers foreachfeaturemapsize.Thenumbersof
Ô¨Åltersare
{
16
,
32
,
64
}
respectively.Thesubsamplingisper-
formedbyconvolutionswithastrideof2.Thenetwork ends

withaglobalaveragepooling,a10-wayfully-connected

layer,andsoftmax. Therearetotally6
n
+2stackedweighted
layers.Thefollowingtablesummarizesthearchitecture:
outputmapsize
32
√ó
32
16
√ó
16
8
√ó
8
#layers
1+2
n
2
n
2
n
#Ô¨Ålters
16
32
64
When shortcutconnectionsareused,theyareconnected

tothepairsof3
√ó
3layers(totally
3
n
shortcuts).Onthis
datasetweuseidentityshortcutsinallcases(
i.e
.,optionA),
method
error(%)
Maxout[9]
9.38
NIN[25]
8.81
DSN[24]
8.22
#layers
#params
FitNet[34]
19
2.5M
8.39
Highway[41,42]
19
2.3M
7.54
(7.72
¬±
0.16)
Highway[41,42]
32
1.25M
8.80
ResNet
20
0.27M
8.75
ResNet
32
0.46M
7.51
ResNet
44
0.66M
7.17
ResNet
56
0.85M
6.97
ResNet
110
1.7M
6.43
(6.61
¬±
0.16)
ResNet
1202
19.4M
7.93
Table6.ClassiÔ¨Åcationerroronthe
CIFAR-10
testset.Allmeth-
odsarewithdataaugmentation.ForResNet-110,werunit5 times

andshow‚Äúbest(mean
¬±
std)‚Äùasin[42].
soourresidualmodelshave exactlythesamedepth,width,

andnumberofparametersastheplaincounterparts.
Weuseaweightdecayof0.0001andmomentumof0.9,
andadopttheweightinitializationin[12]andBN[16]but

withnodropout.Thesemodelsaretrainedwithamini-

batchsizeof128ontwoGPUs.Westartwithalearning

rateof0.1,divideitby10at32kand48kiterations, and

terminatetrainingat64kiterations,whichisdeterminedon

a45k/5ktrain/valsplit.Wefollowthesimpledataaugmen-

tationin[24]fortraining:4pixelsarepaddedoneachside,

anda32
√ó
32cropisrandomlysampledfromthepadded
imageor itshorizontalÔ¨Çip.Fortesting,weonlyevaluate

thesingleviewof theoriginal32
√ó
32image.
Wecompare
n
=
{
3
,
5
,
7
,
9
}
,leadingto20,32,44,and
56-layernetworks.Fig. 6(left)showsthe behaviorsofthe

plainnets.Thedeepplainnetssufferfromincreaseddepth,

andexhibithighertrainingerrorwhengoingdeeper.This

phenomenonissimilartothatonImageNet(Fig. 4,left) and

onMNIST(see [41]),suggestingthat suchanoptimization

difÔ¨Åcultyisafundamentalproblem.
Fig.6(middle)showsthebehaviorsofResNets.Also
similartotheImageNetcases(Fig.4,right),ourResNets

managetoovercometheoptimizationdifÔ¨Åcultyanddemon-

strateaccuracygainswhenthedepthincreases.
Wefurtherexplore
n
=18
thatleadstoa110-layer
ResNet.Inthiscase,weÔ¨Åndthattheinitiallearningrate

of0.1isslightlytoolargetostartconverging
5
.Soweuse
0.01towarmupthetraininguntilthetrainingerrorisbelow

80%(about400iterations),and thengobackto0.1andcon-

tinuetraining.Therestofthe learningscheduleisasdone

previously.This110-layernetworkconvergeswell(Fig.6,

middle).Ithas
fewer
parametersthanotherdeepandthin
5
Withaninitiallearningrateof0.1,itstartsconverging(
<
90%error)
afterseveralepochs,butstillreachessimilaraccuracy.
7
776

0
1
2
3
4
5
6
0
5
10
20
iter. (1e4)
error (%)
plain-20
plain-32
plain-44
plain-56
0
1
2
3
4
5
6
0
5
10
20
iter. (1e4)
error (%)
ResNet-20
ResNet-32
ResNet-44
ResNet-56
ResNet-110
56-layer

20-layer
110-layer
20-layer
4
5
6
0
1
5
10
20
iter. (1e4)
error (%)
residual-110
residual-1202
Figure6.Trainingon
CIFAR-10
.Dashedlinesdenotetrainingerror,andboldlinesdenotetestingerror.
Left
:plainnetworks.Theerror
ofplain-110ishigherthan60%andnotdisplayed.
Middle
:ResNets.
Right
:ResNetswith110and1202layers.
0
20
40
60
80
100
1
2
3
layer index (sorted by magnitude)
std
plain-20
plain-56
ResNet-20
ResNet-56
ResNet-110
0
20
40
60
80
100
1
2
3
layer index (original)
std
plain-20
plain-56
ResNet-20
ResNet-56
ResNet-110
Figure7.Standarddeviations(std)oflayerresponsesonCIFAR-

10.Theresponsesaretheoutputsofeach3
√ó
3layer,afterBNand
beforenonlinearity.
Top
:thelayersareshownintheiroriginal
order.
Bottom
:theresponsesarerankedindescendingorder.
networkssuchasFitNet[34]andHighway[41](Table6),

yetisamongthestate-of-the-artresults(6.43%,Table6).

AnalysisofLayerResponses.
Fig.7showsthestandard
deviations(std)ofthelayerresponses.Theresponses are

theoutputsofeach3
√ó
3layer,afterBNandbeforeother
nonlinearity(ReLU/addition).ForResNets,thisanaly-

sisrevealstheresponsestrengthof theresidualfunctions.

Fig.7showsthatResNetshavegenerallysmallerresponses

thantheir plaincounterparts.Theseresultssupportourba-

sicmotivation(Sec.3.1)thattheresidualfunctions might

begenerally closertozerothan thenon-residualfunctions.

WealsonoticethatthedeeperResNethassmallermagni-

tudesof responses,asevidencedbythecomparisonsamong

ResNet-20,56,and110inFig.7.Whentherearemore

layers,anindividual layerofResNetstendstomodifythe

signalless.

ExploringOver1000layers.
Weexploreanaggressively
deepmodelofover1000layers.Weset
n
=200
that
leadstoa1202-layernetwork,whichistrainedas described

above.Ourmethodshows
nooptimizationdifÔ¨Åculty
,and
this
10
3
-layernetworkisabletoachieve
trainingerror
<
0.1%(Fig.6,right).Itstesterrorisstillfairlygood
(7.93%,Table6).
Buttherearestillopenproblemsonsuchaggressively
deepmodels.Thetestingresultofthis1202-layernetwork

isworsethanthatofour110-layernetwork,althoughboth
trainingdata
07+12
07++12
testdata
VOC07test
VOC12test
VGG-16
73.2
70.4
ResNet-101
76.4
73.8
Table7.ObjectdetectionmAP(%)onthePASCALVOC

2007/2012testsetsusing
baseline
FasterR-CNN.Seealsoap-
pendixforbetterresults.
metric
mAP@.5
mAP@[.5,.95]
VGG-16
41.5
21.2
ResNet-101
48.4
27.2
Table8.ObjectdetectionmAP(%)ontheCOCOvalidationset

usingbaseline
FasterR-CNN.Seealsoappendixforbetterresults.
havesimilartrainingerror.Wearguethat thisis becauseof

overÔ¨Åtting.The1202-layernetworkmaybeunnecessarily

large(19.4M)forthis smalldataset.Strongregularization

suchasmaxout[9]ordropout[13]isappliedtoobtainthe

bestresults([9,25,24,34])onthisdataset.Inthis paper,we

use nomaxout/dropoutandjustsimplyimposeregulariza-

tionviadeepandthinarchitecturesbydesign,withoutdis-

tractingfromthefocusonthedifÔ¨Åcultiesofoptimization.

Butcombiningwithstrongerregularizationmayimprove

results,whichwe willstudyinthefuture.

4.3.ObjectDetectiononPASCALandMSCOCO
Ourmethodhasgoodgeneralizationperformanceon
otherrecognitiontasks.Table7and8showtheobjectde-

tectionbaselineresultsonPASCALVOC2007 and2012

[5]and COCO[26].Weadopt
FasterR-CNN
[32]asthede-
tection method.Hereweareinterestedintheimprovements

ofreplacingVGG-16[40]withResNet-101.Thedetection

implementation(seeappendix)ofusingbothmodelsisthe

same,sothegainscanonlybeattributedtobetternetworks.

Mostremarkably,onthechallengingCOCOdatasetweob-

taina6.0%increaseinCOCO‚Äôs standardmetric(mAP@[.5,

.95]),whichisa28%relativeimprovement.Thisgainis

solelyduetothelearnedrepresentations.
Basedondeepresidual nets, wewonthe1st placesin
severaltracksinILSVRC&COCO2015competitions:Im-

ageNetdetection,ImageNetlocalization,COCOdetection,

andCOCOsegmentation.Thedetailsareintheappendix.
8
777

References
[1]Y. Bengio,P. Simard,andP.Frasconi.Learninglong-termdependen-
cieswithgradientdescentisdifÔ¨Åcult.
IEEETransactionsonNeural
Networks
,5(2):157‚Äì166,1994.
[2]C.M.Bishop.
Neuralnetworksforpatternrecognition
.Oxford
universitypress,1995.
[3]W.L.Briggs,S.F.McCormick,etal.
AMultigridTutorial
.Siam,
2000.
[4]K.ChatÔ¨Åeld,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil
isin thedetails:anevaluationofrecentfeatureencodingmethods.

InBMVC
,2011.
[5]M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zis-
serman.ThePascalVisualObjectClasses(VOC)Challenge.
IJCV
,
pages303‚Äì338,2010.
[6]R.Girshick.FastR-CNN.In
ICCV
,2015.
[7]R.Girshick,J.Donahue,T. Darrell,andJ.Malik.Richfeaturehier-
archiesforaccurateobjectdetectionandsemanticsegmentation.In

CVPR,2014.
[8]X.GlorotandY.Bengio.UnderstandingthedifÔ¨Åcultyoftraining
deepfeedforwardneuralnetworks.In
AISTATS
,2010.
[9]I.J.Goodfellow,D.Warde-Farley,M.Mirza,A.Courville,and
Y.Bengio.Maxout networks.
arXiv:1302.4389
,2013.
[10]K.HeandJ.Sun.Convolutionalneuralnetworksatconstrainedtime
cost.In
CVPR
,2015.
[11]K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep
convolutionalnetworksforvisualrecognition.In
ECCV
,2014.
[12]K.He,X.Zhang,S.Ren, andJ.Sun.DelvingdeepintorectiÔ¨Åers:
Surpassinghuman-levelperformanceonimagenetclassiÔ¨Åcation.In

ICCV,2015.
[13]G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,and
R.R.Salakhutdinov.Improvingneuralnetworksbypreventingco-

adaptationoffeaturedetectors.
arXiv:1207.0580
,2012.
[14]S. Hochreiter.Untersuchungenzudynamischenneuronalennetzen.
Diplomathesis,TUMunich
,1991.
[15]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Neural
computation
,9(8):1735‚Äì1780,1997.
[16]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeep
networktrainingbyreducinginternalcovariateshift. In
ICML
,2015.
[17]H.Jegou,M.Douze,andC.Schmid.Productquantizationfornearest
neighborsearch.
TPAMI
,33,2011.
[18]H.Jegou,F.Perronnin,M.Douze,J.Sanchez,P.Perez,and
C.Schmid.Aggregatinglocalimagedescriptorsintocompactcodes.

TPAMI
,2012.
[19]Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,
S.Guadarrama,andT.Darrell.Caffe:Convolutionalarchitecturefor

fastfeatureembedding.
arXiv:1408.5093
,2014.
[20]A.Krizhevsky.Learningmultiplelayersoffeaturesfromtinyim-
ages.
TechReport
,2009.
[21]A.Krizhevsky,I.Sutskever,and G.Hinton.ImagenetclassiÔ¨Åcation
withdeepconvolutionalneuralnetworks.In
NIPS
,2012.
[22]Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,
W.Hubbard,andL.D.Jackel.Backpropagationappliedtohand-

writtenzipcoderecognition.
Neuralcomputation
,1989.
[23]Y.LeCun,L.Bottou,G.B.Orr,andK.-R.M
¬®
uller.EfÔ¨Åcientbackprop.
In
NeuralNetworks: TricksoftheTrade
,pages9‚Äì50.Springer,1998.
[24]C.-Y.Lee,S.Xie,P.Gallagher, Z.Zhang,andZ.Tu.Deeply-
supervisednets.
arXiv:1409.5185
,2014.
[25]M.Lin,Q.Chen,andS.Yan.Networkinnetwork.
arXiv:1312.4400
,
2013.
[26]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
P.Doll
¬¥
ar,andC.L.Zitnick.MicrosoftCOCO:Commonobjectsin
context. In
ECCV
.2014.
[27]J.Long,E.Shelhamer, andT.Darrell. Fullyconvolutionalnetworks
forsemanticsegmentation.In
CVPR
,2015.
[28]G.Mont
¬¥
ufar, R.Pascanu,K.Cho,andY.Bengio.Onthenumberof
linearregionsofdeepneuralnetworks.In
NIPS
,2014.
[29]V.NairandG.E.Hinton.RectiÔ¨Åedlinearunitsimproverestricted
boltzmannmachines.In
ICML
,2010.
[30]F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor
imagecategorization.In
CVPR
,2007.
[31]T.Raiko,H.Valpola, andY.LeCun.Deeplearningmadeeasierby
lineartransformationsinperceptrons.In
AISTATS
,2012.
[32]S.Ren,K.He,R.Girshick,andJ.Sun.FasterR-CNN:Towards
real-timeobjectdetectionwithregionproposalnetworks. In
NIPS
,
2015.
[33]B.D. Ripley.
Patternrecognitionandneuralnetworks
.Cambridge
universitypress,1996.
[34]A.Romero,N.Ballas,S.E.Kahou,A.Chassang,C.Gatta,and
Y.Bengio.Fitnets:Hintsforthindeepnets.In
ICLR
,2015.
[35]O.Russakovsky,J.Deng,H.Su,J.Krause,S. Satheesh,S. Ma,
Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,etal.Imagenet

largescalevisualrecognitionchallenge.
arXiv:1409.0575
,2014.
[36]A.M.Saxe,J.L.McClelland,andS.Ganguli.Exactsolutionsto
thenonlineardynamicsoflearningindeeplinearneuralnetworks.

arXiv:1312.6120,2013.
[37]N. N. Schraudolph.Acceleratedgradientdescentbyfactor-centering
decomposition.Technicalreport,1998.
[38]N.N.Schraudolph.Centeringneuralnetworkgradientfactors.In
NeuralNetworks:Tricksof theTrade
,pages207‚Äì226.Springer,
1998.
[39]P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le-
Cun.Overfeat:Integratedrecognition,localizationanddetection

usingconvolutionalnetworks.In
ICLR
,2014.
[40]K.Simonyanand A.Zisserman.Verydeepconvolutionalnetworks
forlarge-scaleimagerecognition.In
ICLR
,2015.
[41]R.K.Srivastava,K.Greff, andJ.Schmidhuber.Highwaynetworks.
arXiv:1505.00387
,2015.
[42]R.K.Srivastava,K.Greff,andJ.Schmidhuber.Training verydeep
networks.
1507.06228
,2015.
[43]C.Szegedy,W.Liu,Y.Jia,P. Sermanet,S.Reed,D.Anguelov,D.Er-
han,V.Vanhoucke,andA.Rabinovich.Goingdeeperwithconvolu-

tions.In
CVPR
,2015.
[44]R.Szeliski.Fast surfaceinterpolationusinghierarchical basisfunc-
tions.
TPAMI
,1990.
[45]R.Szeliski.Locallyadaptedhierarchicalbasispreconditioning.In
SIGGRAPH
,2006.
[46]T. Vatanen,T. Raiko,H.Valpola,andY. LeCun.Pushingstochas-
ticgradienttowardssecond-ordermethods‚Äìbackpropagationlearn-

ingwithtransformationsinnonlinearities.In
NeuralInformation
Processing
,2013.
[47]A.VedaldiandB.Fulkerson.VLFeat:Anopenandportablelibrary
ofcomputervisionalgorithms,2008.
[48]W.VenablesandB.Ripley. Modernappliedstatisticswiths-plus.
1999.
[49]M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-
tionalneuralnetworks.In
ECCV
,2014.
9
778

