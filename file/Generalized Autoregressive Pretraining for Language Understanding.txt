XLNet: Generalized Autoregressive Pretraining
for Language Understanding
Zhilin Yang

1
, Zihang Dai

12
, Yiming Yang
1
, Jaime Carbonell
1
,
Ruslan Salakhutdinov
1
, Quoc V. Le
2
1
Carnegie Mellon University,
2
Google AI Brain Team
{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com
Abstract
With the capability of modeling bidirectional contexts, denoising autoencoding
based pretraining like BERT achieves better performance than pretraining ap-
proaches based on autoregressive language modeling. However, relying on corrupt-
ing the input with masks, BERT neglects dependency between the masked positions
and suff ers from a pretrain-netune discrepancy. In light of these pros and cons, we
propose XLNet, a generalized autoregressive pretraining method that (1) enables
learning bidirectional contexts by maximizing the expected likelihood over all
permutations of the factorization order and (2) overcomes the limitations of BERT
thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas
from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.
Empirically, under comparable experiment setting, XLNet outperforms BERT on
20 tasks, often by a large margin, including question answering, natural language
inference, sentiment analysis, and document ranking.
1
.
1 Introduction
Unsupervised representation learning has been highly successful in the domain of natural language
processing [
7
,
22
,
27
,
28
,
10
]. Typically, these methods rst pretrain neural networks on large-scale
unlabeled text corpora, and then netune the models or representations on downstream tasks. Under
this shared high-level idea, different unsupervised pretraining objectives have been explored in
literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been
the two most successful pretraining objectives.
AR language modeling seeks to estimate the probability distribution of a text corpus with an au-
toregressive model [
7
,
27
,
28
]. Specically, given a text sequence
x
= (
x
1
;
  
; x
T
)
, AR language
modeling factorizes the likelihood into a forward product
p
(
x
) =
Q
T
t
=1
p
(
x
t
j
x
<t
)
or a backward
one
p
(
x
) =
Q
1
t
=
T
p
(
x
t
j
x
>t
)
. A parametric model (e.g. a neural network) is trained to model each
conditional distribution. Since an AR language model is only trained to encode a uni-directional con-
text (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the
contrary, downstream language understanding tasks often require bidirectional context information.
This results in a gap between AR language modeling and effective pretraining.
In comparison, AE based pretraining does not perform explicit density estimation bu t instead aims to
reconstruct the original data from corrupted input. A notable example is BERT [
10
], which has been
the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens
are replaced by a special symbol
[MASK]
, and the model is trained to recover the original tokens from
the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize

Equal contribution. Order determined by swapping the one in [9].
1
Pretrained models and code are available at
https://github.com/zihangdai/xlnet
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
bidirectional contexts for reconstruction. As an immediate benet, this closes the aforementioned
bidirectional information gap in AR language modeling, leading to improved performance. However,
the articial symbols like
[MASK]
used by BERT during pretraining are absent from real data at
netuning time, resulting in a pretrain-netune discrepancy. Moreover, since the predicted tokens are
masked in the input, BERT is not able to model the joint probability using the product rule as in AR
language modeling. In other words, BERT assumes the predicted tokens are independent of each
other given the unmasked tokens, which is oversimplied as high-order, long-range dependency is
prevalent in natural language [9].
Faced with the pros and cons of existing language pretraining objectives, in this work, we propose
XLNet, a generalized autoregressive method that leverages the best of both AR language modeling
and AE while avoiding their limitations.

Firstly, instead of using a xed forward or backward factorization order as in conventional AR mod-
els, XLNet maximizes the expected log likelihood of a sequence w.r.t.
all possible permutations
of the factorization order
. Thanks to the permutation operation, the context for each position can
consist of tokens from both left and right. In expectation, each position learns to utilize contextual
information from all positions, i.e., capturing bidirectional context.

Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,
XLNet does not suff er from the pretrain-netune discrepancy that BERT is subject to. Meanwhile,
the autoregressive objective also provides a natural way to use the product rule for factorizing the
joint probability of the predicted tokens, eliminating the independence assumption made in BERT.
In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.

Inspired by the latest advancements in AR language modeling, XLNet integrates the segment
recurrence mechanism and relative encoding scheme of Transformer-XL [
9
] into pretraining, which
empirically improves the performance especially for tasks involving a longer text sequence.

Naively applying a Transformer(-XL) architecture to permutation-based language modeling does
not work because the factorization order is arbitrary and th e target is ambiguous. As a solution, we
propose to reparameterize the Transformer(-XL) network to remove the ambiguity.
Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [
10
] on a
wide spectrum of problems including GLUE language understanding tasks, reading comprehension
tasks like SQuAD and RACE, text classication tasks such as Yelp and IMDB, and the ClueWeb09-B
document ranking task.
Related Work
The idea of permutation-based AR modeling has been explored in [
32
,
12
], but there
are several key differences. Firstly, previous models aim to improve density estimation by baking
an Ô¨ÅorderlessÔ¨Ç inductive bias into the model while XLNet is motivated by enabling AR language
models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction
distribution, XLNet incorporates the target position into the hidden state via two-stream attention
while previous permutation-based AR models relied on implicit position awareness inherent to their
MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that
Ô¨ÅorderlessÔ¨Ç does not mean that the input sequence can be randomly permuted but that the model
allows for different factorization orders of the distribution.
Another related idea is to perform autoregressive denoising in the context of text generation [
11
],
which only considers a xed order though.
2 Proposed Method
2.1 Background
In this section, we rst review and compare the conventional AR language modeling and BERT for
language pretraining. Given a text sequence
x
= [
x
1
;
  
; x
T
]
, AR language modeling performs
pretraining by maximizing the likelihood under the forward autoregressive factorization:
max

log
p

(
x
) =
T
X
t
=1
log
p

(
x
t
j
x
<t
) =
T
X
t
=1
log
exp

h

(
x
1:
t

1
)
>
e
(
x
t
)

P
x
0
exp (
h

(
x
1:
t

1
)
>
e
(
x
0
))
;
(1)
2
where
h

(
x
1:
t

1
)
is a context representation produced by neural models, such as RNNs or Transform-
ers, and
e
(
x
)
denotes the embedding of
x
. In comparison, BERT is based on denoising auto-encoding.
Specically, for a text sequence
x
, BERT rst constructs a corrupted version
^
x
by randomly setting
a portion (e.g. 15%) of tokens in
x
to a special symbol
[MASK]
. Let the masked tokens be

x
. The
training objective is to reconstruct

x
from
^
x
:
max

log
p

(

x
j
^
x
)
Àá
T
X
t
=1
m
t
log
p

(
x
t
j
^
x
) =
T
X
t
=1
m
t
log
exp

H

(
^
x
)
>
t
e
(
x
t
)

P
x
0
exp

H

(
^
x
)
>
t
e
(
x
0
)

;
(2)
where
m
t
= 1
indicates
x
t
is masked, and
H

is a Transformer that maps a length-
T
text sequence
x
into a sequence of hidden vectors
H

(
x
) = [
H

(
x
)
1
; H

(
x
)
2
;
  
; H

(
x
)
T
]
. The pros and cons of
the two pretraining objectives are compared in the following aspects:

Independence Assumption
: As emphasized by the
Àá
sign in Eq. (2), BERT factorizes the joint
conditional probability
p
(

x
j
^
x
)
based on an independence assumption that all masked tokens

x
are separately reconstructed. In comparison, the AR language modeling objective
(1)
factorizes
p

(
x
)
using the product rule that holds universally without such an independence assumption.

Input noise
: The input to BERT contains articial symbols like
[MASK]
that never occur in
downstream tasks, which creates a pretrain-netune discrepancy. Replacing
[MASK]
with original
tokens as in [
10
] does not solve the problem because original tokens can be only used with a small
probability ≈† otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling
does not rely on any input corruption and does not suffer from this issue.

Context dependency
: The AR representation
h

(
x
1:
t

1
)
is only conditioned on the tokens up
to position
t
(i.e. tokens to the left), while the BERT representation
H

(
x
)
t
has access to the
contextual information on both sides. As a result, the BERT objective allows the model to be
pretrained to better capture bidirectional context.
2.2 Objective: Permutation Language Modeling
According to the compariso n above, AR language modeling and BERT possess their unique advan-
tages over the other. A natural question to ask is whether there exists a pretraining objective that
brings the advantages of both while avoiding their weaknesses.
Borrowing ideas from orderless NADE [
32
], we propose the permutation language modeling objective
that not only retains the benets of AR models but also allows models to capture bidirectional
contexts. Specically, for a sequence
x
of length
T
, there are
T
!
different orders to perform a valid
autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,
in expectation, the model will learn to gather information from all positions on both sides.
To formalize the idea, let
Z
T
be the set of all possible permutations of the length-
T
index sequence
[1
;
2
; : : : ; T
]
. We use
z
t
and
z
<t
to denote the
t
-th element and the rst
t

1
elements of a permutation
z
2 Z
T
. Then, our proposed permutation lan guag e modeling objective can be expressed as follows:
max

E
z
ÀòZ
T
"
T
X
t
=1
log
p

(
x
z
t
j
x
z
<t
)
#
:
(3)
Essentially, for a text sequence
x
, we sample a factorization order
z
at a time and decompose the
likelihood
p

(
x
)
according to factorization order. Since the same model parameter

is shared across
all factorization orders during training, in expectation,
x
t
has seen every possible element
x
i
6
=
x
t
in
the sequence, hence being able to capture the bidirectional context. Moreover, as this objective ts
into the AR framework, it naturally avoids the independence assumption and the pretrain-netune
discrepancy discussed in Section 2.1.
Remark on Permutation
The proposed objective only permutes the factorization order, not the
sequence order. In other words, we keep the original sequence order, use the positional encodings
corresponding to the original sequence, and rely on a proper attention mask in Transformers to
achieve permutation of the factorization order. Note that this choice is necessary, since the model
will only encounter text sequences with the natural order during netuning.
To provide an overall picture, we show an example of predicting the token
x
3
given the same input
sequence
x
but under different factorization orders in the Appendix A.7 with Figure 4.
3
2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations
Figure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query
stream attention, which does not have access information about the content
x
z
t
. (c): Overview of the
permutation language modeling training with two-stream attention.
While the permutation language modeling objective has desired properties, naive implementation with
standard Transformer parameterization may not work. To see the problem, assume we parameterize
the next-token distribution
p

(
X
z
t
j
x
z
<t
)
using the standard Softmax formulation, i.e.,
p

(
X
z
t
=
x
j
x
z
<t
) =
exp
(
e
(
x
)
>
h

(
x
z
<t
)
)
P
x
0
exp
(
e
(
x
0
)
>
h

(
x
z
<t
)
)
;
where
h

(
x
z
<t
)
denotes the hidden representation of
x
z
<t
produced by the shared Transformer network after proper masking. Now notice that the representation
h

(
x
z
<t
)
does not depend on which position it will predict, i.e., the value of
z
t
. Consequently, the
same distribution is predicted regardless of the target position, which is not able to learn useful
representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to
re-parameterize the next-token distribution to be target position aware:
p

(
X
z
t
=
x
j
x
z
<t
) =
exp

e
(
x
)
>
g

(
x
z
<t
; z
t
)

P
x
0
exp (
e
(
x
0
)
>
g

(
x
z
<t
; z
t
))
;
(4)
where
g

(
x
z
<t
; z
t
)
denotes a new type of representations which additionally take the target position
z
t
as input.
Two-Stream Self-Attention
While the idea of target-aware representations removes the ambiguity
in target prediction, how to formulate
g

(
x
z
<t
; z
t
)
remains a non-trivial problem. Among other
possibilities, we propose to Ô¨ÅstandÔ¨Ç at the target position
z
t
and rely on the position
z
t
to gather
information from the context
x
z
<t
through attention. For this parameterization to work, there are two
requirements that are contradictory in a standard Transformer architecture: (1) to predict the token
x
z
t
,
g

(
x
z
<t
; z
t
)
should only use the
position
z
t
and not the
content
x
z
t
, otherwise the objective
becomes trivial; (2) to predict the other tokens
x
z
j
with
j > t
,
g

(
x
z
<t
; z
t
)
should also encode the
content
x
z
t
to provide full contextual information. To resolve such a contradiction, we propose to use
two sets of hidden representations instead of one:

The content representation
h

(
x
z

t
)
, or abbreviated as
h
z
t
, which serves a similar role to the
standard hidden states in Transformer. This representation encodes
both
the context and
x
z
t
itself.

The query representation
g

(
x
z
<t
; z
t
)
, or abbreviated as
g
z
t
, which only has access to the contex-
tual information
x
z
<t
and the position
z
t
, but not the content
x
z
t
, as discussed above.
Computationally, the rst layer query stream is initialized with a trainable vector, i.e.
g
(0)
i
=
w
,
while the content stream is set to the corresponding word embedding, i.e.
h
(0)
i
=
e
(
x
i
)
. For each
self-attention layer
m
= 1
; : : : ; M
, the two streams of representations are
schematically
2
updated
2
To avoid clutter, we omit the implementation details including multi-head attention, residual connection,
layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in
Appendix A.2 for reference.
4
with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):
g
(
m
)
z
t
 
Attention
(
Q
=
g
(
m

1)
z
t
;
KV
=
h
(
m

1)
z
<t
;

)
;
(
query stream: use
z
t
but cannot see
x
z
t
)
h
(
m
)
z
t
 
Attention
(
Q
=
h
(
m

1)
z
t
;
KV
=
h
(
m

1)
z

t
;

)
;
(
content stream: use both
z
t
and
x
z
t
)
:
where Q, K, V denote the query, key, and value in an attention operation [
33
]. The update rule of the
content representations is exactly the same as the standard self-attention, so during netuning, we
can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,
we can use the last-layer query representation
g
(
M
)
z
t
to compute Eq. (4).
Partial Prediction
While the permutation language modeling objective
(3)
has several benets, it is
a much more challenging optimization problem due to the permutation and causes slow convergence
in preliminary experiments. To reduce the optimization difculty, we choose to only predict the last
tokens in a factorization order. Formally, we split
z
into a non-target subsequence
z

c
and a target
subsequence
z
>c
, where
c
is the cutting point. The objective is to maximize the log-likelihood of the
target subsequence conditioned on the non-target subsequence, i.e.,
max

E
z
ÀòZ
T
h
log
p

(
x
z
>c
j
x
z

c
)
i
=
E
z
ÀòZ
T
2
4
j
z
j
X
t
=
c
+1
log
p

(
x
z
t
j
x
z
<t
)
3
5
:
(5)
Note that
z
>c
is chosen as the target because it possesses the longest context in the sequence given the
current factorization order
z
. A hyperparameter
K
is used such that about
1
=K
tokens are selected
for predictions; i.e.,
j
z
j
=
(
j
z
j 
c
)
Àá
K
. For unselected tokens, their query representations need not
be computed, which saves speed and memory.
2.4 Incorporating Ideas from Transformer-XL
Since our objective function ts in the AR framework, we incorporate the state-of-the-art AR
language model, Transformer-XL [
9
], into our pretraining framework, and name our method after it.
We integrate two important techniques in Transformer-XL, namely the relative positional encoding
scheme and the segment recurrence mechanism. We apply relative positional encodings based on the
original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the
recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden
states from prev ious segments. Without loss of generality, suppose we have two segments taken from
a long sequence
s
; i.e.,
~
x
=
s
1:
T
and
x
=
s
T
+1:2
T
. Let
~
z
and
z
be permutations of
[1
  
T
]
and
[
T
+ 1
  
2
T
]
respectively. Then, based on the permutation
~
z
, we process the rst segment, and then
cache the obtained content representations
~
h
(
m
)
for each layer
m
. Then, for the next segment
x
, the
attention update with memory can be written as
h
(
m
)
z
t
 
Attention
(
Q
=
h
(
m

1)
z
t
;
KV
=
h
~
h
(
m

1)
;
h
(
m

1)
z

t
i
;

)
where
[
:; :
]
denotes concatenation along the sequence dimension. Notice that positional encodings
only depend on the actual positions in the original sequence. Thus, the above attention update is
independent of
~
z
once the representations
~
h
(
m
)
are obtained. This allows caching and reusing the
memory without knowing the factorization order of the previous segment. In expectation, the model
learns to utilize the memory over all factorization orders of the last segment. The query stream can
be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation
language modeling with two-stream attention (see Appendix A.7 for more detailed illustration).
2.5 Modeling Multiple Segments
Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in
question answering. We now discuss how we pretrain XLNet to model multiple segments in the
autoregressive framework. During the pretraining phase, following BERT, we randomly sample two
segments (either from the same context or not) and treat the concatenation of two segments as one
sequence to perform permutation language modeling. We only reuse the memory that belongs to
the same context. Specically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP],
where Ô¨ÅSEPÔ¨Ç and Ô¨ÅCLSÔ¨Ç are two special symbols and Ô¨ÅAÔ¨Ç and Ô¨ÅBÔ¨Ç are the two segments. Although
5
we follow the two-segment data format, XLNet-Large does not use the objective of next sentence
prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).
Relative Segment Encodings
Architecturally, different from BERT that adds an absolute segment
embedding to the word embedding at each position, we extend the idea of relative encodings from
Transformer-XL to also encode the segments. Given a pair of positions
i
and
j
in the sequence, if
i
and
j
are from the same segment, we use a segment encoding
s
ij
=
s
+
or otherwise
s
ij
=
s

,
where
s
+
and
s

are learnable model parameters for each attention head. In other words, we only
consider whether the two positions are
within the same segment
, as opposed to considering
which
specic segments they are from
. This is consistent with the core idea of relative encodings; i.e., only
modeling the relationships between positions. When
i
attends to
j
, the segment encoding
s
ij
is used
to compute an attention weight
a
ij
= (
q
i
+
b
)
>
s
ij
, where
q
i
is the query vector as in a standard
attention operation and
b
is a learnable head-specic bias vector. Finally, the value
a
ij
is added to
the normal attention weight. There are two benets of using relative segment encodings. First, the
inductive bias of relative encodings improves generalization [
9
]. Second, it opens the possibility of
netuning on tasks that have more than two input segments, which is not possible using absolute
segment encodings.
2.6 Discussion
Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,
only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all
tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT
and XLNet, partial prediction plays a role of reducing optimization difculty by only predicting
tokens with sufcient context. However, the independence assumption discussed in Section 2.1
disables BERT to model dependency between targets.
To better understand the diff erence, let's consider a concrete example [New, York, is, a, city]. Sup pose
both BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize
log
p
(
New York
j
is a city
)
. Also suppose that XLNet samples the factorization order [is, a, city,
New, York]. In this case, BERT and XLNet respectively reduce to the following objectives:
J
BERT
= log
p
(
New
j
is a city
) + log
p
(
York
j
is a city
)
;
J
XLNet
= log
p
(
New
j
is a city
) + log
p
(
York
j
New
;
is a city
)
:
Notice that XLNet is able to capture the dependency between the pair (New, York), which is omitted
by BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and
(York, city), it is obvious that XLNet always learns
more
dependency pairs given the same target and
contains Ô¨ÅdenserÔ¨Ç effective training signals.
For more formal analysis and further discussion, please refer to Appendix A.5.
3 Experiments
3.1 Pretraining and Implementation
Following BERT [
10
], we use the BooksCorpus [
40
] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [
26
],
ClueWeb 2012-B (extended from [
5
]), and Common Crawl [
6
] for pretraining. We use heuristics
to aggressively lter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [
17
], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.
Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which
results in a similar model size. During pretraining, we always use a full sequence length of 512.
Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks
on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the
original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described
above. Specically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay
optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was
6
observed that the model still underts the data at the end of training. Finally, we perform ablation
study (section 3.4) based on the XLNet-Base-wikibooks.
Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each
of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set
the partial prediction constant
K
as 6 (see Section 2.3). Our netuning procedure follows BERT [
10
]
except otherwise specied
3
. We employ an idea of
span-based prediction
, where we rst sample a
length
L
2
[1
;
  
;
5]
, and then randomly select a consecutive span of
L
tokens as prediction targets
within a context of
(
K L
)
tokens.
We use a variety of natural language understanding datasets to evaluate the performance of our
method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.
3.2 Fair Comparison with BERT
Model
SQuAD1.1 SQuAD2.0 RACE MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B
BERT-Large
(Best of 3)
86.7/92.8 82.8/85.5 75.1 87.3 93.0 91.4 74.0 94.0 88.7 63.7 90.2
XLNet-Large-
wikibooks
88.2/94.0 85.1/87.8 77.4 88.4 93.9 91.8 81.2 94.4 90.0 65.2 91.1
Table 1:
Fair comparison with BERT. All models are trained using the same data and hyperparameters as in
BERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word
masking, and BERT without next sentence prediction.
Here, we rst compare the performance of BERT and XLNet in a fair setting to decouple the eff ects
of using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best
performance of three different variants of BERT and (2) XLNet trained with the same data and
hyperparameters. As we can see, trained on the same data with an almost identical training recipe,
XLNet outperforms BERT by a sizable margin on all the considered datasets.
3.3 Results After Scaling Up
RACE Accuracy Middle High
Model NDCG@20 ERR@20
GPT [28] 59.0 62.9 57.4
DRMM [13] 24.3 13.8
BERT [25] 72.0 76.6 70.1
KNRM [8] 26.9 14.9
BERT+DCMN

[38] 74.1 79.5 71.8
Conv [8] 28.7 18.1
RoBERTa [21] 83.2 86.5 81.8
BERT
y
30.53 18.67
XLNet
85.4 88.6 84.0
XLNet
31.10 20.28
Table 2:
Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on
ClueWeb09-B, a document ranking task.

indicates using ensembles.
y
indicates our implementations. Ô¨ÅMiddleÔ¨Ç
and Ô¨ÅHighÔ¨Ç in RACE are two subsets representing middle and high school dif culty levels. All BERT, RoBERTa,
and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).
After the initial publication of our manuscript, a few other pretrained models were released such as
RoBERTa [
21
] and ALBERT [
19
]. Since ALBERT involves increasing the model hidden size from
1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we
exclude ALBERT from the following results as it is hard to lead to scientic conclusions. To obtain
relatively fair comparison with RoBERTa, the experiment in this section is based on full data and
reuses the hyper-parameters of RoBERTa, as described in section 3.1.
The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question
answering), 4 (text classication) and 5 (natural language understanding), where XLNet generally
outperforms BERT and RoBERTa. In addition, we make two more interesting observations:
3
Hyperparameters for pretraining and netuning are in Appendix A.4.
7
SQuAD2.0 EM F1
SQuAD1.1 EM F1
Dev set results (single model)
BERT [10] 78.98 81.77
BERT
y
[10] 84.1 90.9
RoBERTa [21] 86.5 89.4
RoBERTa [21] 88.9 94.6
XLNet
87.9 90.6
XLNet
89.7 95.1
Test set results on leaderboard (single model, as of Dec 14, 2019)
BERT

[10] 80.005 83.061
RoBERTa [21] 86.820 89.795
XLNet
87.926 90.689
Table 3:
Results on SQuAD, a reading comprehension dataset.
y
marks our runs with the ofcial code. We are
not able to obtain the test results on SQuAD1.1 from the organizers after submitting our result for more than one
month.
Model IMDB Yelp-2 Yelp-5 DBpedia AG Amazon-2 Amazon-5
CNN [15] - 2.90 32.39 0.84 6.57 3.79 36.24
DPCNN [15] - 2.64 30.58 0.88 6.87 3.32 34.81
Mixed VAT [31, 23] 4.32 - - 0.70 4.95 - -
ULMFiT [14] 4.6 2.16 29.98 0.80 5.01 - -
BERT [35] 4.51 1.89 29.32 0.64 - 2.63 34.17
XLNet
3.20 1.37 27.05 0.60 4.45 2.11 31.67
Table 4:
Comparison with state-of-the-art error rates on the test sets of several text classication datasets. All
BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).
Model MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI
Single-task single models on dev
BERT [2] 86.6/- 92.3 91.3 70.4 93.2 88.0 60.6 90.0 -
RoBERTa [21] 90.2/90.2 94.7 92.2
86.6
96.4
90.9
68.0 92.4 -
XLNet
90.8/90.8 94.9 92.3
85.9
97.0
90.8
69.0 92.5
-
Multi-task ensembles on test (from leaderboard as of Oct 28, 2019)
MT-DNN

[20] 87.9/87.4 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0
RoBERTa

[21] 90.8/90.2 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0
XLNet

90.9/90.9
y
99.0
y
90.4
y
88.5 97.1
y
92.9 70.2 93.0 92.5
Table 5:
Results on GLUE.

indicates using ensembles, and
y
denotes single-task results in a multi-task row.
All dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower
section shows comparison with state-of-the-art results on the public leaderboard.

For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance
gain of XLNet is usually larger. This superiority at dealing with longer context could come from
the Transformer-XL backbone in XLNet.

For classication tasks that already have abundant supervised examples such as MNLI (>390K),
Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains.
3.4 Ablation Study
We perform an ablation study to un derstand the importance of each design choice based on four
datasets with diverse characteristics. Specically, there are three main aspects we hope to study:

The effectiveness of the permutation language modeling objective alone, especially compared to
the denoising auto-encoding objective used by BERT.

The importance of using Transformer-XL as the backbone neural architecture.

The necessity of some implementation details including span-based prediction, the bidirectional
input pipeline, and next-sentence prediction.
With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with diff erent implemen-
tation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL
8
baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-
rectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture
with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the
BooksCorpus. All results reported are the median of 5 runs.
# Model
RACE SQuAD2.0 MNLI SST-2
F1 EM m/mm
1 BERT-Base
64.3 76.30 73.66 84.34/84.65 92.78
2 DAE + Transformer-XL
65.03 79.56 76.80 84.88/84.45 92.60
3 XLNet-Base (
K
= 7
)
66.05
81.33 78.46 85.84/85.43
92.66
4 XLNet-Base (
K
= 6
)
66.66 80.98 78.18 85.63/85.12
93.35
5 - memory
65.55 80.15 77.27 85.32/85.05 92.78
6 - span-based pred
65.95 80.61 77.91 85.49/85.02 93.12
7 - bidirectional data
66.34 80.65 77.87 85.31/84.99 92.66
8 + next-sent pred
66.76
79.83 76.94 85.32/85.09 92.89
Table 6:
The results of BERT on RACE are taken from [
38
]. We run BERT on the other datasets using the
ofcial implementation and the same hyperparameter search space as XLNet.
K
is a hyperparameter to control
the optimization difculty (see Section 2.3).
Examining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly
contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory
caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the
longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and
the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly nd the the
next-sentence prediction objective proposed in the original BERT does not necessarily lead to an
improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.
Finally, we also perform a qualitative study of the attention patterns, which is included in Appendix
A.6 due to page limit.
4 Conclusions
XLNet is a generalized AR pretraining method that uses a permutation language modeling objective
to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to
work seamlessly with the AR objective, including integrating Transformer-XL and the careful design
of the two-stream attention mechanism. XLNet achieves substantial improvement over previous
pretraining objectives on various tasks.
Acknowledgments
The authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the
project, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo
Wang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai
for clarifying the setting of the document ranking task. ZY and RS were supported by the Ofce of
Naval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562,
the Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under
the grant IIS-1546329 and by the DOE-Ofce of Science under the grant ASCR #KJ040201.
References
[1]
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level
language modeling with deeper self-attention.
arXiv preprint arXiv:1808.04444
, 2018.
[2]
Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-
mous preprint under review, 2018.
[3]
Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.
arXiv preprint arXiv:1809.10853
, 2018.
[4]
Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer
neural networks. In
Advances in Neural Information Processing Systems
, pages 400≈í406, 2000.
9
[5]
 Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009.
[6]
 Common Crawl. Common crawl.
URl: http://http://commoncrawl. org
, 2019.
[7]
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In
Advances in neural
information processing systems
, pages 3079≈í3087, 2015.
[8]
Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks
for soft-matching n-grams in ad-hoc search. In
Proceedings of the eleventh ACM international
conference on web search and data mining
, pages 126≈í134. ACM, 2018.
[9]
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a xed-length
context.
arXiv preprint arXiv:1901.02860
, 2019.
[10]
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805
,
2018.
[11]
William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via lling
in the_.
arXiv preprint arXiv:1801.07736
, 2018.
[12]
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In
International Conference on Machine Learning
, pages 881≈í889,
2015.
[13]
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for
ad-hoc retrieval. In
Proceedings of the 25th ACM International on Conference on Information
and Knowledge Management
, pages 55≈í64. ACM, 2016.
[14]
Jeremy Howard and Sebastian Ruder. Universal language model ne-tuning for text classica-
tion.
arXiv preprint arXiv:1801.06146
, 2018.
[15]
Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego-
rization. In
Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)
, pages 562≈í570, 2017.
[16]
Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
Lukasiewicz. A surprisingly robust trick for winograd schema challenge.
arXiv preprint
arXiv:1905.06290
, 2019.
[17]
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.
arXiv preprint arXiv:1808.06226
, 2018.
[18]
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale
reading comprehension dataset from examinations.
arXiv preprint arXiv:1704.04683
, 2017.
[19]
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations.
arXiv
preprint arXiv:1909.11942
, 2019.
[20]
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks
for natural language understanding.
arXiv preprint arXiv:1901.11504
, 2019.
[21]
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach.
arXiv preprint arXiv:1907.11692
, 2019.
[22]
 Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:
Contextualized word vectors. In
Advances in Neural Information Processing Systems
, pages
6294≈í6305, 2017.
[23]
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-
supervised text classication.
arXiv preprint arXiv:1605.07725
, 2016.
[24]
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural
networks.
arXiv preprint arXiv:1601.06759
, 2016.
[25]
Xiaoman Pan, Kai Sun, Dian Yu, Heng Ji, and Dong Yu. Improving question answering with
external knowledge.
arXiv preprint arXiv:1902.00993
, 2019.
10
[26]
Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword
fth edition, linguistic data consortium.
Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia, Tech. Rep.
, 2011.
[27]
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-
ton Lee, and Luke Zettlemoyer. Deep contextualized word representations.
arXiv preprint
arXiv:1802.05365
, 2018.
[28]
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training.
URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf
, 2018.
[29]
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable
questions for squad.
arXiv preprint arXiv:1806.03822
, 2018.
[30]
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
for machine comprehension of text.
arXiv preprint arXiv:1606.05250
, 2016.
[31]
Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks
for semi-supervised text classication via mixed objective function. 2018.
[32]
Benigno Uria, Marc-Alexandre C√¥t√©, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation.
The Journal of Machine Learning Research
, 17(1):7184≈í
7220, 2016.
[33]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In
Advances in neural information
processing systems
, pages 5998≈í6008, 2017.
[34]
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR.
[35]
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data
augmentation.
arXiv preprint arXiv:1904.12848
, 2019.
[36]
Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural
ad-hoc ranking with kernel pooling. In
Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval
, pages 55≈í64. ACM, 2017.
[37]
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax
bottleneck: A high-rank rnn language model.
arXiv preprint arXiv:1711.03953
, 2017.
[38]
Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. Dual co-
matching network for multi-choice reading comprehension.
arXiv preprint arXiv:1901.09381
,
2019.
[39]
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text
classication. In
Advances in neural information processing systems
, pages 649≈í657, 2015.
[40]
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In
Proceedings of the IEEE international conference on
computer vision
, pages 19≈í27, 2015.
11
