arXiv:1907.11692v1  [cs.CL]  26 Jul 2019
RoBERTa:ARobustlyOptimizedBERTPretrainingApproach
YinhanLiu

x
MyleOtt
x
Naman Goyal
x
JingfeiDu
x
MandarJoshi
y
DanqiChen
x
OmerLevy
x
MikeLewis
x
LukeZettlemoyer
yx
Veselin Stoyanov
x
y
PaulG.AllenSchoolofComputerScience&Engineering,
UniversityofWashington,Seattle,WA
f
mandar90,lsz
g
@cs.washington.edu
x
FacebookAI
f
yinhanliu,myleott,naman,jingfeidu,

danqi,omerlevy,mikelewis,lsz,ves
g
@fb.com
Abstract
Languagemodelpretraininghasledtosig-

nicantperformancegainsbutcarefulcom-

parisonbetweendifferentapproachesischal-

lenging.Trainingiscomputationallyexpen-

sive,oftendoneonprivatedatasetsofdifferent

sizes,and,aswewillshow,hyperparameter

choiceshavesignicantimpactonthenalre-

sults.WepresentareplicationstudyofBERT

pretraining(
Devlinetal.
,
2019
)thatcarefully
measurestheimpactofmanykeyhyperparam-

etersandtrainingdatasize.WendthatBERT

wassignicantlyundertrained,andcanmatch

orexceedtheperformanceofeverymodel

publishedafterit.Ourbestmodelachieves

state-of-the-artresultsonGLUE,RACEand

SQuAD.Theseresultshighlighttheimpor-

tanceofpreviouslyoverlookeddesignchoices,

andraisequestionsaboutthesourceofre-

centlyreportedimprovements.Wereleaseour

modelsandcode.
1
1Introduction

S elf-trainingm ethodssuchasE L M o(
P eters etal.
,
2018
),G P T(
R adford etal.
,
2018
),B E RT 
(
D evlinetal. 
, 
2019
),X L M(
L ampleandC onneau
,
2019
),and X L N et (
Yangetal.
,
2019
)have
brought signi cant perform ancegains,butitcan

bechallengingtodeterm inew hichaspects of

the m ethodscontributethe most.Trainingis

com putationallyexpensive, limitingtheamount

of tuningthatcanbe done, and isoftendonew ith

privatetrainingdata ofvaryingsizes, limiting

ourabilitytom easuretheeffectsofthemodeling

advances. 

Equalcontribution.
1
Ourmodelsandcodeareavailableat:
https://github.com/pytorch/fairseq
WepresentareplicationstudyofBERTpre-
training(
D evlinetal.
,
2019
),whichincludesa
carefulevaluation oftheeffectsofhyperparmeter

tuningandtraining setsize.We ndthatBERT 

wassigni cantlyundertrainedand proposeanim-

provedrecipefortrainingBERTmodels,which

w ecallR oB E RTa,thatcanmatchorexceed the

performanceofallofthepost-BERTmethods.

O urmodi cationsaresimple,theyinclude:(1)

trainingthemodellonger,withbiggerbatches,

overmoredata;(2)removingthenextsentence

predictionobjective;(3)trainingon longerse-

quences;and(4)dynamicallychangingthemask-

ingpatternappliedtothetraining data.Wealso

collectalargenewdataset(C C -N
EWS
)ofcompa-
rablesizetootherprivatelyuseddatasets,tobetter

controlfortraining setsizeeffects.
When controllingfortrainingdata,ourim-
provedtrainingprocedureimprovesuponthepub-

lished B E RTresults onbothGLUEandS QuAD.

Whentrained forlongeroveradditionaldata,our

model achievesascoreof88.5 onthepublic

G L UEleaderboard,matchingthe88.4reported

by
Yangetal.
(
2019
). Ourmodelestablishesa
newstate-of-the-arton4/9oftheGLUEtasks:

MN LI,Q N LI,RT Eand S TS -B.Wealsomatch

state-of-the-artresults onS QuADandRACE.

O verall,w ere-establish thatBERT 'smaskedlan-

guagemodeltrainingobjectiveiscompetitive

w ithotherrecentlyproposedtrainingobjectives

suchas perturbedautoregressivelanguagemodel-

ing(
Yangetal.
,
2019
).
2
Insummary,the contributionsofthispaper
are:(1)We presentasetofimportantBERTde-

signchoicesandtraining strategiesandintroduce
2
Itispossiblethattheseothermethodscouldalsoimprove
withmoretuning.Weleavethisexplorationtofuturework.
alternatives thatleadtobetterdow nstream task

perform ance;(2)Weuseanoveldataset,C C -
N 
EWS
,andcon rmthatusing moredataforpre-
trainingfurtherim provesperformanceondow n-

streamtasks; (3)O urtraining improvements show 

that m asked languagem odelpretraining, under

therightdesign choices,iscompetitive w ithall

otherrecentlypublishedm ethods.We releaseour

m odel,pretrainingand ne-tuningcodeimple-

m ented inP yTorch(
Paszke etal.
,
2017
).
2Background

Inthissection,w egiveabriefoverview ofthe

B E RT(
D evlin etal. 
, 
2019
)pretrainingapproach
and som e ofthetrainingchoicesthatw ew ill ex-

am ine experim entallyinthefollow ing section.

2. 1 S etu p 

B E RTtakesasinputa concatenation oftwo

segm ents (sequencesoftokens),
x
1
; : : :; x
N
and
y 
1
; : : :; y 
M
.S egm entsusuallyconsistof
m orethanonenaturalsentence.T hetwoseg-

m entsarepresentedasasingleinputsequence

toB E RTw ith specialtokensdelimitingthem:

[
CLS
]
; x 
1
; : : :; x 
N
;
[
SE P 
]
; y 
1
; : : :; y 
M
;
[
E O S
]
.
M
and
N
areconstrainedsuchthat
M
+ 
N<T
,
w here
T
isa param eterthatcontrolsthemaximum
sequencelength during training.
T hem odelis rstpretrained onalargeunla-
beledtextcorpusandsubsequently netuned us-

ingend-tasklabeled data. 

2. 2 A rch itectu re

B E RTusesthenow ubiquitoustransformerarchi-

tecture(
Vaswanietal. 
, 
2017
),w hichw ew illnot
reviewin detail.We useatransformerarchitecture

w ith
L 
layers.E achblockuses
A
self-attention
heads andhidden dim ension
H 
.
2. 3 Train in g O b jectives

D uringpretraining,B E RTusestwoobjectives:

m askedlanguagem odelingandnextsentencepre-

diction. 

M asked L an gu ageM od el(ML M)
Arandom
sam pleofthetokens intheinputsequence is

selectedandreplacedw iththespecialtoken

[
M A SK 
]
. T heM L Mobjective isacross-entropy
lossonpredicting them askedtokens.B E RT uni-

form lyselects15% of theinputtokensforpossi-

blereplacem ent.O ftheselected tokens,80%are

replacedw ith
[
M A SK 
]
,10%are leftunchanged,
and10%arereplaced byarandomlyselectedvo-

cabularytoken.
Intheoriginalimplementation,randommask-
ingandreplacementisperformedonceinthebe-

ginningandsavedforthedurationoftraining,al-

thoughinpractice,dataisduplicated sothemask

isnotalwaysthe same foreverytrainingsentence

(seeS ection
4.1
).
N ext S en ten ceP red iction(NSP )
NSP isabi-
naryclassi cationlossforpredictingwhethertwo

segmentsfolloweach otherintheoriginaltext.

P ositive examplesarecreatedbytakingconsecu-

tivesentencesfrom thetextcorpus.Negativeex-

amples arecreated bypairing segmentsfromdif-

ferentdocuments.P ositiveandnegativeexamples

aresampled w ithequalprobability.
T heN S Pobjective wasdesignedtoimprove
performanceondow nstreamtasks,such asNatural

L anguageInference(
B owman etal.
,
2015
),which
requirereasoningabouttherelationshipsbetween

pairsofsentences.

2.4Op timization

B E RTisoptimizedw ithAdam(
KingmaandBa
,
2015
)using thefollow ingparameters:

1
=0
:
9
,

2
= 0
:
999
,

= 
1e-6and
L 
2
weightde-
cayof
0
:
01
.T helearningrateiswarmedup
overthe rst10,000stepstoapeakvalueof

1e-4,and thenlinearlydecayed.BERTtrains

w ithadropoutof0.1onalllayersandat-

tentionw eights,andaGEL Uactivationfunc-

tion (
H endrycksandG impel
,
2016
).M odelsare
pretrainedfor
S
= 
1,000,000updates,withmini-
batchescontaining
B 
=
256sequencesofmaxi-
mumlength
T
= 
512tokens.
2.5D ata

B E RTis trainedon a combinationofB
OOK
C
OR
-
PUS
(
Z huetal.
,
2015
)plusE nglishW 
IKIPEDIA
,
w hich totals16G B ofuncompressedtext.
3
3ExperimentalSetup

Inthissection,w edescribetheexperimentalsetup

forourreplicationstudy ofBERT.

3.1Imp lementation 

WereimplementB E RTin
FAIRSEQ
(
Ottetal.
,
2019
). WeprimarilyfollowtheoriginalBERT 
3
Yangetal.
(
2019
)usethesamedatasetbutreporthaving
only13GBoftextafterdatacleaning.Thisismostlikelydue

tosubtledifferencesincleaningoftheWikipediadata.
optim izationhyperparam eters,givenin S ection
2
,
exceptforthe peaklearningrate andnumberof

warm upsteps, w hicharetunedseparatelyforeach

setting.Weadditionallyfoundtrainingtobevery

sensitivetotheA damepsilonterm,andinsome

casesw eobtainedbetterperformanceorimproved

stabilityafter tuningit. S imilarly,w efound setting


2
= 0
:
98
toim provestabilityw hen trainingw ith
large batchsizes. 
Wepretrain w ithsequencesofatmost
T
= 512
tokens.U nlike
D evlinetal. 
(
2019
),w edo notran-
dom lyinjectshortsequences,andw e donottrain

w ith areducedsequencelengthforthe  rst 90% of

updates.Wetrainonlyw ithfull-lengthsequences.
Wetrainw ith m ixedprecision oatingpoint
arithm eticonD G X -1m achines,each w ith8
 
32G BN vidiaV 100G P U sinterconnectedby In-

 niband (
M icikeviciusetal. 
, 
2018
).
3. 2 D ata

B E RT-stylepretrainingcruciallyreliesonlarge

quantitiesoftext. 
B aevskietal.
(
2019
)demon-
stratethatincreasing datasizecanresultinim-

provedend-taskperform ance.S everalefforts

have trained ondatasetslargerandmore diverse

thantheoriginal B E RT(
R adford etal.
,
2019
;
Yangetal. 
, 
2019
;
Z ellers etal.
,
2019
). U nfortu-
nately,notallof theadditionaldatasetscanbe

publiclyreleased.Forour study, w e focuson gath-

eringasm uchdataaspossibleforexperimenta-

tion,allow ing ustom atch theoverallquality and

quantityof data as appropriateforeachcompari-

son. 
WeconsiderveE nglish-languagecorporaof
varyingsizesand dom ains,totalingover160G B 

of uncom pressed text. We usethefollow ingtext

corpora:


B 
OOK
C 
ORPUS
(
Z hu etal. 
, 
2015
)plus E nglish
W 
IKIPEDIA
.T his is theoriginaldata used to
trainB E RT.(16G B ). 

C C -N 
EWS
,w hich w ecollectedfrom theE n-
glish portionofthe C ommonCraw lN ew s

dataset(
N agel
, 
2016
).T hedatacontains63
m illionE nglishnew sarticlescraw ledbetw een

S eptember 2016and F ebruary2019. (76G Baf-

ter ltering). 
4

O 
PEN
W 
EB
T 
EXT
(
G okaslanandC ohen
,
2019
),
anopen-source recreationoftheWebTextcor-
4
Weuse
news-please
(
Hamborgetal.
,
2017
)tocol-
lectandextractCC-N
EWS
. CC-N
EWS
issimilartotheR
E
-
AL
N
EWS
datasetdescribedin
Zellersetal.
(
2019
).
pusdescribedin
R adford etal.
(
2019
).T hetext
isw ebcontentextractedfromURL ssharedon

R eddit w ithatleastthreeupvotes.(38GB).
5

S 
TORIES
,a datasetintroducedin
TrinhandL e
(
2018
)containingasubsetofCommonCrawl
data lteredtomatchthestory-likestyleof

Winograd schemas.(31GB).
3.3E valu ation 

Follow ingprevious work,weevaluateourpre-

trainedmodelsondow nstream tasksusingthefol-

low ing threebenchmarks.

GL UE 
T heG eneralL anguageUnderstand-
ingE valuation(G L UE )benchmark(
Wangetal.
,
2019b
)isacollection of9datasetsforevaluating
naturallanguageunderstandingsystems.
6
Tasks
areframedaseithersingle-sentenceclassi cation

orsentence-pairclassi cationtasks.T heGLUE

organizers providetraining anddevelopmentdata

splits asw ell asasubmissionserverand leader-

board thatallow s participantstoevaluateand com-

paretheirsystems onprivateheld-outtestdata.
ForthereplicationstudyinS ection
4
,wereport
resultsonthedevelopmentsetsafter netuning

thepretrainedmodels onthecorrespondingsingle-

task trainingdata(i.e.,withoutmulti-tasktraining

orensembling).O ur  netuningprocedurefollows

theoriginalB E RTpaper(
Devlinetal.
,
2019
).
InS ection
5
w eadditionally reporttestsetre-
sultsobtainedfromthepublicleaderboard.T hese

resultsdependonaseveraltask-speci cmodica-

tions,w hich w edescribeinS ection
5.1
.
S QuA D 
T heS tanfordQuestionAnswering
D ataset (S Q uA D)providesaparagraphofcontext

and aquestion.T he task isto answerthequestion

byextracting therelevantspanfromthecontext.

WeevaluateontwoversionsofS QuAD:V1.1

andV 2.0(
R ajpurkaretal.
,
2016
,
2018
).In V1.1
thecontextalwayscontainsananswer,whereasin
5
Theauthors andtheirafliatedinstitutionsarenotinany
wayafliatedwiththecreationoftheOpenWebTextdataset.
6
Thedatasetsare:CoLA(
Warstadtetal.
,
2018
),
Stanford SentimentTreebank(SST) (
Socheretal.
,
2013
),MicrosoftResearchParagraphCorpus
(MRPC)(
DolanandBrockett
,
2005
),SemanticTex-
tualSimilarityBenchmark(STS)(
Agirreetal.
,
2007
),
QuoraQuestionPairs(QQP)(
Iyeretal.
,
2016
),Multi-
GenreNLI(MNLI)(
Williamsetal.
,
2018
),QuestionNLI
(QNLI)(
Rajpurkaretal.
,
2016
),RecognizingTextual
Entailment(RTE)(
Daganetal.
,
2006
;
Bar-Haimetal.
,
2006
;
Giampiccoloetal.
,
2007
;
Bentivoglietal.
,
2009
)and
WinogradNLI(WNLI)(
Levesqueetal.
,
2011
).
V 2. 0 som equestionsarenotansw eredinthepro-

vided context,m aking thetaskmorechallenging.
ForS Q uA D V 1. 1 w eadoptthesamespanpre-
dictionm ethodasB E RT (
D evlinetal.
,
2019
).For
S Q uA DV 2. 0,w e addanadditionalbinaryclassi-

 er to predictw hetherthequestionis answ erable,

w hichw etrainjointlybysum mingtheclassi ca-

tionandspanlossterm s.D uringevaluation,w e

onlypredictspanindiceson pairsthatareclassi-

 edasansw erable. 

R AC E 
T heR eA dingC omprehensionfromE x-
am inations(R AC E )(
L aietal.
,
2017
)taskisa
large-scale readingcom prehensiondatasetw ith

m orethan28, 000passages andnearly100,000

questions.T hedataset iscollected fromE nglish

exam inationsinC hina,w hich aredesignedfor

m iddle and highschoolstudents.InR AC E ,each

passageisassociatedw ith m ultiple questions.For

everyquestion,thetaskistoselectonecorrectan-

sw erfromfouroptions.R AC Ehassigni cantly

longercontextthanotherpopularreadingcompre-

hensiondatasetsand the proportionofquestions

thatrequiresreasoningisverylarge.

4TrainingProcedureAnalysis

T hissectionexploresand quanti esw hichchoices

areim portantforsuccessfully pretraining B E RT 

m odels.Wekeepthem odelarchitecture xed.
7
S peci cally,w e begin bytrainingB E RTmodels

w iththesam econ guration asB E RT 
BASE
(
L
= 
12
, 
H 
= 768
, 
A
= 12
,110M params).
4. 1 S taticvs.D yn am icM ask in g

A sdiscussed inS ection
2
,B E RT reliesonran-
dom lym askingandpredictingtokens.T heorig-

inalB E RTim plementationperformedmasking

once duringdata preprocessing,resultingin asin-

gle
static
m ask.Toavoid using thesame mask for
each traininginstanceineveryepoch,trainingdata

was duplicated10 tim esso thateachsequenceis

m askedin10different waysoverthe40epochsof

training.T hus,each training sequencewasseen

w iththesam e m askfour tim esduring training.
Wecom pare thisstrategyw ith
dynam ic m ask-
ing
w herew egeneratethem askingpatternevery
tim ew efeedasequencetothemodel.T hisbe-

com es crucial w henpretrainingformore stepsor

w ithlarger datasets. 
7
Studyingarchitecturalchanges,includinglargerarchi-
tectures,isanimportantareaforfuturework.
Mask in g S QuA D2.0M NLI-mS ST-2
reference76.384.392.8
O ur reim plem entation:

static78.384.392.5

dynamic78.784.092.9
Table1:Comparisonbetweenstaticanddynamic

maskingforBERT
BASE
.WereportF1forSQuADand
accuracyforMNLI-mandSST-2.Reportedresultsare

mediansover5randominitializations(seeds).Refer-

enceresultsarefrom
Yangetal.
(
2019
).
R esu lts
Table
1
compares thepublished
B E RT 
BASE
resultsfrom
Devlinetal.
(
2019
)to our
reimplementation w itheitherstaticordynamic

masking.We ndthatourreimplementation

w ithstaticmaskingperformssimilartothe

originalB E RTmodel, anddynamicmaskingis

comparable orslightly betterthanstaticmasking.
G iventheseresultsandtheadditionalef ciency
bene tsofdynamicmasking,weusedynamic

masking intheremainderoftheexperiments.

4.2Mod el In p u t Format an dNextS enten ce
P red iction 
IntheoriginalB E RTpretrainingprocedure,the

modelobservestwoconcatenateddocumentseg-

ments,w hichareeithersampledcontiguously

fromthe samedocument(with
p
=0
:
5
)orfrom
distinctdocuments. Inaddition tothemaskedlan-

guagemodelingobjective,themodelistrainedto

predictw hethertheobserveddocumentsegments

comefromthesameordistinctdocumentsviaan

auxiliaryN extS entenceP rediction(NSP ) loss.
T heN S Plosswashypothesizedtobeanimpor-
tantfactorintrainingtheoriginalBERTmodel.
D evlinetal.
(
2019
)observethatremovingNSP 
hurtsperformance,w ithsigni cantperformance

degradationonQ N LI,M NL I,andS QuAD1.1.

H ow ever,somerecentworkhasquestionedthe

necessity oftheN S Ploss(
L ampleandConneau
,
2019
;
Yangetal.
,
2019
;
Joshietal.
,
2019
).
To betterunderstandthisdiscrepancy,wecom-
pareseveralalternativetrainingformats:


SEGMENT
-
PAIR
+
NSP
:T hisfollowstheoriginal
inputformat usedinBERT(
Devlin etal.
,
2019
),
w iththeN S P loss.E achinputhasapairofseg-

ments,w hichcaneachcontainmultiplenatural

sentences,butthetotalcombinedlengthmust

belessthan512 tokens.
M od el S QuA D1.1/2.0MN L I-mS S T-2RACE
O ur reim plem entation(w ith N SPloss):
SEGMENT
-
PAIR
90.4/78.784.092.9 64.2
SENTENCE
-
PAIR
88.7/76.282.992.1 63.0
O ur reim plem entation(w ithoutN SPloss):
FULL
-
SENTENCES
90.4/79.184.792.5 64.8
DOC
-
SENTENCES
90.6/79.784.792.7 65.6
B E RT 
BASE
88.5/76.384.392.8 64.3
X L N et
BASE
(K=7) Œ/81.385.892.7 66.1
X L N et
BASE
(K=6) Œ/81.085.693.4 66.7
Table2:Developmentsetresultsforbasemodelspretrained
overB
OOK
C
ORPUS
andW
IKIPEDIA
.Allmodelsare
trainedfor1Mstepswithabatchsizeof256sequences.Werep
ortF1forSQuADandaccuracyforMNLI-m,
SST-2andRACE.Reportedresultsaremediansoververandom
initializations(seeds).ResultsforBERT
BASE
and
XLNet
BASE
arefrom
Yangetal.
(
2019
).

SENTENCE
-
PAIR
+
NSP
: E ach inputcontainsa
pair ofnatural
sentences
,eithersampledfrom
acontiguousportionofonedocumentorfrom

separatedocum ents.S incetheseinputsaresig-

ni cantlyshorterthan512tokens,w eincrease

thebatchsizesothatthetotalnumber oftokens

rem ains sim ilar to
SEGMENT
-
PAIR
+
NSP
.Were-
taintheN S Ploss. 

FULL
-
SENTENCES
:E achinputispackedw ith
fullsentencessam pledcontiguously fromone

orm ore docum ents, such thatthe totallengthis

atm ost 512tokens.Inputs may crossdocument

boundaries.W henw ereachtheendofonedoc-

um ent,w ebeginsam plingsentencesfromthe

nextdocum ent andaddanextraseparatortoken

betw een docum ents.WeremovetheN S P loss.

DOC
-
SENTENCES
:Inputsareconstructedsim-
ilarlyto
FULL
-
SENTENCES
, exceptthatthey
m ay not cross docum entboundaries.Inputs

sam plednearthe end of a documentmaybe

shorterthan512tokens,sow edynamicallyin-

crease thebatch sizeinthesecasestoachieve

asim ilarnum berof totaltokensas
FULL
-
SENTENCES
.Werem ovetheN S P loss.
R esu lts
Table
2
show sresultsforthe fourdif-
ferentsettings.We rstcompare theoriginal
SEGMENT
-
PAIR
input form atfrom
D evlinetal.
(
2019
) to the
SENTENCE
-
PAIR
format;bothfor-
m atsretaintheN S Ploss,butthelatterusessin-

glesentences.We ndthat
u sin g in d ivid u al
sen ten cesh u rtsp erform an ceond ow n stream

task s
, w hichw ehypothesize isbecausethemodel
isnotabletolearnlong-rangedependencies.
We nextcomparetrainingwithouttheNSP 
lossand trainingw ith blocksoftextfromasin-

gledocument (
DOC
-
SENTENCES
).We ndthat
thissettingoutperformstheoriginally published

B E RT 
BASE
resultsand that
removin gth eNSPloss
match es orsligh tly improvesd ownstreamtask 

p erforman ce
,incontrastto
Devlinetal.
(
2019
).
ItispossiblethattheoriginalBERT implementa-

tionmayonlyhaveremovedthelosstermwhile

stillretainingthe
SEGMENT
-
PAIR
inputformat.
F inallyw e ndthatrestricting sequencesto
comefrom asingle document(
DOC
-
SENTENCES
)
performsslightlybetterthanpackingsequences

from multipledocuments(
FULL
-
SENTENCES
).
H ow ever,becausethe
DOC
-
SENTENCES
format
resultsinvariablebatchsizes,weuse
FULL
-
SENTENCES
inthe remainderofourexperiments
foreasiercomparisonwithrelated work.

4.3Train in gw ithlargeb atches

PastworkinN eural M achineTranslation has

show nthattraining w ithverylargemini-batches

canbothimproveoptimization speedandend-task

performance w henthelearningrateisincreased

appropriately (
O ttetal.
,
2018
).Recentworkhas
show nthatB E RT isalsoamenabletolargebatch

training (
Youetal.
,
2019
).
D evlinetal.
(
2019
)originallytrained
B E RT 
BASE
for1Mstepswithabatchsizeof
256sequences.T hisisequivalentincomputa-

tionalcost,viagradientaccumulation,totraining

for125K stepsw ith abatch sizeof2Ksequences,

orfor31Ksteps w ithabatch sizeof8K.
InTable
3
w ecompareperplexity andend-
b szstep slr p p lMN L I-mS S T-2
2561M1e-43. 99 84.792.7
2K125K 7e-4
3. 68 85.292.9
8K31K1e-33. 77 84.692.8
Table3:Perplexityonheld-outtrainingdata(
ppl
)and
developmentsetaccuracyforbasemodelstrainedover
B
OOK
C
ORPUS
andW
IKIPEDIA
withvaryingbatch
sizes(
bsz
).Wetunethelearningrate(
lr
)foreachset-
ting.Modelsmakethesamenumberofpassesoverthe

data(epochs)andhavethesamecomputationalcost.

taskperform anceof B E RT 
BASE
as w e increasethe
batchsize,controllingforthenumberofpasses

throughthetrainingdata.We observethattrain-

ing w ith largebatchesim proves perplexityforthe

m askedlanguagem odelingobjective,as w ellas

end-taskaccuracy.L argebatchesarealsoeasierto

parallelizeviadistributeddataparalleltraining,
8
and inlaterexperim entsw e train w ithbatchesof

8Ksequences. 
N otably
Youetal. 
(
2019
)trainB E RT w itheven
largerbatche sizes,upto32Ksequences.Weleave

furtherexplorationofthe limitsoflargebatch

trainingtofuture work. 

4. 4 TextE n cod in g

B yte-PairE ncoding(B P E )(
S ennrich etal.
,
2016
)
isahybridbetw eencharacter- andword-levelrep-

resentationsthatallow shandlingthelargevocab-

ulariescom moninnaturallanguagecorpora.In-

stead offullwords,B P E relies onsubwords units,

w hichareextractedbyperforming statisticalanal-

ysisofthetrainingcorpus. 
B P Evocabularysizestypicallyrangefrom
10K -100Ksubword units.H ow ever,unicodechar-

acterscanaccountforasizeableportionofthis

vocabularyw henm odeling large anddiversecor-

pora,such asthe ones consideredinthiswork.
R adford etal. 
(
2019
)introduceacleverimple-
m entation ofB P Ethatuses
bytes
insteadofuni-
codecharactersasthebasesubwordunits. U sing

bytesm akesitpossibleto learna subwordvocab-

ularyofam odestsize(50Kunits)thatcanstillen-

codeanyinputtextw ithoutintroducingany ﬁun-

know nﬂ tokens. 
8
Largebatchtrainingcanimprovetrainingefciencyeven
withoutlargescaleparallelhardwarethrough
gradientac-
cumulation
,wherebygradientsfrommultiplemini-batches
areaccumulatedlocallybeforeeachoptimizationstep.Thi
s
functionalityissupportednativelyin
FAIRSEQ
(
Ottetal.
,
2019
).
T heoriginalBERT implementa-
tion(
D evlinetal.
,
2019
)usesacharacter-level
B P E vocabularyofsize30K,whichislearned

afterpreprocessingtheinputwithheuristictok-

enization rules.Following
Radfordetal.
(
2019
),
w einstead considertraining BERTwithalarger

byte-levelB P E vocabularycontaining50Ksub-

wordunits,w ithoutanyadditionalpreprocessing

ortokenizationofthe input.T hisaddsapproxi-

mately 15Mand20Madditionalparametersfor

B E RT 
BASE
andB E RT 
LARGE
,respectively.
E arlyexperimentsrevealed onlyslightdif-
ferencesbetw eentheseencodings,withthe
R adford etal.
(
2019
)BPEachievingslightly
worseend-taskperformanceonsometasks.Nev-

ertheless, w ebelievetheadvantagesofauniver-

salencodingschemeoutweighstheminordegre-

dationin performanceandusethisencodingin

theremainderofourexperiments.Amorede-

tailedcomparison oftheseencodingsislefttofu-

turework.

5RoBERTa

In the previoussectionweproposemodications

totheB E RTpretrainingprocedurethatimprove

end-taskperformance.Wenowaggregatethese

improvementsandevaluatetheircombinedim-

pact.We callthiscon guration
RoBERTa
for
R 
obustly
o
ptimized
B ERT
a
pproach.S peci-
cally,R oB E RTaistrainedwithdynamicmask-

ing(S ection
4.1
),
FULL
-
SENTENCES
withoutNSP 
loss(S ection
4.2
),large mini-batches(S ection
4.3
)
andalargerbyte-levelBPE (S ection
4.4
).
A dditionally,w einvestigatetwootherimpor-
tantfactors thathavebeenunder-emphasizedin

previouswork:(1)thedatausedforpretraining,

and(2)thenumberoftrainingpassesthroughthe

data.Forexample,therecentlyproposedXLNet

architecture(
Yangetal.
,
2019
)ispretrainedus-
ingnearly 10timesmoredatathantheoriginal

B E RT(
D evlinetal.
,
2019
).Itisalsotrainedwith
abatchsizeeighttimeslargerforhalfasmanyop-

timizationsteps,thus seeing fourtimesasmany

sequencesinpretrainingcompared toBERT.
Tohelpdisentangletheimportanceofthesefac-
torsfromothermodelingchoices(e.g.,thepre-

training objective),w ebeginbytraining RoBE RTa

follow ingthe B E RT 
LARGE
architecture(
L 
=24
,
H 
= 1024
,
A
= 16
,355Mparameters).We
pretrainfor100KstepsoveracomparableB
OOK
-
C 
ORPUS
plusW
IKIPEDIA
datasetaswasused in
M od el d atab szstep s
S QuA D 
M NLI-mS ST-2
(v1.1/2.0)
R oB E RTa
w ith B 
OOKS
+ W 
IKI
16G B8K100K93.6/87.389.095.3
+additionaldata(
x
3.2
)160G B 8K100K94.0/87.789.395.6
+pretrain longer160G B 8K300K94.4/88.790.096.1

+pretrain evenlonger160G B 8K500K 
94.6
/
89.4 90.296.4
B E RT 
LARGE
w ith B 
OOKS
+ W 
IKI
13G B 2561M90.9/81.886.693.7
X L N et
LARGE
w ith B 
OOKS
+ W 
IKI
13G B 2561M94.0/87.888.494.4
+additionaldata126G B 2K500K94.5/88.889.895.6
Table4:DevelopmentsetresultsforRoBERTaaswepretraino
vermoredata(16GB
!
160GBoftext)andpretrain
forlonger(100K
!
300K
!
500Ksteps).Eachrowaccumulatesimprovementsfromtherow
sabove.RoBERTa
matchesthearchitectureandtrainingobjectiveofBERT
LARGE
.ResultsforBERT
LARGE
andXLNet
LARGE
arefrom
Devlinetal.
(
2019
)and
Yangetal.
(
2019
),respectively.CompleteresultsonallGLUEtaskscanbefo
undinthe
Appendix.
D evlinetal. 
(
2019
).We pretrainourmodelusing
1024 V 100G P U s forapproximatelyoneday.

R esu lts
We presentourresultsinTable
4
. When
controllingfor trainingdata,w e observethat

R oB E RTaprovidesalargeimprovementoverthe

originallyreported B E RT 
LARGE
results,reaf rming
theim portanceofthedesignchoicesw e explored

inS ection
4
. 
N ext,w ecom bine thisdataw iththethreead-
ditionaldatasetsdescribedinS ection
3.2
.We
train R oB E RTaover thecom bineddata w ith the

sam enum ber oftrainingstepsasbefore(100K ).

Intotal, w epretrainover 160G Boftext.Weob-

servefurtherim provem ents inperformance across

alldow nstreamtasks,validatingtheimportanceof

datasizeanddiversityinpretraining.
9
F inally,w epretrainR oB E RTaforsigni cantly
longer,increasingthenum berofpretrainingsteps

from100Kto300K ,andthenfurtherto500K .We

again observesigni cantgains in dow nstreamtask

perform ance, andthe300Kand500Kstepmod-

elsoutperform X L N et
LARGE
acrossmosttasks.We
notethatevenourlongest-trainedmodeldoesnot

appeartoover t ourdata and would likely bene t

fromadditionaltraining. 
Intherestofthepaper, w eevaluateourbest
R oB E RTa m odel onthethreedifferentbench-

m arks:G L U E ,S Q uaDand R AC E .S peci cally
9
Ourexperimentsconateincreasesindatasizeanddi-
versity.Weleaveamorecarefulanalysisofthesetwodimen-

sionstofuturework.
w e considerR oB E RTatrainedfor500Kstepsover

allveofthedatasetsintroducedinS ection
3.2
.
5.1GL UER esu lts

ForG L UEw econsidertwo netuningsettings.

In the rstsetting(
single-task,dev
)we netune
R oB E RTaseparatelyforeachoftheGLUEtasks,

usingonly thetrainingdataforthecorrespond-

ingtask.We consideralimitedhyperparameter

sw eepforeach task, w ithbatchsizes
2f
16
;
32
g
and learningrates
2 f
1
e

5
;
2
e

5
;
3
e

5
g
,witha
linearwarmup forthe rst 6%ofstepsfollowedby

alineardecay to0.We netunefor10epochsand

performearlystopping basedoneachtask'seval-

uationmetriconthedevset.T herestofthehyper-

parameters remain thesameasduring pretraining.

Inthissetting,w e reportthemediandevelopment

setresultsforeachtaskoververandominitial-

izations,w ithout model ensembling.
Inthesecondsetting(
ensembles,test
),wecom-
pareR oB E RTatootherapproachesonthetestset

via the G L UEleaderboard.W hilemany submis-

sionstothe G L UEleaderboarddependon multi-

task netuning,
ou rsu b missiond epen ds on lyon 
sin gle-task  n etu nin g
.ForRT E,S TSandM RP C
w efoundithelpfulto netunestartingfromthe

MN LIsingle-taskmodel,ratherthanthebaseline

pretrainedR oB E RTa.Weexploreaslightlywider

hyperparameterspace,describedintheAppendix,

andensemble betw een 5and7modelspertask.
M N L I QN LIQQPRT ES S TMR P CC oL AS TSWNLIAvg
Single-tasksingle m odels ondev

B E RT 
LARGE
86. 6/- 92.3 91.370.493.288.0 60.690.0--
X L N et
LARGE
89. 8/- 93.9 91.883.895.689.2 63.691.8--
R oB E RTa
90. 2
/
90. 294.7 92.286.696.490.9 68.092.491.3
-
E nsem bles ontest(from leaderboardasofJuly25,2019)

A L IC E 88. 2/87. 995.7
90.7
83.595.292.6
68.6
91.180.8 86.3
M T-D N N87. 9/87. 496.0 89.986.396.592.7 68.491.189.0 87.6

X L N et 90. 2/89. 898.6 90.386.3
96.893.0
67.891.6
90.4
88.4
R oB E RTa
90. 8/90. 298.9
90.2
88.2
96.792.3 67.8
92.2
89.0
88.5
Table5:ResultsonGLUE.Allresultsarebasedona24-layera
rchitecture.BERT
LARGE
andXLNet
LARGE
results
arefrom
Devlinetal.
(
2019
)and
Yangetal.
(
2019
),respectively.RoBERTaresultsonthedevelopmentsetare
a
medianoververuns.RoBERTaresultsonthetestsetareense
mblesof
single-task
models.ForRTE,STSand
MRPCwenetunestartingfromtheMNLImodelinsteadoftheba
selinepretrainedmodel.Averagesareobtained
fromtheGLUEleaderboard.

Task -sp eci c m od i cation s
Two oftheG L UE 
tasksrequiretask-speci c netuningapproaches

toachieve com petitiveleaderboardresults.
Q N L I
:R ecentsubm issions ontheG L UE 
leaderboardadoptapairw iseranking formulation

forthe Q N L Itask,in w hichcandidateansw ers

arem inedfromthetrainingsetandcomparedto

oneanother,andasingle (question,candidate)

pairisclassi ed aspositive(
L iuetal.
,
2019b
,
a
;
Yangetal. 
, 
2019
). T his form ulationsigni cantly
sim pli esthetask,butisnotdirectly comparable

to B E RT(
D evlinetal. 
, 
2019
). Follow ingrecent
work,w eadopttherankingapproachforourtest

subm ission,butfor directcomparisonw ith B E RT 

w e reportdevelopm entset resultsbasedon apure

classi cationapproach. 
W N L I
:Wefoundthe providedN L I-format
datatobechallengingtoworkw ith. Instead

w eusethereform atted W N LIdatafromS uper-

G L U E(
Wangetal. 
, 
2019a
),w hichindicatesthe
spanofthequery pronoun andreferent.We ne-

tuneR oB E RTausingthem arginrankinglossfrom
Kocijanetal. 
(
2019
).Foragiveninputsentence,
w eusespaC y(
H onnibal andMontani
,
2017
)to
extractadditionalcandidatenounphrasesfromthe

sentenceand netuneourm odelsothatitassigns

higherscorestopositivereferentphrases thanfor

any ofthe generatednegative candidatephrases.

O neunfortunateconsequenceofthisformulation

is thatw e can onlym akeuse ofthe positivetrain-

ingexam ples,w hichexcludesoverhalfofthepro-

vided training exam ples. 
10
10
WhileweonlyusetheprovidedWNLItrainingdata,our
R esu lts
WepresentourresultsinTable
5
.Inthe
 rstsetting(
single-task,dev
),RoBE RTaachieves
state-of-the-artresultsonall9 oftheGLUE

taskdevelopmentsets.Crucially,RoBE RTauses

thesamemaskedlanguagemodelingpretrain-

ing objectiveandarchitectureasBERT 
LARGE
,yet
consistently outperformsbothBERT 
LARGE
and
X L Net
LARGE
. T hisraisesquestionsabouttherel-
ativeimportanceofmodel architectureandpre-

trainingobjective,comparedto moremundanede-

tailslikedatasetsizeandtraining timethatweex-

ploreinthiswork.
Inthesecondsetting (
ensembles,test
),we
submitR oB E RTa totheGLUEleaderboardand

achievestate-of-the-artresultson4outof9tasks

and thehighestaveragescoreto date.T hisisespe-

ciallyexcitingbecauseRoBE RTadoesnotdepend

onmulti-task netuning,unlikemostoftheother

topsubmissions.We expectfutureworkmayfur-

therimprovetheseresultsbyincorporatingmore

sophisticatedmulti-task netuning procedures.

5.2S QuA DR esu lts

We adoptamuchsimplerapproachforS QuAD

comparedtopastwork.Inparticular,while

bothB E RT(
D evlin etal.
,
2019
)andXL-
N et(
Yangetal.
,
2019
)augmenttheirtrainingdata
w ith additionalQ A datasets,
weon ly netu ne
R oB E RTau sin gth ep rovid edS Qu ADtrain ing

d ata
.
Yangetal.
(
2019
)also employedacustom
layer-w iselearningratescheduleto netune
resultscouldpotentiallybeimprovedbyaugmentingthiswi
th
additionalpronoundisambiguationdatasets.
M od el
S Qu A D1. 1S QuA D2.0
E MF 1E MF 1
Singlem odels ondev,w /odataaugm entation

B E RT 
LARGE
84. 1 90. 979.081.8
X L N et
LARGE
89. 0
94. 586.188.8
R oB E RTa88. 9
94. 686.589.4
Singlem odels ontest(asofJuly25,2019)

X L N et
LARGE
86.3
y
89.1
y
R oB E RTa86.889.8

X L N et +S G -N et Veri er
87.0
y
89.9
y
Table6:ResultsonSQuAD.
y
indicatesresultsthatde-
pendonadditionalexternaltrainingdata.RoBERTa

usesonlythe providedSQuADdatainbothdevand

testsettings.BERT
LARGE
andXLNet
LARGE
resultsare
from
Devlinetal.
(
2019
)and
Yangetal.
(
2019
),re-
spectively.

X L N et,w hilew euse the samelearningratefor

alllayers. 
ForS Q uA D v1. 1w efollow thesame netun-
ingprocedure as
D evlinetal. 
(
2019
). ForS Q uA D
v2. 0, w eadditionallyclassify w hetheragiven

questionisansw erable;w etrainthisclassi er

jointlyw ith the spanpredictorby summingthe

classi cationandspan lossterms.

R esu lts
Wepresentourresults inTable
6
.O n
theS Q uA Dv1. 1developm entset,R oB E RTa

m atchesthestate-of-the-art setbyX L Net.O n the

S Q uA Dv2. 0developm entset,R oB E RTasets a

new state-of-the-art,im provingoverX L Netby0.4

points (E M ) and0. 6points (F 1).
Wealsosubm itR oB E RTatothepublicS Q uA D
2. 0leaderboardandevaluateits performancerel-

ative to othersystem s.M ostofthe top systems

buildupon eitherB E RT(
D evlinetal.
,
2019
)or
X L N et(
Yangetal. 
, 
2019
),both ofw hich rely on
additionalexternaltrainingdata.Incontrast,our

subm issiondoesnotuseanyadditionaldata.
O ursingleR oB E RTam odeloutperforms allbut
oneofthesinglem odelsubmissions,andisthe

topscoring system am ong thosethatdonotrely

ondataaugm entation. 

5. 3 R AC ER esu lts

InR AC E ,system sareprovidedw itha passageof

text,anassociatedquestion, andfourcandidatean-

sw ers.S ystemsare requiredto classifyw hichof

thefourcandidateansw ers iscorrect.
Wem odifyR oB E RTafor thistask byconcate-
Mod el A ccu racyM idd le High
Single m odels ontest(asofJuly25,2019)

B E RT 
LARGE
72.076.670.1
X L Net
LARGE
81.785.480.2
R oB E RTa
83.286.581.3
Table7:ResultsontheRACEtestset.BERT
LARGE
and
XLNet
LARGE
resultsarefrom
Yangetal.
(
2019
).
natingeach candidateanswerwiththecorrespond-

ingquestion andpassage.Wethenencodeeachof

these foursequencesandpasstheresulting
[CLS]
representationsthroughafully-connectedlayer,

w hich isused topredictthecorrectanswer.We

truncatequestion-answ erpairsthatarelongerthan

128tokensand,ifneeded,thepassagesothatthe

totallength isatmost512tokens.
R esultsontheR AC Etestsetsarepresentedin
Table
7
.R oB E RTaachievesstate-of-the-artresults
onboth middle-schoolandhigh-schoolsettings.

6Related Work

P retraining methodshavebeendesigned

w ith differenttrainingobjectives, includ-

inglanguage modeling (
DaiandL e
,
2015
;
P eters etal.
,
2018
;
HowardandRuder
,
2018
),
machine translation(
M cCannetal.
,
2017
),and
maskedlanguagemodeling(
Devlin etal.
,
2019
;
L ample andC onneau
,
2019
).M anyrecent
papershaveusedabasicrecipeof netuning

models foreachend task(
HowardandRuder
,
2018
;
R adford etal.
,
2018
), and pretraining
w ithsome variantofamasked languagemodel

objective. H ow ever,newermethodshave

improvedperformanceby multi-task netun-

ing(
D ong etal.
,
2019
),incorporating entity
embeddings(
S un etal.
,
2019
), span predic-
tion(
Joshietal.
,
2019
),andmultiplevariants
ofautoregressivepretraining(
S ongetal.
,
2019
;
C hanetal.
,
2019
;
Yangetal.
,
2019
).P erfor-
manceisalsotypically improvedbytraining

biggermodelsonmoredata(
Devlinetal.
,
2019
;
B aevskietal.
,
2019
;
Yangetal.
,
2019
;
R adford etal.
,
2019
). Ourgoalwastoreplicate,
simplify, andbettertunethetrainingofBERT,

asa referencepointforbetterunderstandingthe

relativeperformanceofallofthesemethods.
7Conclusion

Wecarefullyevaluateanumber ofdesign de-

cisionsw henpretrainingB E RTmodels.We

 ndthatperform ancecanbesubstantiallyim-

provedbytrainingthem odellonger,w ithbigger

batchesoverm oredata; rem ovingthenextsen-

tenceprediction objective;trainingonlongerse-

quences;anddynam icallychangingthemasking

patternappliedto the trainingdata. O ur improved

pretraining procedure,w hichw ecallR oB E RTa,

achieves state-of-the-artresults onG L UE ,R AC E 

andS Q uA D ,w ithoutm ulti-task netuningfor

G L U Eor additionaldataforS Q uA D.T hesere-

sultsillustratetheim portance oftheseprevi-

ouslyoverlookeddesign decisionsandsuggest

that B E RT 'spretrainingobjectiveremainscom-

petitive w ithrecentlyproposedalternatives.
Weadditionallyuseanoveldataset,
C C -N 
EWS
, andreleaseourmodelsand
codeforpretrainingand netuningat:
h t t p s : / / gi t h u b .c om /p yt o rc h/ fa ir se q 
.
References

EnekoAgirre,Llu'isM`arquez,andRichardWicen-
towski,editors.2007.
ProceedingsoftheFourth
InternationalWorkshoponSemanticEvaluations

(SemEval-2007)
.
AlexeiBaevski,SergeyEdunov,YinhanLiu,Luke
Zettlemoyer,andMichaelAuli.2019.Cloze-

drivenpretrainingofself-attentionnetworks.
arXiv
preprintarXiv:1903.07785
.
RoyBar-Haim,IdoDagan,BillDolan,LisaFerro,
DaniloGiampiccolo,BernardoMagnini,andIdan

Szpektor.2006.ThesecondPASCALrecognising

textualentailmentchallenge.In
Proceedingsofthe
secondPASCALchallengesworkshoponrecognis-

ingtextualentailment
.
LuisaBentivogli,IdoDagan,HoaTrangDang,Danilo
Giampiccolo,andBernardoMagnini.2009.The

fthPASCALrecognizingtextualentailmentchal-

lenge.
SamuelRBowman,GaborAngeli,ChristopherPotts,
andChristopherDManning.2015.Alargeanno-

tatedcorpusforlearningnaturallanguageinference.

In
EmpiricalMethodsinNaturalLanguageProcess-
ing(EMNLP)
.
WilliamChan,NikitaKitaev,KelvinGuu,Mitchell
Stern,andJakobUszkoreit.2019.KERMIT:Gener-

ativeinsertion-basedmodelingforsequences.
arXiv
preprintarXiv:1906.01604
.
IdoDagan,OrenGlickman,andBernardoMagnini.
2006.ThePASCALrecognisingtextualentailment

challenge.In
Machinelearningchallenges.evalu-
atingpredictiveuncertainty,visualobjectclassica-

tion,andrecognisingtectualentailment
.
AndrewMDaiandQuocVLe.2015.Semi-supervised
sequencelearning.In
AdvancesinNeuralInforma-
tionProcessingSystems(NIPS)
.
JacobDevlin,Ming-WeiChang,KentonLee,and
KristinaToutanova.2019.BERT:Pre-trainingof

deepbidirectionaltransformersforlanguageunder-

standing.In
NorthAmericanAssociationforCom-
putationalLinguistics(NAACL)
.
WilliamBDolanandChrisBrockett.2005.Auto-
maticallyconstructingacorpusofsententialpara-

phrases.In
ProceedingsoftheInternationalWork-
shoponParaphrasing
.
LiDong,NanYang,WenhuiWang,FuruWei,
XiaodongLiu,YuWang,JianfengGao,Ming

Zhou,andHsiao-WuenHon.2019.Unied

languagemodelpre-trainingfornaturallanguage

understandingandgeneration.
arXivpreprint
arXiv:1905.03197
.
DaniloGiampiccolo,BernardoMagnini,IdoDagan,
andBillDolan.2007.ThethirdPASCALrecog-

nizingtextualentailmentchallenge.In
Proceedings
oftheACL-PASCALworkshopontextualentailment

andparaphrasing
.
AaronGokaslanandVanyaCohen.2019.Openweb-
textcorpus.
http://web.archive.org/
save/http://Skylion007.github.io/

OpenWebTextCorpus
.
FelixHamborg,NormanMeuschke,CorinnaBre-
itinger,andBelaGipp.2017.news-please:A

genericnewscrawlerandextractor.In
Proceedings
ofthe15thInternationalSymposiumofInformation

Science
.
DanHendrycksandKevinGimpel.2016.Gaus-
sianerrorlinearunits(gelus).
arXivpreprint
arXiv:1606.08415
.
MatthewHonnibalandInesMontani.2017.spaCy2:
NaturallanguageunderstandingwithBloomembed-

dings,convolutionalneuralnetworksandincremen-

talparsing.Toappear.
JeremyHowardandSebastianRuder.2018.Universal
languagemodelne-tuningfortextclassication.

arXivpreprintarXiv:1801.06146
.
ShankarIyer,NikhilDandekar,andKornlCser-
nai.2016.Firstquoradatasetrelease:Question

pairs.
https://data.quora.com/First-
Quora-Dataset-Release-Question-

Pairs
.
MandarJoshi,DanqiChen,YinhanLiu,DanielS.
Weld,LukeZettlemoyer,andOmerLevy.2019.

SpanBERT:Improvingpre-trainingbyrepre-

sentingandpredictingspans.
arXivpreprint
arXiv:1907.10529
.
DiederikKingmaandJimmyBa.2015.Adam:A
methodforstochasticoptimization.In
International
ConferenceonLearningRepresentations(ICLR)
.
VidKocijan,Ana-MariaCretu,Oana-MariaCamburu,
YordanYordanov,andThomasLukasiewicz.2019.

Asurprisinglyrobusttrickforwinogradschema

challenge.
arXivpreprintarXiv:1905.06290
.
GuokunLai,QizheXie,HanxiaoLiu,YimingYang,
andEduardHovy.2017.Race:Large-scalereading

comprehensiondatasetfromexaminations.
arXiv
preprintarXiv:1704.04683
.
GuillaumeLampleandAlexisConneau.2019.Cross-
linguallanguagemodelpretraining.
arXivpreprint
arXiv:1901.07291
.
HectorJLevesque,ErnestDavis,andLeoraMorgen-
stern.2011.TheWinogradschemachallenge.In

AAAISpringSymposium:LogicalFormalizationsof

CommonsenseReasoning
.
XiaodongLiu,PengchengHe,WeizhuChen,and
JianfengGao.2019a.Improvingmulti-taskdeep

neuralnetworksviaknowledgedistillationfor

naturallanguageunderstanding.
arXivpreprint
arXiv:1904.09482
.
XiaodongLiu,PengchengHe,WeizhuChen,andJian-
fengGao.2019b.Multi-taskdeepneuralnetworks

fornaturallanguageunderstanding.
arXivpreprint
arXiv:1901.11504
.
BryanMcCann,JamesBradbury,CaimingXiong,and
RichardSocher.2017.Learnedintranslation:Con-

textualizedwordvectors.In
AdvancesinNeuralIn-
formationProcessingSystems(NIPS)
,pages6297Œ
6308.
PauliusMicikevicius,SharanNarang,JonahAlben,
GregoryDiamos,ErichElsen,DavidGarcia,Boris

Ginsburg,MichaelHouston,OleksiiKuchaiev,

GaneshVenkatesh,andHaoWu.2018.Mixedpreci-

siontraining.In
InternationalConferenceonLearn-
ingRepresentations
.
SebastianNagel.2016.Cc-news.
http:
//web.archive.org/save/http:

//commoncrawl.org/2016/10/news-

dataset-available
.
MyleOtt,SergeyEdunov,AlexeiBaevski,Angela
Fan,SamGross,NathanNg,DavidGrangier,and

Michael Auli.2019.
FAIRSEQ
:Afast,exten-
sibletoolkitforsequencemodeling.In
North
AmericanAssociationforComputationalLinguis-

tics(NAACL):SystemDemonstrations
.
MyleOtt,SergeyEdunov,DavidGrangier,and
MichaelAuli.2018.Scalingneuralmachinetrans-

lation.In
ProceedingsoftheThirdConferenceon
MachineTranslation(WMT)
.
AdamPaszke,SamGross,SoumithChintala,Gre-
goryChanan,EdwardYang,ZacharyDeVito,Zem-

ingLin,AlbanDesmaison,LucaAntiga,andAdam

Lerer.2017.AutomaticdifferentiationinPyTorch.

In
NIPSAutodiffWorkshop
.
MatthewPeters,MarkNeumann,MohitIyyer,Matt
Gardner,ChristopherClark,KentonLee,andLuke

Zettlemoyer.2018.Deepcontextualizedwordrepre-

sentations.In
NorthAmericanAssociationforCom-
putationalLinguistics(NAACL)
.
AlecRadford,KarthikNarasimhan,TimeSalimans,
andIlyaSutskever.2018.Improvinglanguageun-

derstandingwithunsupervisedlearning.Technical

report,OpenAI.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
DarioAmodei,andIlyaSutskever.2019.Language

modelsareunsupervisedmultitasklearners.Techni-

calreport,OpenAI.
PranavRajpurkar,RobinJia,andPercyLiang.2018.
Knowwhatyoudon'tknow:Unanswerableques-

tionsforsquad.In
AssociationforComputational
Linguistics(ACL)
.
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
PercyLiang.2016.SQuAD:100,000+questionsfor

machinecomprehensionoftext.In
EmpiricalMeth-
odsinNaturalLanguageProcessing(EMNLP)
.
RicoSennrich,BarryHaddow,andAlexandraBirch.
2016.Neuralmachinetranslationofrarewordswith

subwordunits.In
AssociationforComputational
Linguistics(ACL)
,pages1715Œ1725.
RichardSocher,AlexPerelygin,JeanWu,Jason
Chuang,ChristopherDManning,AndrewNg,and

ChristopherPotts.2013.Recursivedeepmodels

forsemanticcompositionalityoverasentimenttree-

bank.In
EmpiricalMethodsinNaturalLanguage
Processing(EMNLP)
.
KaitaoSong,XuTan,TaoQin,JianfengLu,and
Tie-YanLiu.2019.MASS:Maskedsequence

tosequencepre-trainingforlanguagegeneration.

In
InternationalConferenceonMachineLearning
(ICML)
.
YuStephanieSun,ShuohuanWang,YukunLi,Shikun
Feng,XuyiChen,HanZhang,XinlunTian,Danxi-

angZhu,HaoTian,andHuaWu.2019.ERNIE:En-

hancedrepresentationthroughknowledgeintegra-

tion.
arXivpreprintarXiv:1904.09223
.
TrieuHTrinhandQuocVLe.2018.Asimple
methodforcommonsensereasoning.
arXivpreprint
arXiv:1806.02847
.
AshishVaswani,NoamShazeer,NikiParmar,Jakob
Uszkoreit,LlionJones,AidanNGomez,ukasz

Kaiser,andIlliaPolosukhin.2017.Attentionisall

youneed.In
Advancesinneuralinformationpro-
cessingsystems
.
AlexWang,YadaPruksachatkun,NikitaNangia,
AmanpreetSingh,JulianMichael,FelixHill,Omer

Levy,andSamuelR.Bowman.2019a.SuperGLUE:

Astickierbenchmarkforgeneral-purposelanguage

understandingsystems.
arXivpreprint1905.00537
.
AlexWang,AmanpreetSingh,JulianMichael,Felix
Hill,OmerLevy,andSamuelR.Bowman.2019b.

GLUE:Amulti-taskbenchmarkandanalysisplat-

formfornaturallanguageunderstanding.In
Inter-
nationalConferenceonLearningRepresentations

(ICLR)
.
AlexWarstadt,AmanpreetSingh,andSamuelR.Bow-
man.2018.Neuralnetworkacceptabilityjudg-

ments.
arXivpreprint1805.12471
.
AdinaWilliams,NikitaNangia,andSamuelBowman.
2018.Abroad-coveragechallengecorpusforsen-

tenceunderstandingthroughinference.In
North
AmericanAssociationforComputationalLinguis-

tics(NAACL)
.
ZhilinYang,ZihangDai,YimingYang,JaimeCar-
bonell,RuslanSalakhutdinov,andQuocVLe.

2019.Xlnet:Generalizedautoregressivepretrain-

ingforlanguageunderstanding.
arXivpreprint
arXiv:1906.08237
.
YangYou,JingLi,JonathanHseu,XiaodanSong,
JamesDemmel,andCho-JuiHsieh.2019.Reduc-

ingbertpre-trainingtimefrom3daysto76minutes.

arXivpreprintarXiv:1904.00962
.
RowanZellers,AriHoltzman,HannahRashkin,
YonatanBisk,AliFarhadi,FranziskaRoesner,and

YejinChoi.2019.Defendingagainstneuralfake

news.
arXivpreprintarXiv:1905.12616
.
YukunZhu,RyanKiros,RichardZemel,Ruslan
Salakhutdinov,RaquelUrtasun,AntonioTorralba,

andSanjaFidler.2015.Aligningbooksandmovies:

Towardsstory-likevisualexplanationsbywatch-

ingmoviesandreadingbooks.In
arXivpreprint
arXiv:1506.06724
.
AppendixforﬁRoBERTa:ARobustly

Optimized BERTPretrainingApproachﬂ

AFullresultsonGLUE

InTable
8
w epresent thefullsetofdevelopment
setresultsfor R oB E RTa.Wepresentresultsfor

a
LARGE
con guration thatfollow sB E RT 
LARGE
,
as w ell as a
BASE
con gurationthatfollow s
B E RT 
BASE
. 
BPretrainingHyperparameters

Table
9
describesthehyperparametersforpre-
training ofR oB E RTa
LARGE
andRoBE RTa
BASE
CFinetuningHyperparameters

F inetuninghyperparametersforRACE,S QuAD

andG L UE aregiveninTable
10
.Weselectthe
besthyperparametervaluesbased onthemedian

of5randomseedsforeachtask.
MN L IQN LIQQPRT ES S TM RP CCoLAS TS 
R oB E RTa
BASE
+alldata+500k steps87.6 92.891.9 78.794.890.263.6 91.2
R oB E RTa
LARGE
w ithB 
OOKS
+ W 
IKI
89.0 93.991.9 84.595.390.266.3 91.6
+additionaldata(
x
3. 2
)89.3 94.092.0 82.795.6
91.4
66.1 92.2
+pretrainlonger 300k90.0 94.5
92.2
83.396.191.167.4 92.3
+pretrainlonger 500k
90.2 94.792.2 86.696.4
90.9
68.0 92.4
Table8:DevelopmentsetresultsonGLUEtasksforvariousco
ngurationsofRoBERTa.
H yp erp aramR oB E RTa
LARGE
R oB E RTa
BASE
N umber ofL ayers 2412

H idden size 1024768

F F Ninnerhidden size40963072

A ttention heads1612

A ttention headsize 6464

D ropout0.10.1

A ttention D ropout0.10.1

Warm upS teps 30k24k

P eakL earning R ate 4e-46e-4

B atch S ize 8k8k

WeightD ecay0.010.01

M axS teps500k500k

L earning R ateD ecayL inearL inear

A dam
 
1e-61e-6
A dam

1
0.90.9
A dam

2
0.980.98
G radientC lipping0.00.0
Table9:HyperparametersforpretrainingRoBERTa
LARGE
andRoBERTa
BASE
.
H yp erp aramR AC E S QuA DGL UE
L earning R ate1e-51.5e-5
f
1e-5,2e-5,3e-5
g
B atch S ize1648
f
16,32
g
WeightD ecay0.10.010.1

M axE pochs4210

L earning R ateD ecay L inearL inear L inear

Warm up ratio 0.060.06 0.06
Table10:HyperparametersfornetuningRoBERTa
LARGE
onRACE,SQuADandGLUE.
